# BigDL API ReferenceÂ ğŸ§ ğŸ“ˆ

**Distributed Bitcoin Priceâ€‘Forecasting on ApacheÂ Spark**
This document is the *textâ€‘only* reference for the backend utilities shipped with this project.  It explains **what each helper does, why it exists, and how it plugs into BigDLâ€™s native API** â€“ *without embedding any executable code*.  The companion notebooks (`bigdl.API.ipynb`, `bigdl.example.ipynb`) show the same workflow *in action*.

---

## TableÂ ofÂ Contents

1. [Notebook Objectives](#notebook-objectives)
2. [Architecture &Â DataÂ Flow](#architecture--data-flow)
3. [Environment &Â Setup](#environment--setup)
4. [Schema &Â DataÂ Model](#schema--data-model)
5. [API Function Breakdown](#api-function-breakdown)
6. [Analysis Primitives](#analysis-primitives)
7. [UsageÂ Instructions](#usage-instructions)
8. [Bestâ€‘PracticesÂ &Â Gotchas](#bestâ€‘practices--gotchas)
9. [References](#references)

---

## Notebook Objectives

* **Demonstrate** how BigDLâ€™s native Python API (`bigdl.dllib`) can train a recurrent neural network on *timeâ€‘series* data.
* **Integrate** with Spark so that feature engineering, training, and prediction all run on the same distributed cluster.
* **Provide** a thin utility layer (see `template_utils.py`) that hides repetitive boilerplate such as engine initialisation and `Sample` conversion.

---

## ArchitectureÂ &Â DataÂ Flow

```mermaid
flowchart LR
    A["Raw Prices (REST/CSV)"] -->|"Spark ETL"| B["Cleaned DataFrame"]
    B -->|"to RDD of Sample"| C["BigDL Engine"]
    C --> D["LSTM Network"]
    D -->|"Optimizer"| E["Trained Model"]
    E --> F["Predictions"]
    F --> G["Visual Dashboards"]
```

* **Spark** handles ingestion and transformation.
* **BigDL** attaches to the existing `SparkContext` and parallelises gradient computation.
* **Outputs** (model artefacts & predictions) stay inside the cluster or are exported as Parquet for downstream analytics.

---

## EnvironmentÂ &Â Setup

| Layer            | Version        | Notes                                   |
| ---------------- | -------------- | --------------------------------------- |
| Python           | â€¯3.9           | Pinned in Dockerfile                    |
| ApacheÂ Spark     | â€¯3.1.3         | BigDLÂ 2.4 build expects this            |
| BigDLÂ (DLlib)    | â€¯2.4.0         | Sparkâ€‘3 artefact (`bigdlâ€‘dllibâ€‘spark3`) |
| IntelÂ oneAPI/MKL | autoâ€‘installed | CPU acceleration                        |

*The project ships a readyâ€‘made container.*  Rebuild & run:

```bash
./docker_build.sh           # builds image `bigdl-bitcoin:latest`
winpty docker run --rm -it -v "$(pwd)":/app -p 8888:8888 bigdl-bitcoin:latest bash
python Bitcoin_pipeline.py  # executes full pipeline
```

---

## SchemaÂ &Â DataÂ Model

Data never hits a relational DB â€“ instead we keep it in Spark as a Parquet dataset (`output/bitcoin`):

| Column           | Type      | Description                          |
| ---------------- | --------- | ------------------------------------ |
| `time`           | timestamp | Endâ€‘ofâ€‘hour timestamp (UTC)          |
| `price`          | double    | Spot BTC price inÂ USD                |
| `rolling_avg_1h` | double    | 1â€‘hour moving average (feature)      |
| `pct_change`     | double    | Percentage change from previous tick |

The trained model expects a **window tensor** of shape *(time\_steps,Â 1)* and outputs the **next point forecast**.

---

## API Function Breakdown

| Function                                          | Location              | Purpose                                                           |
| ------------------------------------------------- | --------------------- | ----------------------------------------------------------------- |
| `get_spark_session()`                             | `bitcoin_api.py`      | initialise or reuse a local SparkSession                          |
| `fetch_bitcoin_prices(days)`                      | â€³                     | REST call to CoinGecko, returns raw JSON as SparkÂ DF              |
| `process_bitcoin_data(df)`                        | â€³                     | cleanse NA rows, enforce schema                                   |
| `transform_bitcoin_data(df)`                      | â€³                     | add rolling mean & pctâ€‘change features                            |
| `load_bitcoin_data(df, path)`                     | â€³                     | write Parquet partition(s) to `path`                              |
| `prepare_sequences(prices,Â time_steps)`           | `Bitcoin_pipeline.py` | convert a list â†’ NumPy windows + labels                           |
| `train_rnn_model(df, â€¦)`                          | â€³                     | orchestrate BigDL engine, convert to `Sample`s, spawn `Optimizer` |
| `predict_future(model,Â recent_seq,Â future_steps)` | â€³                     | autoregressive inference loop                                     |
| `visualize_results(df,Â preds)`                    | â€³                     | Matplotlib overlay of actual vs forecast                          |

> âš™ï¸  *All heavy lifting (e.g., `Sample.from_ndarray`, `Optimizer`) happens inside helpers so notebooks stay concise.*

---

## AnalysisÂ Primitives

The project illustrates several classic timeâ€‘series diagnostics:

* **Moving Averages** â€“ 1â€¯h window to smooth noise.
* **Rate of Change** â€“ monitors momentum.
* **Volatility** â€“ rolling standard deviation.
* **Prediction Overlay** â€“ quick sanityâ€‘check of model drift.
  These are implemented in pure pandas/Matplotlib for portability.

---

## UsageÂ Instructions

1. **Build** the Docker image (`./docker_build.sh`).
2. **Launch** an interactive shell (or Jupyter) in the container.
3. **Run** `python Bitcoin_pipeline.py` to execute endâ€‘toâ€‘end.

   * The script will print Spark job logs and save Parquet + PNG chart.
4. **Inspect** results: `output/bitcoin/*.parquet` contains engineered features, and `predictions.png` shows forecast vs ground truth.

---

## Bestâ€‘PracticesÂ &Â Gotchas

* Call **`init_engine()` once** â€“ multiple calls create conflicting threadâ€‘pools.
* Ensure **SparkÂ 3.1.x** compatibility; newer versions mismatch BigDLâ€¯2.4 wheels.
* For Windows + Git Bash use **`winpty`** when mapping interactive TTY (`docker run â€¦`).
* Monitor **SparkÂ UI (localhost:4040)** to catch skewed partitions.
* Persist models with `saveModel(path)` and reload via `loadModel` for inferenceâ€‘only containers.

---

## References

* BigDL documentationÂ â€“ [https://bigdl.readthedocs.io](https://bigdl.readthedocs.io)
* ApacheÂ Spark GuideÂ â€“ [https://spark.apache.org/docs/3.1.3](https://spark.apache.org/docs/3.1.3)
* CoinGecko REST APIÂ â€“ [https://www.coingecko.com/en/api](https://www.coingecko.com/en/api)
* Project rubricÂ & templates â€“ `DATA605/tutorial_template/`
