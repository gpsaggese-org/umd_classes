{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigDL & Spark API Notebook  \n",
    "\n",
    "This notebook is a **quick, end-to-end demo** of the helper functions in `BigDL_API.py`.\n",
    "It mirrors the structure of the SQLite example so future readers have a consistent experience.\n",
    "\n",
    "---\n",
    "## Notebook Objectives\n",
    "1. **Spin-up Spark 3 + BigDL** inside the Docker image.  \n",
    "2. **Fetch live BTC-USD prices** (24 h sample) from CoinGecko.  \n",
    "3. **Clean + feature-engineer** the raw data (`rolling_avg_1h`, `% change`).  \n",
    "4. **Inspect the Spark DataFrame** you can feed into the BigDL LSTM pipeline.\n",
    "\n",
    "## Notebook Flow\n",
    "1. Setup & Imports  \n",
    "2. Spark Session  \n",
    "3. Data Download (`fetch_bitcoin_prices`)  \n",
    "4. Cleaning (`process_bitcoin_data`)  \n",
    "5. Feature Engineering (`transform_bitcoin_data`)  \n",
    "6. Preview & sanity-check\n",
    "\n",
    "## References üìö\n",
    "* **BigDL docs:** <https://bigdl.readthedocs.io>  \n",
    "* **Apache Spark 3.3** API refs  \n",
    "* **CoinGecko REST API:** <https://www.coingecko.com/en/api/documentation>  \n",
    "* Project README for full Docker instructions.\n",
    "\n",
    "> üõ† **Prerequisite** ‚Äì Build the image with `./docker_build.sh` and run a shell or Jupyter inside the container so BigDL & Spark are on the PYTHONPATH.\n"
   ],
   "id": "139d0d2c7b1c57d7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/bigdl/dllib/utils/engine.py:47: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /usr/local/spark, and pyspark is found in: /opt/conda/lib/python3.9/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://73a43598896c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5cd053e190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BigDL_API import (\n",
    "    get_spark_session,\n",
    "    fetch_bitcoin_prices,\n",
    "    process_bitcoin_data,\n",
    "    transform_bitcoin_data,\n",
    ")\n",
    "\n",
    "# 1Ô∏è‚É£ Spark Session\n",
    "spark = get_spark_session()\n",
    "spark"
   ],
   "id": "2d133bd7c47fc50c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|             price|    timestamp|\n",
      "+------------------+-------------+\n",
      "|108482.11390421852|1748063463710|\n",
      "| 108434.0833026226|1748063776529|\n",
      "|108377.99266408195|1748064085703|\n",
      "|108349.45800161053|1748064326238|\n",
      "|108361.03052409452|1748064692643|\n",
      "+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Download a ~24-hour slice just for demo purposes\n",
    "raw_df = fetch_bitcoin_prices(days=1)\n",
    "raw_df.show(5)"
   ],
   "id": "8f0dc6236d2e1a8f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- rolling_avg_1h: double (nullable = true)\n",
      " |-- pct_change: double (nullable = true)\n",
      "\n",
      "+-------------------+------------------+------------------+--------------------+\n",
      "|               time|             price|    rolling_avg_1h|          pct_change|\n",
      "+-------------------+------------------+------------------+--------------------+\n",
      "|2025-05-24 05:11:03|108482.11390421852|108482.11390421852|                null|\n",
      "|2025-05-24 05:16:16| 108434.0833026226|108458.09860342057|-0.04427513427543337|\n",
      "|2025-05-24 05:21:25|108377.99266408195|108431.39662364102|-0.05172786713575643|\n",
      "|2025-05-24 05:25:26|108349.45800161053| 108410.9119681334|-0.02632883463698...|\n",
      "|2025-05-24 05:31:32|108361.03052409452|108400.93567932563|0.010680738692585191|\n",
      "+-------------------+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3Ô∏è‚É£ Cleaning + features\n",
    "clean_df = process_bitcoin_data(raw_df)\n",
    "trans_df = transform_bitcoin_data(clean_df)\n",
    "\n",
    "trans_df.printSchema()\n",
    "trans_df.show(5)"
   ],
   "id": "218306bc25720ccf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ All good!\n",
    "\n",
    "You now have a tidy Spark DataFrame ready for **BigDL LSTM** training (see `BigDL_example.py`).\n",
    "Feel free to adjust the `days` parameter or integrate these helpers into your own workflows.\n"
   ],
   "id": "6fb36cc1b06b3f72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
