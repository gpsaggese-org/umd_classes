{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf378e4",
   "metadata": {},
   "source": [
    "# TF-Agents Basic Demo\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "- Install TF-Agents\n",
    "- Create a Gym environment wrapper\n",
    "- Build a DQN agent with a simple Q-Network\n",
    "- Collect data into a replay buffer\n",
    "- Train the agent on sampled experiences\n",
    "- Evaluate the trained policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f971435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4040ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 03:36:34.660902: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-18 03:36:34.672158: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-18 03:36:34.853203: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-18 03:36:34.856004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-18 03:36:36.393575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import random_tf_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb1f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gym environment\n",
    "env_name = 'CartPole-v0'\n",
    "py_env = suite_gym.load(env_name)\n",
    "tf_env = TFPyEnvironment(py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcaec2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple Q-Network\n",
    "fc_layer_params = (100,)\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58be67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up replay buffer\n",
    "replay_buffer = TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=1000)\n",
    "\n",
    "# Function to collect a step using a policy\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "# Collect some initial random data\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    tf_env.time_step_spec(), tf_env.action_spec())\n",
    "for _ in range(100):\n",
    "    collect_step(tf_env, random_policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f28661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from the replay buffer\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64, num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Convert train to a graph function for speed\n",
    "agent.train = common.function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1893fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "Training completed after 200 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_iterations = 200\n",
    "for _ in range(num_iterations):\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = agent.train(experience)\n",
    "print(f\"Training completed after {num_iterations} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea521ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return over 5 episodes: 9.399999618530273\n"
     ]
    }
   ],
   "source": [
    "# Helper to compute average return\n",
    "def compute_avg_return(environment, policy, num_episodes=5):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "# Evaluate the trained policy\n",
    "avg_return = compute_avg_return(tf_env, agent.policy, num_episodes=5)\n",
    "print(f'Average Return over 5 episodes: {avg_return}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
