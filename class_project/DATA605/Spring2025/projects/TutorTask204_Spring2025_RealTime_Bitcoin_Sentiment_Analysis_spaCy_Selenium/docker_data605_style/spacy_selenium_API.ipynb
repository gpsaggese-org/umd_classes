{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **spaCy and Selenium API Demonstration**\n",
    "\n",
    "This notebook is designed to walk you through the core API functions used in the **Real-Time Bitcoin Sentiment Analysis with spaCy and Selenium** project. We'll focus on two powerful tools: **spaCy** for natural language processing (NLP) and **Selenium** for web scraping. This notebook serves as a companion to the main pipeline notebook, `spacy_selenium_example.ipynb`, and uses functions from `spacy_selenium_utils.py`.\n",
    "\n",
    "If you're new to NLP or web scraping, don't worry! We'll break down each step, explain whatâ€™s happening, and share tips to help you get started. By the end, you'll have a solid understanding of how to process text data with spaCy and scrape tweets from X (Twitter) using Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: spaCy Demonstration**\n",
    "\n",
    "spaCy is a popular Python library for NLP, widely used for tasks like tokenization, lemmatization, named entity recognition (NER), dependency parsing, and part-of-speech (POS) tagging. In our Bitcoin sentiment analysis project, we use spaCy to preprocess tweets before analyzing their sentiment. Letâ€™s explore these capabilities with a simple example.\n",
    "\n",
    "### **What Youâ€™ll Learn Here**\n",
    "- How to clean and preprocess raw text (like tweets).\n",
    "- How to break text into tokens (tokenization).\n",
    "- How to simplify words to their base form (lemmatization).\n",
    "- How to identify entities like people, organizations, or monetary values (NER).\n",
    "- How to understand the grammatical structure of a sentence (dependency parsing and POS tagging).\n",
    "\n",
    "\n",
    "### **Code Example: Processing a Tweet with spaCy**\n",
    "Letâ€™s start with a sample tweet about Bitcoin and walk through the preprocessing steps. Weâ€™ll use spaCyâ€™s `en_core_web_sm` model, a small English model thatâ€™s lightweight and great for beginners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: I just bought some Bitcoin at $50,000!\n",
      "\n",
      "Tokens:\n",
      "I (Lemma: I, POS: PRON)\n",
      "just (Lemma: just, POS: ADV)\n",
      "bought (Lemma: buy, POS: VERB)\n",
      "some (Lemma: some, POS: DET)\n",
      "Bitcoin (Lemma: Bitcoin, POS: PROPN)\n",
      "at (Lemma: at, POS: ADP)\n",
      "$ (Lemma: $, POS: SYM)\n",
      "50,000 (Lemma: 50,000, POS: NUM)\n",
      "! (Lemma: !, POS: PUNCT)\n",
      "\n",
      "Entities:\n",
      "Bitcoin (PERSON)\n",
      "50,000 (MONEY)\n",
      "\n",
      "Dependency Parsing:\n",
      "I --> nsubj (Head: bought)\n",
      "just --> advmod (Head: bought)\n",
      "bought --> ROOT (Head: bought)\n",
      "some --> det (Head: Bitcoin)\n",
      "Bitcoin --> dobj (Head: bought)\n",
      "at --> prep (Head: bought)\n",
      "$ --> nmod (Head: 50,000)\n",
      "50,000 --> pobj (Head: at)\n",
      "! --> punct (Head: bought)\n",
      "\n",
      "POS Tags:\n",
      "I: PRON (pronoun)\n",
      "just: ADV (adverb)\n",
      "bought: VERB (verb)\n",
      "some: DET (determiner)\n",
      "Bitcoin: PROPN (proper noun)\n",
      "at: ADP (adposition)\n",
      "$: SYM (symbol)\n",
      "50,000: NUM (numeral)\n",
      "!: PUNCT (punctuation)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example tweet text\n",
    "text = \"I just bought some Bitcoin #BTC at $50,000!\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "cleaned_text = re.sub(r\"@\\w+|#\\w+\", \"\", cleaned_text)\n",
    "cleaned_text = cleaned_text.encode(\"ascii\", \"ignore\").decode()  # Remove emojis\n",
    "cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
    "print(f\"Cleaned Text: {cleaned_text}\\n\")\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Tokenization\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} (Lemma: {token.lemma_}, POS: {token.pos_})\")\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(\"\\nEntities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n",
    "\n",
    "# Dependency Parsing\n",
    "print(\"\\nDependency Parsing:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} --> {token.dep_} (Head: {token.head.text})\")\n",
    "\n",
    "# Part-of-Speech Tagging\n",
    "print(\"\\nPOS Tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_} ({spacy.explain(token.pos_)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Output Explanation**\n",
    "Letâ€™s break down the output to understand what spaCy is doing:\n",
    "\n",
    "- **Cleaned Text**: We start with a raw tweet: \"I just bought some Bitcoin #BTC at $$50,000!\". After cleaning, we remove URLs, mentions (@username), hashtags (#BTC), emojis, and extra spaces, leaving: \"I just bought some Bitcoin at $50,000!\".\n",
    "- **Tokens**: spaCy splits the text into tokens (words or punctuation). For each token, we see its **lemma** (base form, e.g., \"bought\" becomes \"buy\") and its **POS** (part of speech, e.g., \"VERB\" for \"bought\"). This helps us understand the structure of the sentence.\n",
    "- **Entities (NER)**: spaCy identifies named entities. Here, \"Bitcoin\" is tagged as a `PERSON` (which isnâ€™t quite correctâ€”more on that below), and \"$50,000\" is correctly tagged as `MONEY`. NER is useful for extracting meaningful entities like prices or cryptocurrency names.\n",
    "- **Dependency Parsing**: This shows the grammatical relationships between words. For example, \"I\" is the subject (`nsubj`) of the verb \"bought\", and \"Bitcoin\" is the direct object (`dobj`). This helps us understand how words connect in a sentence.\n",
    "- **POS Tags**: Each token is labeled with its part of speech (e.g., \"PRON\" for pronoun, \"VERB\" for verb). The `spacy.explain()` function gives a beginner-friendly description of each tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Insights for Beginners**\n",
    "- **Why Clean Text First?** : Tweets often contain noise like URLs, hashtags, and emojis that can confuse NLP models. Cleaning ensures spaCy focuses on the meaningful content.\"\n",
    "- **NER Misclassification**: spaCy tagged \"Bitcoin\" as a `PERSON`, but itâ€™s a cryptocurrency. This happens because spaCyâ€™s models are trained on general text, not crypto-specific data. For better accuracy, you can fine-tune spaCy with custom data or add a post-processing step to correct such labels.\n",
    "- **Choosing a Model**: We used `en_core_web_sm` (small model) for speed. If you need better accuracy, try `en_core_web_md` (medium) or `en_core_web_lg` (large), but they require more memory and are slower.\n",
    "- **Practical Tip**: If youâ€™re new to spaCy, start with small examples like this to get comfortable. Use `spacy.explain()` to learn what tags mean, itâ€™s a great way to build your NLP vocabulary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Additional Example: Comparing Different Tweets**\n",
    "Letâ€™s try spaCy on another tweet to see how it handles variations in text. This helps us understand how spaCy behaves with different sentence structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: Elon Musk says Bitcoin will hit $100,000 by 2025!\n",
      "\n",
      "Tokens:\n",
      "Elon (Lemma: Elon, POS: PROPN)\n",
      "Musk (Lemma: Musk, POS: PROPN)\n",
      "says (Lemma: say, POS: VERB)\n",
      "Bitcoin (Lemma: Bitcoin, POS: PROPN)\n",
      "will (Lemma: will, POS: AUX)\n",
      "hit (Lemma: hit, POS: VERB)\n",
      "$ (Lemma: $, POS: SYM)\n",
      "100,000 (Lemma: 100,000, POS: NUM)\n",
      "by (Lemma: by, POS: ADP)\n",
      "2025 (Lemma: 2025, POS: NUM)\n",
      "! (Lemma: !, POS: PUNCT)\n",
      "\n",
      "Entities:\n",
      "Elon Musk (PERSON)\n",
      "Bitcoin (PERSON)\n",
      "100,000 (MONEY)\n",
      "2025 (DATE)\n"
     ]
    }
   ],
   "source": [
    "# Another example tweet\n",
    "text2 = \"Elon Musk says Bitcoin will hit $100,000 by 2025! ðŸš€ #CryptoNews\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text2 = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text2, flags=re.MULTILINE)\n",
    "cleaned_text2 = re.sub(r\"@\\w+|#\\w+\", \"\", cleaned_text2)\n",
    "cleaned_text2 = cleaned_text2.encode(\"ascii\", \"ignore\").decode()\n",
    "cleaned_text2 = re.sub(r\"\\s+\", \" \", cleaned_text2).strip()\n",
    "print(f\"Cleaned Text: {cleaned_text2}\\n\")\n",
    "\n",
    "# Process with spaCy\n",
    "doc2 = nlp(cleaned_text2)\n",
    "\n",
    "# Tokenization\n",
    "print(\"Tokens:\")\n",
    "for token in doc2:\n",
    "    print(f\"{token.text} (Lemma: {token.lemma_}, POS: {token.pos_})\")\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(\"\\nEntities:\")\n",
    "for ent in doc2.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Expected Output**\n",
    "- **Cleaned Text**: \"Elon Musk says Bitcoin will hit $$100,000 by 2025!\"\n",
    "- **Tokens**: Youâ€™ll see tokens like \"Elon\" (Lemma: Elon, POS: PROPN), \"says\" (Lemma: say, POS: VERB), etc.\n",
    "- **Entities**: \"Elon Musk\" should be tagged as `PERSON`, \"Bitcoin\" as `PERSON` (again, a misclassification), \"$100,000\" as `MONEY`, and \"2025\" as `DATE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Beginner Insight: Handling NER Errors**\n",
    "As you saw, spaCy sometimes mislabels \"Bitcoin\" as a `PERSON`. In a real project, you can fix this by:\n",
    "\n",
    "1. Using a Custom List: Check if entities match a list of known cryptocurrencies (like Bitcoin, Ethereum) and relabel them as `PRODUCT` or a custom label.\n",
    "2. Training a Model: Fine-tune spaCy with crypto-related data to improve its accuracy.\n",
    "3. Post-Processing: Write rules to correct common errors, e.g., if an entity is \"Bitcoin,\" change its label to `PRODUCT`.\n",
    "\n",
    "\n",
    "### **Why This Matters for Sentiment Analysis**\n",
    "In our project, we use spaCy to preprocess tweets before feeding them into the VADER sentiment analyzer. Tokenization and lemmatization help standardize the text (e.g., \"bought\" and \"buying\" become \"buy\"), which makes sentiment analysis more consistent. NER helps us identify key entities like prices or crypto names, which we can use to match with CoinGecko data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Selenium Demonstration**\n",
    "\n",
    "Selenium is a tool for automating web browsers, and we use it to scrape tweets from X (Twitter). In our project, we need to collect Bitcoin-related tweets to analyze public sentiment. X requires users to log in to access search results, so weâ€™ll use Selenium to automate the login process and scrape tweets.\n",
    "\n",
    "### **What Youâ€™ll Learn Here**\n",
    "- How to set up Selenium to interact with a website.\n",
    "- How to log in to X using Selenium.\n",
    "- How to scrape tweets and handle dynamic content.\n",
    "- Best practices for web scraping (e.g., avoiding rate limits, handling errors).\n",
    "\n",
    "### **Code Example: Scraping Tweets with Selenium**\n",
    "Weâ€™ll use the `BitcoinSentimentAnalyzer` class from `spacy_selenium_utils.py` to scrape tweets. This class handles the login and scraping process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tweets:\n",
      "- Use your  \n",
      "\n",
      "#BITCOIN $BTC #BTC\\n\n",
      "-  BREAKING: Nach einem Treffen mit El Salvadors PrÃ¤sident \n",
      "@nayibbukele\n",
      " postet Panamas BÃ¼rgermeister \n",
      "@Mayer\n",
      " Ã¼ber eine â€žBitcoin Reserveâ€œ. \\n\n",
      "- Devs, degens, and even your grandma who still thinks Bitcoin is a slot machine.  Letâ€™s dive into what theyâ€™re building, why itâ€™s a big deal for Crypto Twitter (CT), the blockchain, and us commoners, plus why you should be BULLISH\\n\n"
     ]
    }
   ],
   "source": [
    "from spacy_selenium_utils import BitcoinSentimentAnalyzer\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = BitcoinSentimentAnalyzer(\n",
    "    \n",
    "    x_username=\"sidrohtest\",\n",
    "    x_password=\"siddhirohantesting#123\"\n",
    ")\n",
    "\n",
    "# Scrape tweets\n",
    "tweets = analyzer.scrape_tweets(keywords=[\"Bitcoin\"], max_tweets=3)  # Limited to 3 tweets for demo\n",
    "\n",
    "# Display the scraped tweets\n",
    "print(\"Sample Tweets:\")\n",
    "for tweet in tweets:\n",
    "    print(f\"- {tweet['text']}\\\\n\")\n",
    "\n",
    "# Clean up\n",
    "del analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### **Output Explanation**\n",
    "The output shows three sample tweets about Bitcoin. Each tweet is a dictionary with `text` (the tweet content) and `timestamp` (when it was scraped). For example:\n",
    "\n",
    "- \"Bitcoin just touched $$74K. That same HR manager who flagged my crypto side hustle now runs 'on-chain payroll workshops.' Yeah Jessica, glad compliance caught up with capitalism.\"\n",
    "- This tweet mentions Bitcoinâ€™s price ($74K) and has a mix of sentiment (positive about Bitcoin, sarcastic about the HR manager).\n",
    "\n",
    "### **How Selenium Works in This Example**\n",
    "1. **Login**: The `BitcoinSentimentAnalyzer` class navigates to Xâ€™s login page, enters the username and password, and clicks the \"Log in\" button.\n",
    "2. **Search**: It searches for the keyword \"Bitcoin\" using Xâ€™s search bar.\n",
    "3. **Scrolling**: X loads tweets dynamically as you scroll. Selenium scrolls down the page to load more tweets until it collects the desired number `(max_tweets=3)`.\n",
    "4. **Extraction**: It extracts the text of each tweet and stores it with a timestamp.\n",
    "\n",
    "###\n",
    "### **Insights for Beginners**\n",
    "- **Why Selenium?** Unlike simple APIs, X requires a login to access tweets, and its content is dynamic (loaded via JavaScript). Selenium automates a real browser (like Chrome) to interact with the site as a human would.\n",
    "- **Headless Mode**: The `BitcoinSentimentAnalyzer` uses Selenium in headless mode (no visible browser window) for efficiency. If youâ€™re debugging, you can disable headless mode to watch Selenium in action. Just remove the `--headless` argument in `spacy_selenium_utils.py.`\n",
    "- **ChromeDriver Setup**: You need to install ChromeDriver (a separate executable) that matches your Chrome browser version. If you get a version mismatch error, download the correct ChromeDriver from chromedriver.chromium.org and place it in your project directory or PATH.\n",
    "- **Rate Limits and Ethics**: X has strict rules about scraping. Be cautious not to scrape too many tweets at once (we limited to 3 here for safety). Always respect Xâ€™s terms of service, and consider using Xâ€™s official API for larger projects (though it may require a paid plan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why This Matters for Sentiment Analysis**\n",
    "Scraping tweets gives us raw data to analyze. In our project, we collect tweets mentioning \"Bitcoin\" to gauge public sentiment. The more diverse and recent the tweets, the better our sentiment analysis reflects current market moods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Integration with Main Pipeline**\n",
    "\n",
    "The spaCy and Selenium functionalities we explored are integrated into the main pipeline in `spacy_selenium_utils.py`. The `BitcoinSentimentAnalyzer` class combines both tools to:\n",
    "\n",
    "1. Scrape tweets (Selenium).\n",
    "2. Preprocess them (spaCy).\n",
    "3. Analyze sentiment (using VADER, which weâ€™ll cover in the main pipeline).\n",
    "4. Correlate sentiment with Bitcoin prices (using CoinGecko API).\n",
    "5. Visualize the results.\n",
    "\n",
    "For the full pipeline execution, see `spacy_selenium_example.ipynb`. That notebook ties everything together, showing how the preprocessing and scraping steps fit into the larger workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
