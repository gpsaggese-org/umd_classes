{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183c2248-ea3d-43ba-b87e-d821bba1bbc6",
   "metadata": {},
   "source": [
    "# LlamaIndex API Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "LlamaIndex is a powerful data framework for connecting custom data sources to large language models (LLMs). It provides essential tools for building LLM-powered applications over your private data, focusing on:\n",
    "\n",
    "- Data ingestion and connectivity\n",
    "- Data structuring and indexing\n",
    "- Retrieval-augmented generation (RAG)\n",
    "- Structured analytics\n",
    "- Knowledge graph construction\n",
    "- Multi-agent workflows\n",
    "\n",
    "This document provides a concise overview of LlamaIndex's core APIs and their usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265e0d58-a7cd-4edf-a0b4-96b60220e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b548ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install LlamaIndex and its dependencies\n",
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2f997-5c9b-4238-b6d5-e5f2cea43809",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1480ee9-d6a6-437d-b927-da6cbb05bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader, \n",
    "    VectorStoreIndex, \n",
    "    PropertyGraphIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "# to run async code in notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65577ba6",
   "metadata": {},
   "source": [
    "### Settings Configuration\n",
    "\n",
    "LlamaIndex uses a global `Settings` object to manage default behaviors. We also configure `OpenAI` as the LLM provider that configures which model to use and its parameters like temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879a1dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(\"devops/env/default.env\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Configure default settings\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "Settings.llm = llm\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258d7fa",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "The first step in any LlamaIndex workflow is loading your data. LlamaIndex provides a variety of document loaders (called \"readers\") to ingest data from different sources.\n",
    "\n",
    "`SimpleDirectoryReader` is a document loader that reads files from a directory or specific file paths. When invoked with `load_data()`, it returns a list of `Document` objects containing the text content and metadata from your files\n",
    "\n",
    "For this tutorial, we'll create a simple text file and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d90775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Document content preview: \n",
      "    LlamaIndex (GPT Index) is a data framework for your LLM applications.\n",
      "    \n",
      "    It provides:\n",
      "   ...\n"
     ]
    }
   ],
   "source": [
    "# Create a sample text file\n",
    "with open(\"sample_data.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "    LlamaIndex (GPT Index) is a data framework for your LLM applications.\n",
    "    \n",
    "    It provides:\n",
    "    1. Data connectors to ingest your existing data sources\n",
    "    2. Ways to structure your data (indices, graphs, etc.) \n",
    "    3. Query interfaces that make LLMs smarter about your data\n",
    "    4. Tools for evaluation, monitoring, and continual learning\n",
    "    \n",
    "    The primary value props are:\n",
    "    * Building RAG applications\n",
    "    * Structured analytics over your data\n",
    "    * Knowledge graph construction\n",
    "    * Multi-agent frameworks over your data\n",
    "    \"\"\")\n",
    "\n",
    "# Load the data\n",
    "documents = SimpleDirectoryReader(input_files=[\"sample_data.txt\"]).load_data()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Document content preview: {documents[0].text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ebd74",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Index\n",
    "\n",
    "Indices are the heart of LlamaIndex - they structure your data for efficient retrieval and interaction with LLMs.\n",
    "\n",
    "`VectorStoreIndex.from_documents()` creates a vector index by:\n",
    "- Converting document chunks to embeddings\n",
    "- Storing these vectors for similarity search\n",
    "- Maintaining references to the original text\n",
    "\n",
    "The `as_query_engine()` method creates a query interface for your index that handles:\n",
    "- Converting queries to embeddings\n",
    "- Retrieving relevant document chunks\n",
    "- Sending these chunks to the LLM with your query\n",
    "- Generating coherent responses\n",
    "\n",
    "For this tutorial, we'll use the default in-memory vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d871ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main value propositions of LlamaIndex are building RAG applications, structured analytics over your data, knowledge graph construction, and multi-agent frameworks over your data.\n"
     ]
    }
   ],
   "source": [
    "# Create a vector index from the documents\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = vector_index.as_query_engine()\n",
    "\n",
    "# Ask a question\n",
    "response = query_engine.query(\"What are the main value propositions of LlamaIndex?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037cefd",
   "metadata": {},
   "source": [
    "## Saving and Loading Indices\n",
    "\n",
    "Processing large document collections can be time-consuming and computationally expensive. LlamaIndex allows you to persist indices to disk and load them later, saving significant processing time for production applications.\n",
    "\n",
    "`storage_context.persist()` saves your index to disk, avoiding the need to reprocess documents each time. The index components saved include:\n",
    "- Vector embeddings\n",
    "- Document content\n",
    "- Index metadata\n",
    "\n",
    "`StorageContext.from_defaults()` and `load_index_from_storage()` load a previously saved index, recreating the in-memory structures needed for querying.\n",
    "\n",
    "Let's see how to save our vector index to disk and load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ef161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex provides tools for evaluation, monitoring, and continual learning.\n"
     ]
    }
   ],
   "source": [
    "# Create a storage context\n",
    "if not os.path.exists(\"./storage\"):\n",
    "    os.makedirs(\"./storage\")\n",
    "\n",
    "# Save the index to disk\n",
    "vector_index.storage_context.persist(persist_dir=\"./storage\")\n",
    "\n",
    "# Later, we can load the index from disk\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "loaded_index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Use the loaded index\n",
    "loaded_query_engine = loaded_index.as_query_engine()\n",
    "loaded_response = loaded_query_engine.query(\"What tools does LlamaIndex provide?\")\n",
    "\n",
    "print(loaded_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3c7b7",
   "metadata": {},
   "source": [
    "## Creating a Property Graph Index\n",
    "\n",
    "Beyond vector indices, LlamaIndex supports knowledge graphs through its `PropertyGraphIndex`. These graphs capture not just the content of your documents, but the relationships between entities mentioned in them.\n",
    "\n",
    "`PropertyGraphIndex` creates a graph representation of your data by extracting:\n",
    "- Entities (nodes): Entities or concepts (e.g., \"LlamaIndex\", \"RAG\", \"Vector Database\")\n",
    "- Relationships (edges): Relationships between nodes (e.g., \"LlamaIndex -> ENABLES -> RAG\")\n",
    "- Properties (attributes): Attributes attached to nodes and edges\n",
    "\n",
    "`SimpleLLMPathExtractor` uses the LLM to identify subject-predicate-object triplets in your text, building a structured graph. The resulting knowledge graph enables more complex queries that leverage relationships between concepts.\n",
    "\n",
    "Let's create a simple property graph from our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a7ae9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex is a data framework for LLM applications.\n"
     ]
    }
   ],
   "source": [
    "# Create a graph index with a path extractor\n",
    "graph_index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    kg_extractors=[SimpleLLMPathExtractor(llm=llm, max_paths_per_chunk=5)]\n",
    ")\n",
    "\n",
    "# Create a graph query engine\n",
    "graph_query_engine = graph_index.as_query_engine()\n",
    "\n",
    "# Query the graph\n",
    "graph_response = graph_query_engine.query(\"What is the relationship between LlamaIndex and LLMs?\")\n",
    "\n",
    "print(graph_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0133cfd",
   "metadata": {},
   "source": [
    "## Using LlamaIndex Agents\n",
    "\n",
    "LlamaIndex provides a powerful agent framework that enables autonomous and semi-autonomous interactions with your data. Agents can use tools, follow reasoning steps, and execute complex workflows.\n",
    "\n",
    "`FunctionAgent` creates an autonomous assistant that can:\n",
    "- Use tools via `FunctionTool`\n",
    "- Follow a system prompt for behavior guidance\n",
    "- Maintain context across interactions\n",
    "\n",
    "`AgentWorkflow` orchestrates interaction flow, managing:\n",
    "- Tool invocation\n",
    "- Response generation\n",
    "- State management\n",
    "\n",
    "Let's create a simple agent that can search our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41ed394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex can be used for the following purposes:\n",
      "1. Building RAG applications\n",
      "2. Structured analytics over data\n",
      "3. Knowledge graph construction\n",
      "4. Multi-agent frameworks over data\n"
     ]
    }
   ],
   "source": [
    "# Define a simple tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search the knowledge base for information.\"\"\"\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    return str(query_engine.query(query))\n",
    "\n",
    "# Create a tool from the function\n",
    "search_tool = FunctionTool.from_defaults(\n",
    "    name=\"search_knowledge_base\",\n",
    "    description=\"Searches the knowledge base for relevant information\",\n",
    "    fn=search_knowledge_base\n",
    ")\n",
    "\n",
    "# Create an agent\n",
    "agent = FunctionAgent(\n",
    "    name=\"KnowledgeAgent\",\n",
    "    description=\"An agent that answers questions using a knowledge base\",\n",
    "    system_prompt=(\n",
    "        \"You are a helpful assistant that answers questions using a knowledge base. \"\n",
    "        \"If you don't know the answer, say so.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[search_tool]\n",
    ")\n",
    "\n",
    "# Create an agent workflow\n",
    "workflow = AgentWorkflow(agents=[agent], root_agent=agent.name)\n",
    "\n",
    "# Run the agent\n",
    "response = await workflow.run(\"What can I use LlamaIndex for?\")\n",
    "\n",
    "print(response.response.blocks[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c9e52",
   "metadata": {},
   "source": [
    "## Advanced Query Techniques\n",
    "\n",
    "LlamaIndex offers sophisticated querying capabilities beyond basic question-answering. These techniques allow for more precise information retrieval and enhanced response quality.\n",
    "\n",
    "The `as_retriever()` method provides direct access to document retrieval without response synthesis. You can configure:\n",
    "- `similarity_top_k`: Number of documents to retrieve\n",
    "- Metadata filters\n",
    "- Retrieval strategies\n",
    "\n",
    "Let's explore a more advanced retrieval approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87637ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1 documents\n",
      "Document 1 (Score: 0.8177321887934406): LlamaIndex (GPT Index) is a data framework for your LLM applications.\n",
      "    \n",
      "    It provides:\n",
      "    1. Data connectors to ingest your existing data sources\n",
      "    2. Ways to structure your data (indices, graphs, etc.) \n",
      "    3. Query interfaces that make LLMs smarter about your data\n",
      "    4. Tools for evaluation, monitoring, and continual learning\n",
      "    \n",
      "    The primary value props are:\n",
      "    * Building RAG applications\n",
      "    * Structured analytics over your data\n",
      "    * Knowledge graph construction\n",
      "    * Multi-agent frameworks over your data...\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever with metadata filtering\n",
    "retriever = vector_index.as_retriever(\n",
    "    similarity_top_k=2,  # Retrieve top 2 most similar documents\n",
    ")\n",
    "\n",
    "# Retrieve documents\n",
    "retrieved_nodes = retriever.retrieve(\"What are the value props of LlamaIndex?\")\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_nodes)} documents\")\n",
    "for i, node in enumerate(retrieved_nodes):\n",
    "    print(f\"Document {i+1} (Score: {node.score}): {node.text}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e376193",
   "metadata": {},
   "source": [
    "After mastering these basics, you can explore:\n",
    "- Custom data connectors\n",
    "- Different index types\n",
    "- Fine-tuning query parameters\n",
    "- Multi-modal data processing\n",
    "- Production deployment strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
