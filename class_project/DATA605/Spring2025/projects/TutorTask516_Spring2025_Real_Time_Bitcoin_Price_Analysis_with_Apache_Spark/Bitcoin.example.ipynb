{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from pyspark.sql import SparkSession\n",
    "from bitcoin_utils import (\n",
    "    fetch_bitcoin_price,\n",
    "    launch_writer,\n",
    "    launch_spark_stream\n",
    "    )\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"data_demo\"\n",
    "INTERVAL = 15               # Interval (in seconds) between price fetches\n",
    "NUM_POINTS = 20             # Number of records the writer will attempt\n",
    "WINDOW_SIZES = [\"1 minute\"]  # Moving average window sizes\n",
    "SLIDE_INTERVAL = \"15 seconds\"   # Sliding interval for windowed aggregations\n",
    "WATERMARK = \"30 seconds\"          # Watermark to tolerate late-arriving data\n",
    "DELAY_BEFORE_SPARK = 60        # Delay (in seconds) before Spark starts\n",
    "WRITER_DURATION = 420         # Total max run time for writer (in seconds)\n",
    "SPARK_BUFFER = 60              # Extra buffer time to allow Spark to finish writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_pipeline():\n",
    "    \"\"\"\n",
    "    Manages the entire end-to-end pipeline:\n",
    "    - Starts the writer process to fetch and save Bitcoin prices\n",
    "    - Launches Spark streaming queries for multiple window sizes\n",
    "    - Waits for writer completion and allows Spark to finalize batches\n",
    "    - Gracefully stops all Spark streams and terminates the session\n",
    "    \"\"\"\n",
    "    print(\"[MAIN] Launching writer process...\")\n",
    "    writer_process = Process(target=launch_writer, args=(DATA_DIR, INTERVAL, NUM_POINTS))\n",
    "    writer_process.start()\n",
    "    print(f\"[INFO] Writer PID: {writer_process.pid}\")\n",
    "\n",
    "    # Launch Spark streamers\n",
    "    queries, spark = launch_spark_stream(DELAY_BEFORE_SPARK, DATA_DIR, WINDOW_SIZES, WATERMARK, SLIDE_INTERVAL)\n",
    "\n",
    "    try:\n",
    "        print(f\"[MAIN] Streaming for {WRITER_DURATION} seconds...\")\n",
    "        time.sleep(WRITER_DURATION)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[MAIN] Interrupted! Cleaning up early...\")\n",
    "\n",
    "    # Terminate writer\n",
    "    print(\"[MAIN] Terminating writer process...\")\n",
    "    writer_process.terminate()\n",
    "    writer_process.join()\n",
    "\n",
    "    # Wait a buffer period for Spark to flush final batches\n",
    "    print(f\"[MAIN] Waiting {SPARK_BUFFER}s for Spark to finalize batches...\")\n",
    "    time.sleep(SPARK_BUFFER)\n",
    "\n",
    "    # Gracefully stop Spark streaming queries\n",
    "    print(\"[MAIN] Stopping all Spark streams...\")\n",
    "    for q in queries:\n",
    "        try:\n",
    "            q.awaitTermination(timeout=5)\n",
    "            q.stop()\n",
    "            q.awaitTermination()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error while stopping stream: {e}\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    if spark:\n",
    "        try:\n",
    "            spark.stop()\n",
    "            print(\"[MAIN] Spark session stopped.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Error while stopping Spark session: {e}\")\n",
    "\n",
    "    print(\"[MAIN] Pipeline complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-18T16:41:39.604777] Price: $105389.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2025-05-18T16:41:39.604777', 'price': 105389}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch a single Bitcoin price with timestamp\n",
    "fetch_bitcoin_price(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAIN] Launching writer process...\n",
      "[Writer] Starting data collection...\n",
      "[Writer] Record 1 saved at 2025-05-18T16:41:39.676892 → $105389\n",
      "[INFO] Writer PID: 52\n",
      "[Spark] Waiting 60s to let writer populate files...\n",
      "[Writer] Record 2 saved at 2025-05-18T16:41:54.754165 → $105389\n",
      "[Writer] Record 3 saved at 2025-05-18T16:42:09.854908 → $105393\n",
      "[Writer] Record 4 saved at 2025-05-18T16:42:24.940315 → $105393\n",
      "[Spark] Starting Spark session...\n",
      "[Writer] Record 5 saved at 2025-05-18T16:42:40.004295 → $105393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/18 16:42:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Starting stream for WINDOW_SIZE=1 minute → Output: moving_avg_output_1_minute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 16:42:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] All streaming queries started (1 total).\n",
      "[MAIN] Streaming for 420 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 6 saved at 2025-05-18T16:42:55.056188 → $105393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 7 saved at 2025-05-18T16:43:10.130004 → $105399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 8 saved at 2025-05-18T16:43:25.209651 → $105399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 9 saved at 2025-05-18T16:43:40.273174 → $105399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 10 saved at 2025-05-18T16:43:55.360114 → $105399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 11 saved at 2025-05-18T16:44:10.439499 → $105403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 12 saved at 2025-05-18T16:44:25.508935 → $105399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 13 saved at 2025-05-18T16:44:40.579071 → $105403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 14 saved at 2025-05-18T16:44:55.644151 → $105403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 15 saved at 2025-05-18T16:45:10.709517 → $105408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 16 saved at 2025-05-18T16:45:25.793258 → $105408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 17 saved at 2025-05-18T16:45:40.888245 → $105408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 18 saved at 2025-05-18T16:45:55.958366 → $105408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 19 saved at 2025-05-18T16:46:11.029481 → $105411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Writer] Record 20 saved at 2025-05-18T16:46:26.126480 → $105411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAIN] Terminating writer process...\n",
      "[MAIN] Waiting 60s for Spark to finalize batches...\n",
      "[MAIN] Stopping all Spark streams...\n",
      "[MAIN] Spark session stopped.\n",
      "[MAIN] Pipeline complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 16:51:43 WARN StateStore: Error running maintenance thread\n",
      "java.lang.IllegalStateException: SparkEnv not active, cannot do maintenance on StateStores\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:632)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:610)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:453)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN ENTRY POINT ---\n",
    "manage_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
