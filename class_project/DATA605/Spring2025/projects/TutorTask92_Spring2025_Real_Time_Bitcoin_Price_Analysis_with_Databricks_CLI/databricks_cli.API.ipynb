{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b71b775",
   "metadata": {},
   "source": [
    "# Databricks CLI API Demonstration\n",
    "\n",
    "This notebook demonstrates how to interact with the Databricks Command Line Interface (CLI) using Python helper functions defined in `databricks_cli_utils.py`. We will cover basic operations for managing clusters, interacting with the Databricks File System (DBFS), and submitting jobs.\n",
    "\n",
    "**Prerequisites (If you need help to create/configure any of the below Prerequisites refer databricks_cli.api.md):**\n",
    "* Databricks CLI installed and configured (via `databricks configure --token` or environment variables `DATABRICKS_HOST`/`DATABRICKS_TOKEN`). This configuration needs to be done in the environment where this notebook is run (e.g., inside the Docker container).\n",
    "* A `config/cluster_config.json` file defining a valid cluster specification.\n",
    "* A pre-configured Databricks Job (you will need its ID later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6245d7b",
   "metadata": {},
   "source": [
    "### Check if your databricks config is tagged or not before starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2db0909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:38:14.959632Z",
     "start_time": "2025-05-17T20:38:14.949713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, databricks config found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "\n",
    "cfg_path = os.path.expanduser(\"~/.databrickscfg\")\n",
    "\n",
    "if os.path.isfile(cfg_path):\n",
    "    # parse to make sure it actually has a token`\n",
    "    parser = ConfigParser()\n",
    "    parser.read(cfg_path)\n",
    "    if parser.has_option(\"DEFAULT\", \"token\"):\n",
    "        print(\"Yes, databricks config found\")\n",
    "    else:\n",
    "        print(\"Config file exists, but no token found\")\n",
    "else:\n",
    "    print(\"No config file found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81758d15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:38:33.975254Z",
     "start_time": "2025-05-17T20:38:14.964146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Utility functions imported.\n",
      "Cluster config path: config/cluster_config.json\n",
      "Test upload file path: data/api_notebook_test_upload.txt\n",
      "DBFS test upload path: dbfs:/api_notebook_tests/api_notebook_test_upload.txt\n",
      "Job ID to use (replace if needed): 90640552909146\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Import our utility functions\n",
    "import databricks_cli_utils as dcu\n",
    "\n",
    "if not logging.getLogger().hasHandlers():\n",
    "     logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "_LOG = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "CLUSTER_CONFIG_PATH = \"config/cluster_config.json\"\n",
    "CLUSTER_ID_FILE = \"config/cluster_id.txt\" \n",
    "\n",
    "\n",
    "TEST_DIR = \"data\"\n",
    "TEST_UPLOAD_FILENAME = \"api_notebook_test_upload.txt\"\n",
    "TEST_DOWNLOAD_FILENAME = \"api_notebook_test_download.txt\"\n",
    "TEST_UPLOAD_LOCAL_PATH = os.path.join(TEST_DIR, TEST_UPLOAD_FILENAME)\n",
    "TEST_DOWNLOAD_LOCAL_PATH = os.path.join(TEST_DIR, TEST_DOWNLOAD_FILENAME)\n",
    "\n",
    "# Define DBFS paths for testing (use a distinct path for safety)\n",
    "DBFS_TEST_DIR = \"dbfs:/api_notebook_tests\"\n",
    "DBFS_TEST_UPLOAD_PATH = f\"{DBFS_TEST_DIR}/{TEST_UPLOAD_FILENAME}\"\n",
    "\n",
    "# !! IMPORTANT: Replace with a valid Job ID from your Databricks workspace !!\n",
    "# Create a simple job in Databricks UI first (e.g., one that runs a basic notebook)\n",
    "JOB_ID = \"90640552909146\"\n",
    "#\"515902180597777\"\n",
    "\n",
    "os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "cluster_id_to_delete = None\n",
    "\n",
    "print(\"Setup complete. Utility functions imported.\")\n",
    "print(f\"Cluster config path: {CLUSTER_CONFIG_PATH}\")\n",
    "print(f\"Test upload file path: {TEST_UPLOAD_LOCAL_PATH}\")\n",
    "print(f\"DBFS test upload path: {DBFS_TEST_UPLOAD_PATH}\")\n",
    "print(f\"Job ID to use (replace if needed): {JOB_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52a190",
   "metadata": {},
   "source": [
    "## 2. Authentication Prerequisite\n",
    "\n",
    "The utility functions (`dcu.*_cli`) rely on the Databricks CLI being properly authenticated in the environment where this notebook is running.\n",
    "\n",
    "This configuration must be done **before running** the cells below that interact with Databricks. The two common methods are:\n",
    "\n",
    "1.  **`databricks configure --token`:** Run this command in the terminal *once* and provide your Databricks Host URL and Personal Access Token (PAT). Credentials are saved in `~/.databrickscfg`.\n",
    "2.  **Environment Variables:** Set `DATABRICKS_HOST` (your workspace URL) and `DATABRICKS_TOKEN` (your PAT) in your environment before starting Jupyter.\n",
    "\n",
    "We will assume one of these methods has been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367e4be",
   "metadata": {},
   "source": [
    "## 3. Cluster Management\n",
    "\n",
    "The Databricks CLI allows programmatic control over compute clusters. We can create, monitor, and delete clusters.\n",
    "\n",
    "---\n",
    "**Note:** The following cells interact live with your Databricks workspace and require the CLI to be configured. They may incur costs if resources are left running.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20762941",
   "metadata": {},
   "source": [
    "### 3.1 Create Cluster (`databricks clusters create`)\n",
    "\n",
    "This command creates a new cluster based on a JSON configuration file. Our wrapper function `dcu.create_cluster_cli()` handles the call and saves the resulting `cluster_id`.\n",
    "\n",
    "Ensure `config/cluster_config.json` exists and contains valid settings before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b62792f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:38:37.203819Z",
     "start_time": "2025-05-17T20:38:33.981411Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:38:33,988 - __main__ - INFO - Attempting to create cluster using config: config/cluster_config.json\n",
      "2025-05-17 20:38:37,176 - databricks_cli_utils - INFO - Submitted cluster creation. ID: 0517-203836-cjzo8kc\n",
      "2025-05-17 20:38:37,187 - databricks_cli_utils - INFO - Saved cluster ID to config/cluster_id.txt\n",
      "2025-05-17 20:38:37,188 - __main__ - INFO - Cluster creation initiated. ID: 0517-203836-cjzo8kc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster creation request submitted. Cluster ID: 0517-203836-cjzo8kc\n",
      "Cluster ID saved to config/cluster_id.txt: 0517-203836-cjzo8kc\n"
     ]
    }
   ],
   "source": [
    "_LOG.info(f\"Attempting to create cluster using config: {CLUSTER_CONFIG_PATH}\")\n",
    "\n",
    "# Check if config file exists before calling\n",
    "if not os.path.exists(CLUSTER_CONFIG_PATH):\n",
    "    _LOG.error(f\"Cluster configuration file not found at {CLUSTER_CONFIG_PATH}. Please create it.\")\n",
    "else:\n",
    "    cluster_id_created = dcu.create_cluster_cli(\n",
    "        config_file=CLUSTER_CONFIG_PATH,\n",
    "        cluster_id_file=CLUSTER_ID_FILE\n",
    "    )\n",
    "\n",
    "    if cluster_id_created:\n",
    "        cluster_id_to_delete = cluster_id_created\n",
    "        _LOG.info(f\"Cluster creation initiated. ID: {cluster_id_to_delete}\")\n",
    "        print(f\"Cluster creation request submitted. Cluster ID: {cluster_id_to_delete}\")\n",
    "        try:\n",
    "            with open(CLUSTER_ID_FILE, 'r') as f:\n",
    "                id_from_file = f.read().strip()\n",
    "            print(f\"Cluster ID saved to {CLUSTER_ID_FILE}: {id_from_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read cluster ID file {CLUSTER_ID_FILE}: {e}\")\n",
    "\n",
    "    else:\n",
    "        _LOG.error(\"Cluster creation failed. Check logs for details.\")\n",
    "        print(\"Cluster creation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aae820",
   "metadata": {},
   "source": [
    "## 4. DBFS Interaction (`databricks fs ...`)\n",
    "\n",
    "The CLI allows interaction with the Databricks File System (DBFS) for storing and retrieving files.\n",
    "\n",
    "---\n",
    "**Note:** The following cells interact live with your Databricks workspace and require the CLI to be configured.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd08a5",
   "metadata": {},
   "source": [
    "### 4.1 If not created - Create Directory (`databricks fs mkdirs`)\n",
    "\n",
    "While `fs cp` often creates directories, you can also explicitly create directories using `mkdirs`. This is useful for setting up structures beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec78af8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:38:38.471603Z",
     "start_time": "2025-05-17T20:38:37.208264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicitly creating directory: dbfs:/api_notebook_tests/newly_created_dir...\n",
      "Successfully created or ensured directory exists: dbfs:/api_notebook_tests/newly_created_dir\n"
     ]
    }
   ],
   "source": [
    "DBFS_MKDIRS_PATH = f\"{DBFS_TEST_DIR}/newly_created_dir\"\n",
    "print(f\"Explicitly creating directory: {DBFS_MKDIRS_PATH}...\")\n",
    "# Note: Requires configured CLI\n",
    "result = dcu._run_databricks_cli(['databricks', 'fs', 'mkdirs', DBFS_MKDIRS_PATH])\n",
    "if result[\"success\"]:\n",
    "    print(f\"Successfully created or ensured directory exists: {DBFS_MKDIRS_PATH}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to create directory: {result.get('error', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a77f5",
   "metadata": {},
   "source": [
    "### 4.2 Upload File (`databricks fs cp`)\n",
    "\n",
    "We can upload local files to DBFS. Let's create a small test file locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e34252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:38:39.857302Z",
     "start_time": "2025-05-17T20:38:38.474820Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:38:38,479 - __main__ - INFO - Creating dummy file for upload at: data/api_notebook_test_upload.txt\n",
      "2025-05-17 20:38:38,489 - __main__ - INFO - Uploading data/api_notebook_test_upload.txt to dbfs:/api_notebook_tests/api_notebook_test_upload.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy file created at data/api_notebook_test_upload.txt\n",
      "Uploading data/api_notebook_test_upload.txt to dbfs:/api_notebook_tests/api_notebook_test_upload.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:38:39,853 - databricks_cli_utils - INFO - Uploaded 'data/api_notebook_test_upload.txt' to 'dbfs:/api_notebook_tests/api_notebook_test_upload.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "_LOG.info(f\"Creating dummy file for upload at: {TEST_UPLOAD_LOCAL_PATH}\")\n",
    "try:\n",
    "    with open(TEST_UPLOAD_LOCAL_PATH, \"w\") as f:\n",
    "        f.write(f\"This is a test file uploaded by API notebook at {datetime.datetime.now()}.\\n\")\n",
    "    print(f\"Dummy file created at {TEST_UPLOAD_LOCAL_PATH}\")\n",
    "except IOError as e:\n",
    "    _LOG.error(f\"Failed to create dummy file: {e}\")\n",
    "    print(f\"Error creating dummy file: {e}\")\n",
    "\n",
    "# Upload it using the utility function\n",
    "if os.path.exists(TEST_UPLOAD_LOCAL_PATH):\n",
    "    _LOG.info(f\"Uploading {TEST_UPLOAD_LOCAL_PATH} to {DBFS_TEST_UPLOAD_PATH}\")\n",
    "    print(f\"Uploading {TEST_UPLOAD_LOCAL_PATH} to {DBFS_TEST_UPLOAD_PATH}...\")\n",
    "    success = dcu.upload_to_dbfs_cli(\n",
    "        local_path=TEST_UPLOAD_LOCAL_PATH,\n",
    "        dbfs_path=DBFS_TEST_UPLOAD_PATH,\n",
    "        overwrite=True\n",
    "    )\n",
    "    if success:\n",
    "        print(\"Upload successful.\")\n",
    "    else:\n",
    "        print(\"Upload failed.\")\n",
    "else:\n",
    "     print(\"Skipping upload, dummy file not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2ec2c",
   "metadata": {},
   "source": [
    "### 4.3 List Files (`databricks fs ls`)\n",
    "\n",
    "We can list files in DBFS. Our `_run_databricks_cli` helper returns the raw output for commands like `ls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49178fe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:38:40.818117Z",
     "start_time": "2025-05-17T20:38:39.860264Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:38:39,862 - __main__ - INFO - Listing contents of dbfs:/api_notebook_tests\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing contents of dbfs:/api_notebook_tests...\n",
      "Successfully listed files. Raw output:\n",
      "api_notebook_test_upload.txt\n",
      "newly_created_dir\n"
     ]
    }
   ],
   "source": [
    "_LOG.info(f\"Listing contents of {DBFS_TEST_DIR}\")\n",
    "print(f\"Listing contents of {DBFS_TEST_DIR}...\")\n",
    "result = dcu._run_databricks_cli(['databricks', 'fs', 'ls', DBFS_TEST_DIR])\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(\"Successfully listed files. Raw output:\")\n",
    "    # Print the raw output which contains the file listing\n",
    "    print(result[\"raw_stdout\"])\n",
    "else:\n",
    "    print(f\"Failed to list files: {result.get('error', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892f86a",
   "metadata": {},
   "source": [
    "### 4.4 Download File (`databricks fs cp`)\n",
    "\n",
    "We can also download files from DBFS back to our local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc0d66f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:38:42.022476Z",
     "start_time": "2025-05-17T20:38:40.821252Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:38:40,825 - __main__ - INFO - Downloading dbfs:/api_notebook_tests/api_notebook_test_upload.txt to data/api_notebook_test_download.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dbfs:/api_notebook_tests/api_notebook_test_upload.txt to data/api_notebook_test_download.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:38:42,006 - databricks_cli_utils - INFO - Downloaded 'dbfs:/api_notebook_tests/api_notebook_test_upload.txt' to 'data/api_notebook_test_download.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful.\n",
      "Content of downloaded file: 'This is a test file uploaded by API notebook at 2025-05-17 20:38:38.486143.'\n",
      "Cleaned up local download file: data/api_notebook_test_download.txt\n",
      "Cleaned up local upload file: data/api_notebook_test_upload.txt\n"
     ]
    }
   ],
   "source": [
    "_LOG.info(f\"Downloading {DBFS_TEST_UPLOAD_PATH} to {TEST_DOWNLOAD_LOCAL_PATH}\")\n",
    "print(f\"Downloading {DBFS_TEST_UPLOAD_PATH} to {TEST_DOWNLOAD_LOCAL_PATH}...\")\n",
    "success = dcu.download_from_dbfs_cli(\n",
    "    dbfs_path=DBFS_TEST_UPLOAD_PATH,\n",
    "    local_path=TEST_DOWNLOAD_LOCAL_PATH,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"Download successful.\")\n",
    "    # Verify content locally\n",
    "    try:\n",
    "        with open(TEST_DOWNLOAD_LOCAL_PATH, \"r\") as f:\n",
    "            content = f.read()\n",
    "        print(f\"Content of downloaded file: '{content.strip()}'\")\n",
    "        os.remove(TEST_DOWNLOAD_LOCAL_PATH)\n",
    "        print(f\"Cleaned up local download file: {TEST_DOWNLOAD_LOCAL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read or delete downloaded file: {e}\")\n",
    "else:\n",
    "    print(\"Download failed.\")\n",
    "\n",
    "\n",
    "if os.path.exists(TEST_UPLOAD_LOCAL_PATH):\n",
    "    try:\n",
    "        os.remove(TEST_UPLOAD_LOCAL_PATH)\n",
    "        print(f\"Cleaned up local upload file: {TEST_UPLOAD_LOCAL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not remove local upload file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c5986",
   "metadata": {},
   "source": [
    "## 5. Job Management\n",
    "\n",
    "The CLI can trigger pre-defined jobs and check the status of job runs.\n",
    "\n",
    "---\n",
    "**Note:** The following cells interact live with your Databricks workspace and require the CLI to be configured. You also need to replace `\"YOUR_JOB_ID_HERE\"` with a valid Job ID created in your workspace UI.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca0c51",
   "metadata": {},
   "source": [
    "### 5.1 Submit Job (`databricks jobs run-now`)\n",
    "\n",
    "This command starts a run of an existing job. Our wrapper `dcu.submit_notebook_job_cli()` returns the `run_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "170cf4c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:38:44.073851Z",
     "start_time": "2025-05-17T20:38:42.025585Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:38:42,027 - __main__ - INFO - Attempting to submit job with ID: 90640552909146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to submit job with ID: 90640552909146...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:38:44,070 - databricks_cli_utils - INFO - Submitted job '90640552909146'. Run ID: 57365404224252\n",
      "2025-05-17 20:38:44,071 - __main__ - INFO - Job submitted successfully. Run ID: 57365404224252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted successfully. Run ID: 57365404224252\n"
     ]
    }
   ],
   "source": [
    "run_id = None \n",
    "_LOG.info(f\"Attempting to submit job with ID: {JOB_ID}\")\n",
    "print(f\"Attempting to submit job with ID: {JOB_ID}...\")\n",
    "\n",
    "run_id = dcu.submit_notebook_job_cli(JOB_ID)\n",
    "\n",
    "if run_id:\n",
    "    _LOG.info(f\"Job submitted successfully. Run ID: {run_id}\")\n",
    "    print(f\"Job submitted successfully. Run ID: {run_id}\")\n",
    "else:\n",
    "    _LOG.error(\"Job submission failed.\")\n",
    "    print(\"Job submission failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdc97a",
   "metadata": {},
   "source": [
    "### 5.2 Get Job Run Status (`databricks runs get`)\n",
    "\n",
    "Using the `run_id`, we can check the status of that specific job run (e.g., PENDING, RUNNING, SUCCEEDED, FAILED)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5634bf27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:45:10.763527Z",
     "start_time": "2025-05-17T20:45:04.905916Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:45:04,912 - __main__ - INFO - Checking status for job run: 57365404224252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking status for job run: 57365404224252...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:45:10,757 - databricks_cli_utils - INFO - Job run '57365404224252' status: TERMINATED\n",
      "2025-05-17 20:45:10,759 - __main__ - INFO - Current job run state dictionary: {'life_cycle_state': 'TERMINATED', 'result_state': 'SUCCESS', 'state_message': '', 'user_cancelled_or_timedout': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life Cycle State: TERMINATED\n",
      "Result State: SUCCESS\n",
      "State Message: \n"
     ]
    }
   ],
   "source": [
    "if run_id:\n",
    "    _LOG.info(f\"Checking status for job run: {run_id}\")\n",
    "    print(f\"Checking status for job run: {run_id}...\")\n",
    "    time.sleep(5)\n",
    "    run_state = dcu.get_job_run_status_cli(run_id)\n",
    "    if run_state:\n",
    "        _LOG.info(f\"Current job run state dictionary: {run_state}\")\n",
    "        print(f\"Life Cycle State: {run_state.get('life_cycle_state', 'N/A')}\")\n",
    "        print(f\"Result State: {run_state.get('result_state', 'N/A')}\")\n",
    "        print(f\"State Message: {run_state.get('state_message', 'N/A')}\")\n",
    "    else:\n",
    "        _LOG.warning(\"Could not retrieve job run status.\")\n",
    "        print(\"Could not retrieve job run status.\")\n",
    "else:\n",
    "    _LOG.warning(\"Skipping job status check as job submission failed or was skipped.\")\n",
    "    print(\"Skipping job status check - no run ID available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaec963",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "It's crucial to terminate compute resources like clusters when finished to avoid ongoing costs. We also clean up test files.\n",
    "\n",
    "---\n",
    "**Note:** The following cell interacts live with your Databricks workspace and requires the CLI to be configured.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088fb3f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:45:27.547057Z",
     "start_time": "2025-05-17T20:45:26.421769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:45:26,429 - __main__ - INFO - No cluster ID was stored from this session, skipping cluster deletion.\n",
      "2025-05-17 20:45:26,432 - __main__ - INFO - Attempting to remove DBFS test directory: dbfs:/api_notebook_tests\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cluster created in this session, skipping deletion.\n",
      "Attempting to remove DBFS test directory: dbfs:/api_notebook_tests...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:45:27,542 - __main__ - INFO - API Notebook cleanup attempt finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully removed dbfs:/api_notebook_tests\n",
      "Cleanup finished.\n"
     ]
    }
   ],
   "source": [
    "# Delete the cluster created earlier in this notebook\n",
    "if cluster_id_to_delete:\n",
    "    _LOG.info(f\"Attempting to delete cluster: {cluster_id_to_delete}\")\n",
    "    print(f\"Attempting to delete cluster: {cluster_id_to_delete}...\")\n",
    "    deleted = dcu.delete_cluster_cli(cluster_id_to_delete)\n",
    "    if deleted:\n",
    "        _LOG.info(\"Cluster deletion request submitted successfully.\")\n",
    "        print(\"Cluster deletion request submitted successfully.\")\n",
    "        if os.path.exists(CLUSTER_ID_FILE):\n",
    "             try:\n",
    "                 os.remove(CLUSTER_ID_FILE)\n",
    "                 _LOG.info(f\"Removed cluster ID file: {CLUSTER_ID_FILE}\")\n",
    "                 print(f\"Removed cluster ID file: {CLUSTER_ID_FILE}\")\n",
    "             except OSError as e:\n",
    "                 _LOG.warning(f\"Could not remove cluster ID file {CLUSTER_ID_FILE}: {e}\")\n",
    "                 print(f\"Warning: Could not remove cluster ID file {CLUSTER_ID_FILE}\")\n",
    "        cluster_id_to_delete = None\n",
    "    else:\n",
    "        _LOG.error(\"Cluster deletion failed. Manual cleanup may be required.\")\n",
    "        print(\"ERROR: Cluster deletion failed. Please check the Databricks UI.\")\n",
    "else:\n",
    "    _LOG.info(\"No cluster ID was stored from this session, skipping cluster deletion.\")\n",
    "    print(\"No cluster created in this session, skipping deletion.\")\n",
    "\n",
    "\n",
    "# Clean up the test directory created on DBFS\n",
    "_LOG.info(f\"Attempting to remove DBFS test directory: {DBFS_TEST_DIR}\")\n",
    "print(f\"Attempting to remove DBFS test directory: {DBFS_TEST_DIR}...\")\n",
    "cleanup_result = dcu._run_databricks_cli(['databricks', 'fs', 'rm', '-r', DBFS_TEST_DIR])\n",
    "if cleanup_result[\"success\"]:\n",
    "    print(f\"Successfully removed {DBFS_TEST_DIR}\")\n",
    "else:\n",
    "     print(f\"Warning: Failed to remove {DBFS_TEST_DIR} - {cleanup_result.get('error', 'Unknown')}\")\n",
    "\n",
    "\n",
    "_LOG.info(\"API Notebook cleanup attempt finished.\")\n",
    "print(\"Cleanup finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173fc67",
   "metadata": {},
   "source": [
    "## End of API Demonstration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
