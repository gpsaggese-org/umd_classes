# Machine Learning Paradigms
# ##############################################################################

## Major Machine Learning Paradigms
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [36, 214]
* Major Machine Learning Paradigms
  - **Supervised Learning**: Uses labeled data to make predictions, e.g., image classification with ResNet.
  - **Unsupervised Learning**: Identifies patterns in unlabeled data, e.g., customer segmentation using K-means.
  - **Reinforcement Learning**: Involves interaction with an environment to maximize cumulative rewards, e.g., AlphaGo learning to play Go.

* Additional Learning Paradigms
  - **Self-Supervised Learning**: Generates pseudo-labels from unlabeled data for model pre-training, e.g., BERT.
  - **Semi-Supervised Learning**: Combines limited labeled data with abundant unlabeled data, e.g., named entity recognition.
  - **Active Learning**: The model selects informative samples for labeling, e.g., focusing on samples where it is uncertain.

* Advanced Learning Techniques
  - **Federated Learning**: Trains models across multiple devices without raw data sharing, e.g., fraud detection.
  - **Evolutionary Learning**: Optimizes models using genetic algorithms inspired by evolution.
  - **Curriculum Learning**: Gradually trains models on increasing task difficulty, e.g., in robotic simulations.

## Machine Learning in Practice
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [215, 348]
* Machine Learning Flow
  - Key phases: Question, Input Data, Features, Algorithm, Parameters, Evaluation
  - Importance ranking: Clarity of the question > Quality of data > Proper feature selection > Algorithm choice

* Defining the Question
  - Formulate precise questions aligned with objectives
  - Misunderstandings can lead to incorrect problem-solving and data collection
  - Example of poor vs good question formulation provided

* Data and Features
  - Quality data specific to prediction goals is crucial; "Garbage in - garbage out"
  - Good features compress data, retain information, and require expert knowledge
  - Best models are interpretable, simple, accurate, fast, and scalable

## How to Do Research
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [349, 717]
* Principles of Simplicity in Models
  - Occam's Razor emphasizes choosing the simplest model that fits data effectively.
  - A simpler model is less likely to be a coincidence and generally performs better out-of-sample.

* Sampling Bias and Data Snooping
  - Sampling bias occurs when the data is unrepresentative, leading to distorted outcomes and potential unfairness in AI.
  - Data snooping involves using test data during model development, leading to overly optimistic evaluations.

* Effective Research Methodology
  - Focus on achieving good in-sample and out-of-sample performance through proper data handling, feature selection, and regularization.
  - Always summarize results, clarify tasks' purposes, and refine approaches through iterative development for better performance and insights.

## Pipeline Organization
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [718, 808]
* **Pipeline Structure in Machine Learning**
  - ML systems are organized as a pipeline, breaking down problems into sub-problems and solving them sequentially.
  - Overall performance is calculated as a weighted sum of individual stage performances, considering their importance.

* **Photo OCR System Stages**
  - Key stages include text detection, character segmentation, classification, and spelling correction.
  - Example methods for detection include the sliding window approach and classifier training.

* **Data Acquisition and Optimization**
  - To enhance model performance, aim for a low-bias algorithm trained on large datasets, utilizing synthetic data and crowd-sourced labeling.
  - Conduct ceiling analysis to determine where to focus optimization efforts in the pipeline for maximum impact.

## Input Processing
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [809, 966]
* Data Processing Goals
  - Prepare raw data for effective machine learning and improve model performance.
  - Include transformations like normalization, encoding categorical data, and dimensionality reduction.

* Data Cleaning Importance
  - Ensures data quality for accurate model training by detecting and correcting errors.
  - Involves steps like removing duplicates, standardizing formats, and handling missing data effectively.

* Key Techniques in Data Processing
  - Normalization adjusts feature scales; one-hot encoding converts categories into binary vectors.
  - Dimensionality reduction and discretization simplify models, reduce overfitting, and help visualize data.

## Learning Algorithms
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [967, 1390]
* **Gradient Descent Overview**
  - A method to minimize a scalar function \( J(\vw) \) either through analytical methods (setting the gradient to zero) or numerical methods (like gradient descent).
  - It works by iteratively updating weights in the direction of the steepest descent, converging to a local minimum for non-convex functions and to a global minimum for convex functions.

* **Types of Gradient Descent**
  - **Batch Gradient Descent (BGD)** uses all training examples at once, leading to computational intensity and memory requirements.
  - **Stochastic Gradient Descent (SGD)** employs one random training example for updates, introducing randomness that can help avoid local minima.
  - **Mini-Batch Gradient Descent** balances BGD and SGD by using a small subset of data for updates, offering faster convergence and reduced noise.

* **Practical Considerations**
  - Learning rates affect convergence: too large may cause oscillations, while too small results in slow convergence; adaptive methods may help.
  - Feature scaling is crucial for improving convergence speed and stability, with methods like min-max scaling or standardization being effective.
  - Stopping criteria include monitoring changes in cost or reaching a set number of iterations.

## Performance Metrics
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [1391, 1877]
* Research in Machine Learning
  - Explore various aspects like preprocessing methods, models, algorithms, and evaluation techniques.
  - Systematic model evaluation using metrics (e.g., accuracy, F1 score) and statistical tests for validation.

* Measuring Classifier Performance
  - Use metrics such as success/hit rate, log probability, precision, recall, and F-score to assess model effectiveness.
  - Importance of understanding in-sample vs out-of-sample errors and the impact of training/testing data distribution.

* Precision and Recall
  - Crucial for evaluating classifiers, especially when classes are imbalanced; utilize confusion matrices for analysis.
  - Trade-offs between precision and recall can affect model outcomes; AUC and F-scores provide holistic performance views.

## Model Selection
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [1878, 1947]
* Model Selection Overview
  - Involves choosing the best model based on performance metrics when multiple hypotheses exist for the data.
  - Key selection factors include features, learning algorithms, model types, complexity, and regularization parameters.

* Model Selection Process
  - Data is split into training, validation, and test sets (commonly 60/20/20%).
  - Models are trained and evaluated iteratively to minimize validation error, followed by assessment on the test set.

* Model Selection as Learning
  - The process of selecting the model with the smallest validation error is considered a form of learning.
  - True model performance is assessed against the test set, accounting for potential biases in validation estimates.

## Aggregation
// From /Users/saggese/src/umd_msml6101/msml610/lectures_source/Lesson02-Techniques.txt: [1948, 2161]
* Ensemble Learning Overview
  - Involves combining multiple weak learners to create a strong learner, improving prediction accuracy by leveraging diverse model outputs.
  - Techniques include Bagging (reduces variance), Boosting (reduces bias), and Stacking (uses a meta-model to integrate predictions).

* Bagging and Boosting
  - Bagging employs multiple training datasets to enhance model stability and reduce variance without adding bias, often used in Random Forests.
  - Boosting sequentially adjusts weights based on model errors, effectively focusing on difficult cases, examples include AdaBoost.

* Stacking Approach
  - Stacking combines outputs from different model types through a meta-learner, which selects or weights models based on their prediction confidence, enhancing overall model performance.

::: columns
:::: {.column width=15%}
![](lectures_source/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Machine Learning Techniques}}$$**
\endgroup
\vspace{1cm}

**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`

**References**:

- AIMA: ?

- Hastie: ?

// Model assessment and selection
// Hastie 7 (p. 238)

# ##############################################################################