// notes_to_pdf.py --input lectures_source/Lesson9-Causal_inference.txt --output tmp.pdf -t slides
// helpers_root/./dev_scripts_helpers/documentation/render_images.py -i lectures_source/Lesson9-Causal_inference.txt -o lectures_source/Lesson9-Causal_inference.out.txt

::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Causal Inference}}$$**
\endgroup
\vspace{1cm}

**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`

**References**:

- Easy:
  - Hurwitz, Thompson: Causal Artificial Intelligence: The Next Step in Effective
    Business AI, 2024

- Medium / Difficult
  - AIMA
  - Facuce

# ##############################################################################
# Causal AI
# ##############################################################################

// ## Why Causal AI?

## #############################################################################
## Why Causal AI?
## #############################################################################

* Big Data and Traditional AI
- For the past 10 years, **focus of analytics** on:
  - Organize and analyze massive amount of data
  - Data analytics (dashboards, models, reports)
  - Run machine learning on data

- Problems with **traditional AI**
  - Predicts based on observed correlations
  - Can't explain why an outcome occurred

- **AI in decision making**
  - Understand impact of decisions
  - E.g., _"What happens if a product price is reduced by 10%?"_
    - Will more customers buy?
    - If revenue decreases, what to do?
    - Why are customers leaving? Quality issue? Emerging competitor?

* What Are Data Analytics?
- **Collections of data**
  - Aggregated, organized data sets for analysis
  - E.g., customer purchase histories in a CRM system

- **Dashboards**
  - Visual displays of key metrics for insights
  - E.g., dashboard showing quarterly revenue, expenses

- **Descriptive statistics**
  - Summary metrics: mean, median, mode, standard deviation
  - E.g., average sales per quarter to understand trends

- **Historical reports**
  - Examination of past performance
  - E.g., monthly sales reports for past fiscal year

- **Models**
  - Statistical representations to forecast, explain phenomena
  - E.g., predictive model to anticipate customer churn based on behavioral data

* Data Analytics Sophistication

\begingroup \footnotesize
| **Business Question**       | **Methodology**                  |
|-----------------------------|----------------------------------|
| What happened?              | Descriptive statistics           |
| What will happen?           | Predictive models                |
| What should we do?          | Prescriptive programs            |
| What's the best we can do?  | Simulation + optimization        |
\endgroup

\begin{center}
  \includegraphics[width=0.9\textwidth]{msml610/lectures_source/figures/Lesson9-Analytical_sophistication.png}
\end{center}

* Explainability
- **Regulators** require that if you are making decisions using ML / AI, you
  should be able to defend the results of your analysis
  - E.g., decide who to hire, how to set up a policy

- Organizations can:
  - Be **fined** by regulatory authorities
  - Face **backlash** from customers and activists

- E.g., neural networks are "black boxes"
  - Lack of explainability
    - Humans can't understand how inputs are combined into a conclusion
    - Cannot explain to shareholders why certain decisions were made
  - Bias
    - E.g., using age, race, sex as a feature can introduce bias

- **Explainable AI** allow users to:
  - Comprehend
  - Explain
  - Trust the results by the machine

* Correlation is Not Causation!
- **Correlation** is a statistical method for understanding relationships
  between data
  - Pros
    - Use past outcomes to predict future outcomes by finding patterns and
      anomalies
  - Cons
    - Doesn't explain the cause
    - Variables may move together due to coincidence or a hidden factor

- **Causation** explains how changing one variable influences the other
  - Cannot be concluded from correlation alone

- **Data does not understand causes and effects**
  - Only humans can identify variables and relationships based on context
  - Without causation, you can't make intelligent decisions

* Causal AI
- **Understands the why**
  - Determines cause-and-effect between variables
  - E.g., whether a marketing campaign increased sales

- **Identify interventions**
  - Identifies variables and interventions to change outcomes
  - E.g., which lifestyle changes reduce blood pressure

- **Predicting counterfactuals**
  - Hypothesizes outcomes under different circumstances
  - E.g., student grades if they attended a different school

- **Avoiding bias**
  - Traditional AI biased by training data and ignored variables
  - Ensure fairness by accounting for confounding variables

- **Improving decision-making**
  - Provides understanding of relationships for better decisions
  - E.g., improving supply chain by understanding logistic impact

* Causal AI vs Traditional AI
- _"The next revolution of data science is the science of interpreting reality,
  not of summarizing data"_ (Judea Pearl, 2021)

- Current AI uses correlation to:
  - Analyze data
  - Identify patterns
  - Make predictions

- Models depend on data quality
  - Biased or unclean data $\implies$ poor model

## The Ladder of Causation

* The Ladder of Causation

- Pearl provided a 3-layer framework for understanding causality

\begingroup \scriptsize
| **Level**          | **Symbol**         | **Activity** | **Typical Questions**    |
| ------------------ | -------------------| -------------| ------------------------ |
| 1. Association     | $\Pr(Y | X)$       | Observing    | What is?                 |
| 2. Intervention    | $\Pr(Y | do(X), Z)$| Intervening  | What if?                 |
| 3. Counterfactuals | $\Pr(Y_X | x', y')$| Imagining    | Why?                     |
\endgroup

* Rung 1: Association
- **Question**: _"How would seeing $X$ change our belief in $Y$?"_

- **Symbol**: $\Pr(Y|X)$
  - Bayesian update

- **Activity**
  - It is just "passive observation"
  - Determine if two things are related
  - Traditional AI and ML is based on this

- **Example**
  - _"The tree has green leaves during spring"_
  - _"What does a symptom tell you about a disease?"_
  - _"What does a survey tell you about the election results?"_

* Rung 2: Intervention
- **Question**: _"What happens to $Y$ if you do $X$?"_

- **Symbol**: $\Pr(Y | do(X), Z)$

- **Activity**
  - Understand the impact of an action
  - E.g., _"tree has green leaves"_ vs _"spring makes tree leaves turn green"_
  - Association is just about observations
  - Interventions involve "doing something" and need a causal model

- **Example**
  - _"Why did the headache go away?"_
    - "Because the pain reliever" or "Because you ate food after skipping lunch"
  - _"If you take aspirin, will your headache be cured?"_
  - _"What if you ban sodas?"_

* Level 3: Counterfactuals
- **Question**: _"Was $X$ that caused $Y$?"_

- **Symbol**: $\Pr(Y_X | x', y')$

- **Activity**:
  - Imagine what will happen if facts were different
  - Predicting an outcome is the highest form of reasoning
    - It requires to understand relationships between cause and effect

- **Example**
  - Scientific experiments: _"What if we give a child an adult dose of a drug?"_
  - Litigation: _"What would the jury conclude?"_
  - Marketing: _"Why did my marketing campaign fail to generate sales?"_

// # Chap 3: Elements of Causal AI (Causal_AI_in_business.Hurwitz.2023.txt)

## #############################################################################
## Correlation vs Causation Models
## #############################################################################

* Correlation vs Causation Model
- **Correlation** = identify how variables are related to each other
- **Causality** = determine whether one variable causes another variable

  - Both:
    - Accept inputs and transform them to compute predictions
    - Identify how variables are related to each other

  - Correlation-based AI works well when there is abundant historical and
    observational data
  - Causal-based AI first creates a business-focused model before integrating data

* Correlation-Based Model Process
- **Correlation-based AI** is "data first"
  - The more data collected the better

- **Modeling process**
  - Acquire data
  - Integrate and clean data
  - Exploratory data analysis (EDA)
  - Feature engineering
  - Build and test models
  - Deploy models in production

- **Many AI projects fail because**
  - Cultural and organizational issues
  - Models are opaque and lack explainability
  - Spurious correlations
  - Missing articulating "what's the goal of doing ML?"

* Causation-based Model Process
- **Causal AI** is "model first"
  - Understand business question before ingest and transform the data

- **Modeling process**
  - What is the intended outcome?
  - What is the proposed intervention?
  - What are the confounding factors?
  - What are the effecting factors?
  - Create a model graph or diagram
  - Data acquisition
  - ...

# Causal Networks

// ## 13.5 Causal networks (p. 462)

* (Non-Causal) Bayesian Networks
- **Bayesian networks** represent a joint distribution function
  - The direction of the arrow represent **conditional dependence** (not
    causality)
    - $A \to B$ requires to estimate $\Pr(A | B)$
  - There are many possible **edges** and **node ordering** for the same Bayesian
    network

::: columns
:::: {.column width=60%}

- E.g., a Bayesian network with $Fire$ and $Smoke$, which are dependent
  - Specify as $Fire \to Smoke$
    - Need $\Pr(Fire)$ and $\Pr(Smoke | Fire)$ to compute $\Pr(Fire, Smoke)$
  - Represent as $Smoke \to Fire$
    - Need $\Pr(Smoke)$ and $\Pr(Fire | Smoke)$
  - Networks are equivalent and convey the same information
  - Different difficulties to estimate

::::
:::: {.column width=35%}

```graphviz
digraph BayesianNetwork1 {
    rankdir=LR;
    Fire [label="P(Fire)"];
    Smoke [label="P(Smoke)"];
    Fire -> Smoke [label="P(Smoke | Fire)"];
}
```

```graphviz
digraph BayesianNetwork1 {
    rankdir=LR;
    Fire [label="P(Fire)"];
    Smoke [label="P(Smoke)"];
    Smoke -> Fire [label="P(Fire | Smoke)"];
}
```

::::
:::

- There is an **asymmetry in nature**
  - Extinguishing fire stops smoke
  - Clearing smoke doesn't affect fire

* Causal (Bayesian) Networks
- **Causal networks** are Bayesian networks forbidding **non-causal edges**

- Use judgment based on nature instead of just statistics
  - E.g., from _"Are random variables $Smoke$ and $Fire$ correlated?"_ to _"What
    causes what, $Smoke$ or $Fire$?"_

- **"Dependency in nature" is like assignment in programming**
  - E.g., nature assigns $Smoke$ based on $Fire$:
    - ![](emoji/check_mark.png){ width=12px } $Smoke := f(Fire)$
    - ![](emoji/wrong.png){ width=12px } $Fire := f(Smoke)$
  - Structural equations describe "assignment mechanism"
    $$
    x_i := f(x_j) \iff X_j \to X_i
    $$

* Causal DAG
- **Causal DAG**
  - _Directed_: Arrows show direction of cause $\rightarrow$ effect
  - _Acyclic_: No feedback loops
    - Causal relationships assume a temporal order: cause happens before effect
    - A cycle would imply a variable is both a cause and effect of itself
      (paradox)

- **Benefits**
  - DAGs encode _causal_ rather than _associative_ links
  - Enables reasoning about interventions and counterfactuals
  - Supports explainable AI models
  - Stability to estimation

- **Limitations**:
  - Requires domain knowledge to specify structure
  - Assumes all relevant variables are included (no hidden confounders)

* Causal Edges are Stable

- **Causal edge** $X \rightarrow Y$ shows direct causal influence of $X$ on $Y$,
  holding other variables constant
  - Captures how manipulating $X$ changes $Y$, not just their covariance

- Causal edges reflect **stable relationship**

  - Mechanistic stability
    - Causal relationships show system function, not just behavior in one dataset
    - E.g., "Temperature $\rightarrow$ ice melting rate" holds true in Alaska and
      Arizona

  - Invariance under interventions
    - If $X$ causes $Y$, intervening on $X$ affects $Y$ consistently, despite
      confounders or context changes
    - Invariance makes causal edges robust to data distribution shifts

  - Easier estimation through causal modeling
    - Identifying causal direction focuses estimation on effect size (e.g.,
      regression of $Y$ on $X$ under intervention)

  - Reduced sensitivity to sampling and omitted variables
    - Correlations may change with confounder addition or removal
    - True causal edge persists, stable across model specifications

- **Example**: study $Exercise \rightarrow Health$:
  - Correlation may differ in young or elderly populations
  - Causal effect remains stable, as physiological mechanism doesn't change

* Structural Causal Model
- A **Structural Causal Model** (SCM) translates a causal DAG into mathematical
  equations
  - DAGs show structure (variables and arrows)
  - SCMs use equations to define how variables interact

- **Structure of SCMs**
  - Equations model each variable as a function of its direct causes
  - Variables: $X_1, X_2, ..., X_n$ represent quantities in the system
  - Formally, $X_i$ is modeled as:
    $$X_i = f_i(Parents(X_i), \varepsilon_i)$$
    where:
    - $Parents(X_i)$ are direct causes of $X_i$
    - $\varepsilon_i$ is an exogenous (external, unobserved) noise term

- **Properties**:
  - Explain causal relationships between variables
  - Provide a foundation for causal reasoning and simulation
  - Describe how the world works, not just variable correlations

* Structural Causal Model: Example
::: columns
:::: {.column width=50%}
- **Explanatory variables**
  - You can manipulate or observe when changes are applied
  - E.g., _"does a large cup of coffee before an exam help with a test?"_

- **Outcome variables**
  - Result of the action
  - E.g., _"by how much did the score test improve?"_
::::
:::: {.column width=45%}
```graphviz
digraph SCMExample {
  rankdir=TB;
  splines=true;
  nodesep=1.0;
  ranksep=0.75;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  Coffee [label="Large Cup of Coffee", fillcolor="#b3cde3"];
  TestScore [label="Test Score Improvement", fillcolor="#ccebc5"];
  RoomTemp [label="Room Temperature", fillcolor="#decbe4"];

  // Edges
  Coffee -> TestScore;
  RoomTemp -> TestScore [style=dashed];
}
```
::::
:::

- **Unobserved variables**
  - Not seen and more difficult to account
  - E.g., _"temperature of the room makes students sleepy and less alert"_

* Structural Causal Model: Sprinkler Example
::: columns
:::: {.column width=25%}

```graphviz
digraph BayesianFlow {
    rankdir=TD;
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    Cloudy        [label="Cloudy",        fillcolor="#b3cde3"];
    Sprinkler     [label="Sprinkler",     fillcolor="#ccebc5"];
    Rain          [label="Rain",          fillcolor="#ccebc5"];
    WetGrass      [label="Wet Grass",     fillcolor="#decbe4"];
    GreenerGrass  [label="Greener Grass", fillcolor="#fed9a6"];

    // Force ranks
    { rank=same; Sprinkler; Rain; }

    // Edges
    Cloudy -> Sprinkler;
    Cloudy -> Rain;
    Sprinkler -> WetGrass;
    Rain -> WetGrass;
    WetGrass -> GreenerGrass;
}
```
::::
:::: {.column width=70%}
- Structural equations for this model:
  \begingroup \small
  \begin{align*}
    & C = f_C(U_C) \\
    & R = f_R(C, U_R) \\
    & S = f_S(C, U_S) \\
    & W = f_W(R, S, U_W) \\
    & G = f_G(W, U_G) \\
  \end{align*}
  \endgroup

- Unmodeled variables $U_x$ represent error terms
  - E.g., $U_W$ is another source of wetness besides $Sprinkler$ and $Rain$
    (e.g., $MorningDew$)
  - Assume unmodeled variables are exogenous, independent, with a certain
    distribution (prior)
::::
:::

- Express joint distribution of five variables as a product of conditional
  distributions using causal DAG topology:
  \begingroup \small
  $$
  \Pr(C, R, S, W, G) = \Pr(C) \Pr(R|C) \Pr(S|C) \Pr(W|R,S) \Pr(G|W)
  $$
  \endgroup

## #############################################################################
## Variables
## #############################################################################

* Observed Vs. Unobserved Variables
::: columns
:::: {.column width=45%}
- **Observed variables**
  - Aka "measurable" or "visible"
  - Variables directly measured or collected in a dataset
  - E.g.,
    - Education
    - Income
    - Age
    - Blood pressure
    - Product price

::::
:::: {.column width=50%}

```graphviz
digraph EducationIncomeModel_ObservedUnobserved_Vertical {
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#ccebc5"];
    Income          [label="Income",          fillcolor="#ccebc5"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#ccebc5"];
    Motivation      [label="Motivation",      fillcolor="#decbe4"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#decbe4"];
    Weather         [label="Weather",         fillcolor="#decbe4"];
    News            [label="News",            fillcolor="#decbe4"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend (top, vertical stack)
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical stacking

        key1 [label="Observed", shape=box, style="rounded,filled", fillcolor="#ccebc5"];
        key2 [label="Unobserved",  shape=box, style="rounded,filled", fillcolor="#decbe4"];

        key1 -> key2 [style=invis];
    }
}
```
::::
:::

- **Unobserved variables**
  - Aka "latent" or "hidden"
  - Exist but not measured or included in data
  - E.g.,
    - Natural talent
    - Motivation
    - Company culture
  - Ignoring unobserved variables distorts causal relationships
    - Observed: $IceCreamSales$ and $DrowningRates$
    - Unobserved: $Temperature$
    - Misleading conclusion: $IceCream$ causes $Drowning$

* Endogenous Vs. Exogenous Variables
::: columns
:::: {.column width=45%}
- **Endogenous variables**
  - Variables whose values are determined _within_ the model
    - I.e., dependent on other variables in the system
  - Represent the system’s internal behavior and outcomes

::::
:::: {.column width=50%}

```graphviz
digraph EducationIncomeModel_EndoExo{
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#fed9a6"];
    Income          [label="Income",          fillcolor="#b3cde3"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#fed9a6"];
    Motivation      [label="Motivation",      fillcolor="#b3cde3"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#fed9a6"];
    Weather         [label="Weather",         fillcolor="#fed9a6"];
    News            [label="News",            fillcolor="#fed9a6"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend at top
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical stacking

        key1 [label="Endogenous", shape=box, style="rounded,filled", fillcolor="#b3cde3"];
        key2 [label="Exogenous",  shape=box, style="rounded,filled", fillcolor="#fed9a6"];

        key1 -> key2 [style=invis];
    }
}
```
::::
:::
- **Exogenous variables**
  - Variables that originate _outside_ the system being modeled
    - I.e., not caused by other variables in the model
  - Often represent background conditions or external shocks
  - E.g.,
    - Natural talent
    - Economic policy
    - Weather
    - News

* Endo / Exogenous, Observed / Unobserved Vars
::: columns
:::: {.column width=35%}
- In **Structural Causal Models**
  $$
  X_i = f_i(Parents(X_i), \varepsilon_i)
  $$
  where:
  - $X_i$ is endogenous (modeled variable in the system)
  - $\varepsilon_i$ is exogenous noise term (outside causes)

- **Typically**
  - _Endogenous variables_: focus for prediction and intervention
  - _Exogenous variables_: capture randomness or unknown external factors

::::
:::: {.column width=60%}

```graphviz
digraph EducationIncomeModel_EndoExo{
    rankdir=TD; // top-down layout
    splines=true;
    nodesep=0.8;
    ranksep=0.9;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    Education       [label="Education",       fillcolor="#ccebc5"];
    Income          [label="Income",          fillcolor="#b3cde3"];
    EconomicPolicy  [label="Economic Policy", fillcolor="#ccebc5"];
    Motivation      [label="Motivation",      fillcolor="#decbe4"];
    NaturalTalent   [label="Natural Talent",  fillcolor="#fed9a6"];
    Weather         [label="Weather",         fillcolor="#fed9a6"];
    News            [label="News",            fillcolor="#fed9a6"];

    // Vertical order (top to bottom)
    { rank=same; EconomicPolicy; }
    { rank=same; NaturalTalent; }
    { rank=same; Education; }
    { rank=same; Motivation; }
    { rank=same; Income; }
    { rank=same; Weather; }
    { rank=same; News; }

    // Causal links
    EconomicPolicy  -> Education;
    EconomicPolicy  -> Income;
    NaturalTalent   -> Education;
    NaturalTalent   -> Motivation;
    Motivation      -> Education;
    Motivation      -> Income;
    Education       -> Income;
    Weather         -> Income;
    News            -> Income;

    // Legend
    subgraph cluster_legend {
        label="Legend";
        fontsize=11;
        color="#dddddd";
        style="rounded";
        rankdir=TB; // vertical layout for legend

        key1 [label="Endogenous / Observed", shape=box, style="rounded,filled", fillcolor="#b3cde3"];
        key2 [label="Exogenous / Observed",  shape=box, style="rounded,filled", fillcolor="#ccebc5"];
        key3 [label="Endogenous / Unobserved", shape=box, style="rounded,filled", fillcolor="#decbe4"];
        key4 [label="Exogenous / Unobserved",  shape=box, style="rounded,filled", fillcolor="#fed9a6"];

        key1 -> key2 [style=invis];
        key2 -> key3 [style=invis];
        key3 -> key4 [style=invis];
    }
}
```

\begingroup \scriptsize
| **Variable Type**        | **Observability**   | **Example**          |
|---------------------------|---------------------|---------------------|
| Endogenous               | Observed            | Income               |
| Exogenous                | Observed            | Education            |
| Endogenous               | Unobserved          | Motivation           |
| Exogenous                | Unobserved          | Natural Talent       |
\endgroup

::::
:::

## Intervention

* Estimating Causal Effects

- **Goal**: Determine the causal effect of a treatment (aka intervention)
  variable $T$ on an outcome $Y$

::: columns
:::: {.column width=60%}
- **Example:**
    - $T$ = "takes drug"
    - $Y$ = "recovers"
    - $C$ = "overall health"
  - Healthier people may both take medicine and recover faster $\implies$ correlation
    without causation

- In **observational data**
  - Confounding variable $C$ affects both treatment $T$ and outcome $Y$
  - $C$ creates _spurious correlation_ between $T$ and $Y$
::::
:::: {.column width=35%}
```graphviz
digraph Observational {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    rankdir=TD;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];
    T [label="Treatment (T)", fillcolor="#A6C8F4"];
    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];

    // Edges
    C -> T [label="affects"];
    C -> Y [label="affects"];
    T -> Y [label="causal effect?"];
}
```
::::
:::

- **Problem**
  - There is a "backdoor path" from $Treatment \to Confounder \to Outcome$

* Frontdoor and Backdoor Paths: Intuition
::: columns
:::: {.column width=55%}

- **Backdoor path**
  - A backdoor path is any path from $T$ to $Y$ starting with an arrow into $T$
    - E.g., backdoor path: $T \leftarrow C \rightarrow Y$
  - Interpretation:
    - $C$ is a common cause of both $T$ and $Y$ and it confounds the relationship
      between $T$ and $Y$
    - Controlling for (condition) $C$ blocks the backdoor path, allowing
      identification of the causal effect of $T$ on $Y$
::::
:::: {.column width=40%}

```graphviz
digraph Observational {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    rankdir=TD;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    C [label="Confounder (C)",fillcolor="#FFD1A6"];
    T [label="Treatment (T)", fillcolor="#A6C8F4"];
    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];

    // Edges
    C -> T [label="Backdoor", color="#FA5050", fontcolor="#FA5050", penwidth=1.8];
    C -> Y [label="Backdoor", color="#FA5050", fontcolor="#FA5050", penwidth=1.8];
    T -> Y [label="Frontdoor", color="#1E8449", fontcolor="#1E8449", penwidth=2.5];
}
```

::::
:::

- **Frontdoor path**
  - A frontdoor path goes directly or indirectly from $T$ to $Y$ through
    mediators, following the direction of causal flow
    - E.g., front-door path: $T \rightarrow Y$
  - Interpretation:
    - This is the direct causal path of interest
    - No mediators here, so the front-door path is the direct causal effect of
      $T$ on $Y$

* Randomized Controlled Trials (RCTs)

- **Randomized Controlled Trial (RCT)** is an experimental study to assess the
  causal effect of an intervention or treatment
  - Determine whether an intervention causes an effect, not just associated with
    it
  - Eliminate selection bias and confounding variables through randomization

- **Key Components**
  - _Randomization:_ ensures groups are statistically equivalent at baseline
  - _Control Group:_ receives a placebo or standard treatment
  - _Blinding:_ participants and/or researchers do not know the assignment to
    avoid bias
  - _Outcome Measurement:_ pre-defined metrics assess the intervention's
    effect

- **Example**: testing a new drug
  - Treatment group receives the new drug
  - Control group receives a placebo
  - Compare recovery rates after a fixed period

- **Pros**
  - Provides clear causal inference due to randomization

- **Cons**
  - Expensive and time-consuming
  - Ethical or practical constraints may prevent randomization

* RCTs Solve the Problem of Confounders
::: columns
:::: {.column width=70%}
- In **observational data**
  - Confounding variable $C$ affects both treatment $T$ and outcome $Y$
  - $C$ creates _spurious correlation_ between $T$ and $Y$
::::
:::: {.column width=25%}
```graphviz
digraph Observational {
    splines=true;
    nodesep=1.0;
    ranksep=0.75;
    rankdir=TD;

    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Node styles
    C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];
    T [label="Treatment (T)", fillcolor="#A6C8F4"];
    Y [label="Outcome (Y)", fillcolor="#B2E2B2"];

    // Edges
    C -> T [label="affects"];
    C -> Y [label="affects"];
    T -> Y [label="causal effect?"];
}
```
::::
:::

::: columns
:::: {.column width=55%}
- In **experimental settings**
  - Randomization ($R$) breaks the link between $C$ and $T$
  - Everyone is assigned randomly, so nothing can influence both treatment and
    outcome
  - Now $T$ is independent of $C$: $T \perp C$
  - The only open path between $T$ and $Y$ is the causal path $T \rightarrow Y$
::::
:::: {.column width=40%}
```graphviz
digraph RCT {
  splines=true;
  nodesep=1.0;
  ranksep=0.75;
  rankdir=TD;

  node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Node styles
  R [label="Randomization", shape=diamond, fillcolor="#C6A6F4"];
  T [label="Treatment (T)", fillcolor="#A6C8F4"];
  Y [label="Outcome (Y)", fillcolor="#B2E2B2"];
  C [label="Confounder (C)", shape=ellipse, fillcolor="#FFD1A6"];

  // Edges
  R -> T [label="assigns"];
  T -> Y [label="causal effect"];
  C -> Y [label="affects"];
}
```
::::
:::

* Causal Graphs and Interventions
- Simply **observing correlations** between variables _does not reveal causality_
  - $\Pr(Y | T)$ confounds direct and indirect influences

- **Randomized Controlled Trials (RCTs)** provide the _gold standard_ for causal
  inference
  - Randomization breaks all back-door (confounding) paths
  - But RCTs are expensive, slow, or ethically impossible

- **Alternative solution**
  - Can we estimate the _causal effect_ from _observational data alone_?
  - Under _what conditions_ and using _which variables_?

- **Idea**: If we identify and condition on the right _confounders_, you can
  - Block spurious associations between $T$ and $Y$
  - Recover the true causal effect $P(Y | do(T))$

* Intervention in Structural Equations
- **Purpose of Structural Equations**
  - Capture causal mechanisms among variables
  - Go beyond correlations: predict the impact of external interventions

- **Effect of Intervention $do(X_j = x_j)$**
  - Original equation: $X_j = f_j(Parents(X_j), U_j)$
  - Modified by intervention: $X_j = x_j$ (fixed value)
  - "Mutilate" the causal network by _removing incoming edges_ to $X_j$
  - Use the modified structure to recompute the joint distribution of all variables

- **Intuition**
  - The $do$-operator enforces a variable's value externally, breaking causal dependencies
  - Enables reasoning about "what would happen if...?" scenarios

* Adjustment Formula in Causal Networks

- **Goal**
  - Estimate causal effect of intervention $do(X_j = x_{jk})$ on another variable $X_i$

- **The Adjustment Formula**
  - Derived from the post-intervention joint distribution:
    $$
    \Pr(X_i = x_i | do(X_j = x_{jk})) =
    \sum_{Parents(X_j)}
    \Pr(x_i | x_{jk}, Parents(X_j))
    \Pr(Parents(X_j))
    $$
  - The mechanism for $X_j$ is _removed_: it is treated as a fixed cause, not a
    random variable

- **Interpretation**
  - Computes a _weighted average_ of effects of $X_j$ and its parents on $X_i$
  - Weights come from prior probabilities of the parents’ values

- **Back-Door Criterion**
  - A set $Z$ is a valid adjustment set if it blocks _all back-door paths_ from $X_j$ to $X_i$
  - Ensures $X_i \perp \text{Parents}(X_j) \mid X_j, Z$

- **Why It Matters**
  - Enables causal inference from observational data
  - Estimate treatment and policy effects _without randomized trials_

* Intervention: Sprinkler Example
::: columns
:::: {.column width=70%}
- "Intervene" by turning the sprinkler on
  - I.e., in do-calculus $do(Sprinkler = T)$
  - Now the sprinkler variable $s$ is not dependent on whether it's a cloudy day
    $c$

- The structural equations after the intervention become:
  \begingroup \small
  \begin{align*}
  & C = f_C(U_C) \\
  & R = f_R(C, U_R) \\
  & S = True \\
  & W = f_W(R, S, U_W) \\
  & G = f_G(W, U_G) \\
  \end{align*}
  \endgroup

::::
:::: {.column width=25%}

```graphviz
digraph GrassBayes {
    rankdir=TB;
    nodesep=1.0;
    ranksep=0.75;
    splines=true;
    node [shape=box, style="rounded,filled", fontname="Helvetica", fontsize=12, penwidth=1.4];

    // Nodes with custom colors
    Cloudy        [label="Cloudy",        fillcolor="#c6dbef"];
    Rain          [label="Rain",          fillcolor="#c6dbef"];
    Sprinkler     [label="Sprinkler\n= True", fillcolor="#e5d5e7"];
    WetGrass      [label="WetGrass",      fillcolor="#c6dbef"];
    GreenerGrass  [label="GreenerGrass",  fillcolor="#c6dbef"];

    // Edges
    Cloudy -> Rain;
    Sprinkler -> WetGrass;
    Rain -> WetGrass;
    WetGrass -> GreenerGrass;
}
```

::::
:::
- $\Pr(s|c) = 1$ and $\Pr(w|r,s) = \Pr(w|r,s=T)$ and the joint probability
  becomes:
  $$
  \Pr(c, r, w, g | do(S=True)) = \Pr(c) \Pr(r|c) \Pr(w|r,s=True) \Pr(g|w)
  $$
- The variables that are affected are only the descendants of the manipulated
  variable $Sprinkler$

* Intervention vs Observation
- **Intervention** "breaks" normal causal link between $Weather$ and $Sprinkler$
  - Causal graph shows no influence of $Sprinkler$ on $Weather$

- Difference between and $Sprinkler=T$
  (observation)
  - Observing "sprinkler is on"
    - Condition on $| Sprinkler=T$
    - Less likely weather is cloudy
  - Turning on sprinkler:
    - $do(Sprinkler=T)$ (intervention) 
    - Weather unaffected, probability of cloudy unchanged

* Back Door
- A causal network can predict the effect of an intervention using the
  adjustment formula
- The problem is that it requires accurate knowledge of the conditional
  distributions of the model

- E.g., in the Sprinkler example, why does someone turn on the sprinkler? Maybe
  they check the weather, but how do they make their decision?

- Besides the direct route, we needs to take in account the "back door" route

* TBD
- We want to estimate the effect of $Sprinkler$ on $GreenerGrass$
  - We intervene and set $Sprinkler=True$

- Besides the direct route, we needs to take in account the "back door" route
  $Cloudy \to Rain$

```mermaid
flowchart TD
    S("Sprinkler=True"):::highlight --> WetGrass
    Cloudy --> Rain --> WetGrass
    WetGrass --> GreenerGrass

    %% Define a global rounded style
    classDef rounded fill:#f9f9f9,stroke:#15aebf,stroke-width:2px,rx:10,ry:10;

    %% Apply the style to all nodes by default
    class Cloudy,Rain,WetGrass,GreenerGrass rounded;

    classDef highlight fill:#F8F2DC,stroke:#333,stroke-width:2px;

    %% Highlight the backdoor path in red
    linkStyle 1 stroke:red,stroke-width:2px;
    linkStyle 2 stroke:red,stroke-width:2px;
    linkStyle 3 stroke:red,stroke-width:2px;
```

- If we knew the value of $Rain$, this back door path would be blocked and we
  could condition on $Rain$ instead of $Cloudy$

- In formal way we need to find a set of $Z$ variables such that $X_i$ is
  conditionally independent of $Parents(X_j)$ given $X_j$ and $Z$
  - TODO: ?

* Backdoor Path
- $Z$ blocks all backdoor paths from X to Y

- A backdoor path is any path from X to Y that starts with an arrow pointing
  into X. These paths create confounding relationships that can bias the
  estimate of X's effect on Y

* Backdoor Criteria: Condition
- Variables $Z$ satisfy the backdoor criterion for $X$ and $Y$ in a causal
  network if:
  1. No element of $Z$ is a descendant of $X$
     - $Z$ do not capture the effect of $X$ on $Y$ through any causal pathway
  2. $Z$ "blocks" every path from $X$ to $Y$

- The idea is to estimate the causal effect of $X$ on $Y$ without confounding
  relationships, by controlling variables $Z$ satisfying the backdoor criterion
- Then we can use non-experimental (i.e., observational data), assigning $X$
  randomly

```mermaid
flowchart TD
  X["Cause (X)"] --> Y["Effect (Y)"]
  U["Confounder (U)"] --> X["Cause (X)"]
  U["Confounder (U)"] --> Y["Effect (Y)"]
  Z["Backdoor Blocker (Z)"] --- U
```

## Type of Variables in Causal AI

* Mediator Variable
::: columns
:::: {.column width=50%}
- A **mediator variable** $M$ lies on the causal path between a treatment $X$ and an
  outcome $Y$
::::
:::: {.column width=45%}
  ```graphviz
  digraph CausalFlow {
    rankdir=LR;
    node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

    X [fillcolor=white, color=black];
    M [fillcolor="#FFCCCC", color=red, fontcolor=black];
    Y [fillcolor=white, color=black];

    X -> M;
    M -> Y;
  }
  ```
::::
:::

* Mediator Variable: Example

- Does a Training Program increase Employee Productivity?

- Training Program may not directly increase productivity
  - It might increase job satisfaction (mediator), leading to greater productivity

```graphviz
digraph CausalFlow {
  rankdir=LR;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  X [label="Training program", fillcolor=white, color=black];
  M [label="Job satisfaction", fillcolor="#FFCCCC", color=red, fontcolor=black];
  Y [label="Employee productivity", fillcolor=white, color=black];

  X -> M;
  M -> Y;
}
```

* Moderator variable
::: columns
:::: {.column width=50%}
- **Moderator variable** affects the strength or direction between two other
  variables
::::
:::: {.column width=45%}
```graphviz
digraph ModeratorModel {
  // General settings
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  X [label="Independent\nVariable", pos="0,1!"];
  Y [label="Dependent\nVariable", pos="2,1!"];
  M [label="Moderator\nVariable", pos="1,0!", fillcolor="#FFCCCC", style="filled,rounded,bold", color=red];
  XY [label="", shape=point, width=0.01, style=invis, pos="1,1!"];

  // Edges
  X -> XY [arrowhead=none];
  XY -> Y;
  M -> XY;

  // Use neato for fixed positioning
  layout=neato;
}
```
::::
:::

::: columns
:::: {.column width=50%}
- **E.g.,**
  - Study the relationship between stress $X$ and job performance $Y$
  - The level of social support an individual receives $M$ could be a moderator
    - If social support is high, the negative effect of stress on job
      performance might be weaker
    - If social support is low, the negative effect might be stronger
::::
:::: {.column width=45%}
```graphviz
digraph ModeratorModel {
  // General settings
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Nodes
  X [label="Stress", pos="0,1!"];
  Y [label="Job\nPerformance", pos="2,1!"];
  M [label="Social Support", pos="1,0!", fillcolor="#FFCCCC", style="filled,rounded,bold", color=red];
  XY [label="", shape=circle, width=0.01, style=invis, pos="1,1!"];

  // Edges
  X -> XY [arrowhead=none];
  XY -> Y;
  M -> XY;

  // Use neato for fixed positioning
  layout=neato;
}
```
::::
:::


* Confounder Variable
::: columns
:::: {.column width=70%}

- A **confounder**
  - Is a variable in a causal graph that influences multiple variables
  - Can lead to spurious associations

::::
:::: {.column width=30%}
```graphviz
digraph CausalTriangle {
  rankdir=TB;
  splines=true;
  nodesep=1.0;
  ranksep=0.75;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  Confounder [label="Confounder", fillcolor="#b3cde3"];
  Treatment  [label="Treatment", fillcolor="#ccebc5"];
  Outcome    [label="Outcome", fillcolor="#decbe4"];

  Confounder -> Treatment;
  Confounder -> Outcome;
  Treatment  -> Outcome;

  // Align Treatment and Outcome on the same horizontal level
  {rank=same; Treatment; Outcome;}
}
```
::::
:::

* Confounder Variable: Example

::: columns
:::: {.column width=25%}
```tikz
\begin{axis}[
    xlabel={Ice cream sales},
    ylabel={Drownings},
    width=10cm,
    height=6cm,
    axis lines=left,
    xtick=\empty,
    ytick=\empty,
    enlargelimits=true,
    line width=1.0pt
]
\addplot[
    smooth,
    thick,
    color=blue!50,
] coordinates {
    (0,0)
    (1,1)
    (2,3)
    (3,3.5)
    (4,5)
    (5,6.5)
    (6,8)
};
\end{axis}
```
::::
:::: {.column width=35%}
- "Eating Ice Cream" and "Drowning" are associated
- There is no cause-effect, since "Summer" is a confounder
::::
:::: {.column width=25%}
```graphviz
digraph CausalTriangle {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  Summer [label="Summer"];
  IceCream [label="Eating Ice Cream"];
  Drowning [label="Drowning"];

  Summer -> IceCream;
  Summer -> Drowning;
  IceCream -> Drowning;
  Drowning -> IceCream;

  {rank=same; IceCream; Drowning;}
}
```
::::
:::

* Collider
::: columns
:::: {.column width=60%}
- A **collider** is a variable $A$ with incoming edges from variables $B,
  C$ in a causal DAG (i.e., influenced by multiple variables)
- A collider complicates understanding relationships between variables $B, C$
  and those it influences, $X$
::::
:::: {.column width=35%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  B [label="B (Cause 1)", fillcolor="#E6E6FF"];
  C [label="C (Cause 2)", fillcolor="#E6E6FF"];
  A [label="A (Collider)", fillcolor="#FFCCCC", color=red, style="filled,rounded,bold"];
  X [label="X (Effect)", fillcolor="#E6E6FF"];

  B -> A;
  C -> A;
  A -> X;

  // Align B and C on the same level
  {rank=same; B; C;}
}
```
::::
:::

* Collider: Examples
::: columns
:::: {.column width=60%}
- Study the relationship between $Exercise$ and $Heart Disease$
  - $Diet$ and $Exercise$ influence $Body Weight$
  - $Body Weight$ influences $Heart Disease$
  - $Body Weight$ is a collider
::::
:::: {.column width=35%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  E [label="Exercise", fillcolor="#E6E6FF", fontcolor=black];
  D [label="Diet", fillcolor="#E6E6FF", fontcolor=black];
  W [label="Body Weight", fillcolor="#FFCCCC", color=red, fontcolor=black, style="filled,rounded,bold"];
  H [label="Heart Disease", fillcolor="#E6E6FF", fontcolor=black];

  E -> W;
  D -> W;
  W -> H;

  {rank=same; E; D;}
}
```
::::
:::

* Collider Bias
- Aka "Berkson's paradox"

- Conditioning on a collider can introduce a spurious association between its
  parents by _"opening a path that is blocked"_

::: columns
:::: {.column width=70%}
- Consider the variables:
  - $Diet$ (D)
  - $Exercise$ (E)
  - $BodyWeight$ (W)
  - $HeartDisease$ (D)

- **Without conditioning on $W$**
  - $E$ and $D$ are independent
    - E.g., knowing someone's exercise level $E$ doesn't give information about
      diet $D$, and vice versa
  - The collider $W$ blocks any association between $E$ and $D$

- **After conditioning on $W$**
  - E.g., looking for individuals with specific body weight
  - You introduce a dependency between $E$ and $D$
  - Since $W$ is fixed, any change in $E$ must be balanced by a change in $D$ to
    maintain the same body weight, inducing a spurious correlation between $E$
    and $D$
::::
:::: {.column width=25%}
```graphviz
digraph ColliderDiagram {
  layout=dot;
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  E [label="Exercise (E)", fillcolor="#E6E6FF", fontcolor=black];
  D [label="Diet (D)", fillcolor="#E6E6FF", fontcolor=black];
  W [label="Body Weight (W)", fillcolor="#FFCCCC", color=red, fontcolor=black, style="filled,rounded,bold"];
  H [label="Heart Disease (H)", fillcolor="#E6E6FF", fontcolor=black];

  E -> W;
  D -> W;
  W -> H;

  {rank=same; E; D;}
}
```
::::
:::

// TODO: Add an example in the tutorial

## Paths

* Fork Structure
::: columns
:::: {.column width=70%}
- A **fork** occurs when a single variable causally influences two or more
  variables
  - Formally: $X \rightarrow C$ and $X \rightarrow D$
- $X$ is a common cause (confounder) of $C$ and $D$
- Forks induce statistical dependence between $C$ and $D$
  - Even if $C$ and $D$ are not causally linked
- Conditioning on $X$ blocks the path and removes spurious correlation
::::
:::: {.column width=25%}
```graphviz[width=70%]
digraph CausalDAG {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  X
  C
  D

  X -> C;
  X -> D;

  {rank=same; C; D;}
}
```
::::
:::

\vspace{1cm}

::: columns
:::: {.column width=70%}
- Example:
  - Lifestyle factors as confounders
  - $Lifestyle$ affects both $Weight$ and $BloodPressure$
  - These outcomes may appear correlated due to shared cause
::::
:::: {.column width=25%}
```graphviz[width=70%]
digraph CausalDAG {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  Lifestyle [label="Lifestyle"];
  Weight [label="Weight"];
  BP [label="Blood\nPressure"];

  Lifestyle -> Weight;
  Lifestyle -> BP;

  {rank=same; Weight; BP;}
}
```
::::
:::

* Inverted Fork

::: columns
:::: {.column width=70%}
- An **inverted fork** occurs when two or more arrows converge on a common node
  - Also known as a **collider**
- Colliders block associations unless the collider or its descendants are
  conditioned on
- Conditioning on a collider "opens" a path, inducing spurious correlations
  - This is the basis of selection bias
::::
:::: {.column width=25%}
```graphviz[width=70%]
digraph ColliderExample {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  X [label="X"];
  Y [label="Y"];
  Z [label="Z"];

  X -> Z;
  Y -> Z;

  {rank=same; X; Y;}
}
```
::::
:::

::: columns
:::: {.column width=70%}
- Example:
  - Sales influenced by multiple independent causes
  - $MarketingSpend$ and $ProductQuality$ both influence $Sales$
  - Conditioning on $Sales$ can induce false dependence between $MarketingSpend$
    and $ProductQuality$
::::
:::: {.column width=25%}
```graphviz[width=70%]
digraph ColliderExample {
  rankdir=TB;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  Marketing [label="Marketing Spend"];
  Quality [label="Product Quality"];
  Sales [label="Sales"];

  Marketing -> Sales;
  Quality -> Sales;

  {rank=same; Marketing; Quality;}
}
```
::::
:::

* Path connecting unobserved variables
- **Unobserved variables** affect the model but we don't have a direct measure of
  it

::: columns
:::: {.column width=60%}
- E.g., consider the causal DAG
  - A retailer does market research, expecting $Price$ to influence $Sales$ in a
    predictable way
  - A retailer sets the $Price$ of a new product based on market research
  - The retailer can observe and measure $Behavior$, e.g.,
    - Discounts
    - Promotional campaign
  - There are unobserved vars that influence the model, e.g.,
    - Social media buzz
    - Word-of-mouth recommendation
::::
:::: {.column width=35%}
```graphviz
digraph CausalDAG {
  layout=neato;
  node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];

  // Node positions
  U [label="Unobserved\nVariables", pos="2,1!"];
  M [label="Price", pos="1,1!"];
  X [label="Behavior", pos="0,0!"];
  Y [label="Sales", pos="2,0!"];

  M -> X;
  M -> Y;
  X -> Y;
  U -> Y [style=dashed];

  // Prevent layout engine from moving nodes
  edge [splines=true];
}
```
::::
:::

* Front-door Paths in Causal Inference
- A front-door path reveals causal influence through an observable mediator
  - The causal effect flows: $A \rightarrow P \rightarrow S$
- Requirements for identifiability:
  - All confounders of $A \rightarrow P$ and $P \rightarrow S$ are observed and
    controlled
  - There are no back-door paths from $A$ to $S$ through unobserved variables
- Enables causal estimation when back-door adjustment is infeasible
- Example:
  - Advertising impacts sales through customer perception of price
  - $A$: Advertising, $P$: Price perception, $S$: Sales
- Pearl's front-door criterion provides a formal method for adjustment
  - Estimate $P(P|A)$, $P(S|P,A)$, and $P(A)$ from data to compute causal effect

```graphviz[width=50%]
digraph CausalDAG {
  rankdir=TB;
  node [shape=box, style=rounded];

  A [label="Advertising"];
  P [label="Price"];
  S [label="Sales"];

  A -> P;
  P -> S;
  A -> S [style=dotted, label="confounding path"];

  {rank=same; P; S;}
}
```

// RESUME

* Back-Door Paths
- A company wants to understand the causal effect of price on sales

  ```graphviz[width=50%]
  digraph CausalDAG {
    rankdir=TB;
    node [shape=box, style=rounded];

    Price [label="Price"];
    Sales [label="Sales"];
    AdvSpend [label="Advertisement Spend"];

    Price -> Sales;
    AdvSpend -> Price;
    AdvSpend -> Sales;

    {rank=same; Price; Sales;}
  }
  ```

- `Price` $\rightarrow$ `Sales` is the front-door path

- A confounder is `Advertising spend` since it can affect both:
  - The price the company can set (e.g., the cost increases to cover
    advertisement costs and the product is perceived as more valuable)
  - The sales (directly)

- The back-door path goes from `Sales` to `Price` via `Advertising spend`

- The company needs to control for `Advertising spend` to estimate the causal
  effect of `Price` on `Sales` by:
  - Using `Advertising spend` as covariate in the regression
  - Designing experiment holding `Advertising spend` constant or randomized

* Frontdoor and Backdoor Paths

- Question: _Will increasing our customer satisfaction increase our sales?_

::: columns
:::: {.column width=45%}
- Assume that the Causal DAG is
  ```graphviz
  digraph CausalGraph {
    rankdir=TB;
    bgcolor="transparent";

    node [shape=box, style="rounded,filled", fontname="Helvetica"];

    // Define nodes
    PQ [label="Product Quality", fillcolor="#ffcccc", color="#ff0000", penwidth=2];
    CS [label="Cust Satisfaction", fillcolor="white"];
    S [label="Sales", fillcolor="white"];

    // Force PQ on top by putting CS and S in the same rank
    { rank = same; CS; S }

    // Define edges
    PQ -> CS;
    PQ -> S;
    CS -> S;
  }
  ```
- **Front-door path** (i.e., a direct causal relationship):
  $CustomerSatisfaction \rightarrow Sales$
::::
:::: {.column width=45%}
- **Backdoor path**: $ProductQuality$ is a common cause (confounder) of
  both $CustomerSatisfaction$ and $Sales$
- To analyze the relationship between customer satisfaction and sales, we need
  to:
  - Control for $ProductQuality$ to close the backdoor path
  - Eliminate the confounding effect
- In reality there are more confounding effects (e.g., price)
::::
:::


* Building a DAG
- **Causal models** visually represent complex environments and relationships
- Nodes are like "nouns" in the model:
  - E.g., "price", "sales", "revenue", "birth weight", "gestation period"
  - Variables can be endogenous/exogenous and observed/unobserved
  - Complex relationships between variables:
    - Parents, children (direct relationships)
    - Descendants, ancestors (along the path)
    - Neighbors
- **Iterative Refinement**:
  - Models are continuously updated with new variables and insights
- **Modeling as a Communication Tool**:
  - A shared language that bridges gaps between technical and non-technical team
    members
- **Unobservable Variables**:
  - Supports inclusion of variables not empirically observed but known to exist
  - E.g., trust or competitor activity can be modeled despite lack of direct
    data

* Heart Attack: Example
- What's the relationship between stress and heart attacks?
  - Stress is the treatment
  - Heart attack is the outcome
  - Stress is not a direct cause of heart attack
    - E.g., a stressed person tend to have poor eating habits

  ```graphviz
  digraph CausalGraph {
    rankdir=LR;
    node [shape=box, style=rounded, fontname="Helvetica", fontsize=12];

    Stress -> Diet;
    Stress -> Exercise;
    Diet -> Heart_Attacks;
    Exercise -> Heart_Attacks;
    Genetic -> Heart_Attacks;
  }
  ```

* Weights
- Weights can be assigned to paths to represent the strength of the causal
  relationship
  - Weights can be estimated using statistical methods
- Sign represents the direction

  ```graphviz
  digraph RiskFactors {
    rankdir=LR;
    node [shape=box, style=rounded, fontname="Helvetica", fontsize=12];

    Age -> Heart_Disease [label="+0.8"];
    Gender -> Heart_Disease [label="+0.3"];
    Blood_Pressure -> Heart_Disease [label="+0.7"];
    Cholesterol -> Heart_Disease [label="+0.5"];
    Exercise -> Heart_Disease [label="-0.6"];
  }
  ```

* Counterfactuals

- A **counterfactual** describes what would have happened under a different
  scenario
  - _"What would the outcome have been _if_ X had been different?"_
  - _"If kangaroos had no tails, they would topple over"_
  - _"What if we had two suppliers of our product, rather than one? Would we
    have more sales?"_
  - _"Would customers be more satisfied if we could ship products in one week,
    rather than three weeks?"_

- **Causal reasoning**:
  - Goes beyond correlation and association
  - Requires a causal model (like an SCM) to simulate alternate realities
  - E.g.,
    - Actual: A student received tutoring and scored 85%
    - Counterfactual: What if the student didn't receive tutoring?
    - Causal model estimates the alternative outcome (e.g., 70%)

- **Challenges**:
  - Requires strong assumptions and accurate models
  - Difficult to validate directly since counterfactuals are unobservable


# ##############################################################################
# Business Processes Around Data Modeling
# ##############################################################################

## #############################################################################
## Modeling Processes
## #############################################################################

* Digital Transformation

- Integration of Digital Technology
  - Embed digital tools (AI, cloud, IoT, automation) into all business areas to
    enhance efficiency and value delivery
- Cultural & Organizational Change
  - Encourage innovation, agility, and a data-driven mindset to adapt to new
    digital workflows and business models
- Customer-Centric Approach
  - Use digital solutions (e.g., personalized experiences, AI-driven insights)
    to enhance customer engagement and satisfaction
- Process Automation & Optimization
  - Streamline operations through AI, robots, and analytics to reduce costs and
    improve decision-making
- Data-Driven Decision Making
  - Leverage big data, machine learning, and real-time analytics to make
    smarter, faster, and more strategic business decisions

* Causal Modeling Process

- The overall modeling process looks like:

```graphviz
digraph CausalFlowGrid {
  layout=neato;
  node [shape=box, style=rounded, fontname="Helvetica", fontsize=12, width=1.4, height=0.6, fixedsize=true];

  // Top row
  BusinessGoals   [label="Business Goals", pos="0,2!"];
  UseCases        [label="Use Cases", pos="2,2!"];
  CausalModels    [label="Causal Models", pos="4,2!"];

  // Middle row
  Results         [label="Key Results/\nTakeaways", pos="0,1!"];
  Placeholder     [label="", pos="2,1!", style=invis]; // Center of grid
  DataSources     [label="Data Sources", pos="4,1!"];

  // Bottom row
  Interventions   [label="Interventions/\nCounterfactuals", pos="0,0!"];
  Algorithms      [label="Algorithms/\nLibraries", pos="2,0!"];
  KnowledgeModels [label="Knowledge\nModels", pos="4,0!"];

  // Edges connecting flow
  BusinessGoals -> UseCases;
  UseCases -> CausalModels;
  CausalModels -> DataSources;
  DataSources -> KnowledgeModels;
  KnowledgeModels -> Algorithms;
  Algorithms -> Interventions;
  Interventions -> Results;
  Results -> BusinessGoals;

  edge [splines=true];
}
```

* Step 1: What Are the Intended Outcomes?
- What is the process/environment we are interested in analyzing?

- What will happen if a course of action is (or not) taken?
- What outcomes are positive, negative, unacceptable, optimal?
- What are the possible / feasible interventions?
- What confounding factors might be correlated with outcomes and treatments?
- What factors exist but we cannot accurately measure?
- What related data sets can be combine / leverage?

* Step 2: What Are the Proposed Interventions?
- We will make reference to a use case of customer marketing

- Can we introduce a new product?
- Should we buy one or more competitors?
- Does bundling multiple products improve sales?
- Does bundling multiple products inhibit long-term sales?
- Should advertisement focus on quality of our product vs other options?
- Should we divest the product line?
- Should we discontinue the product?
- Should we add more variations of the same product?

* Marketing example: price intervention
::: columns
:::: column
- Assume price is our intervention and the Causal DAG is:
  ```graphviz[height=50%]
  digraph CausalDiagram {
    rankdir=TB;
    node [shape=box, style="rounded,filled", fillcolor=white, color=black];

    M [label="M = Price"];
    X [label="X = Behavior"];
    Y [label="Y = Outcome"];

    M -> X;
    M -> Y;
    X -> Y;
  }
  ```
::::
:::: column

- What happens to sales when we change the price often?
- What pricing interventions are optimal?
  - Should we increase the price and how much?
  - Should we decrease the price and how much?
  - Should price change in one-time or over time?
- Should we adopt a dynamic pricing model?
- Should we develop individual pricing model for each customer?

::::
:::

* Step 3: What are the confounding factors?
- There can be a variable $W$ that affects both $X$ and $Y$
  ```graphviz[width=50%]
  digraph CausalTriangle {
    rankdir=TB;
    node [shape=box, style=rounded, fontname="Helvetica", fontsize=12];

    Confounder [label="W (Confounder)"];
    Treatment  [label="X (Treatment)"];
    Outcome    [label="Y (Outcome)"];

    Confounder -> Treatment;
    Confounder -> Outcome;
    Treatment  -> Outcome;

    // Align Treatment and Outcome on the same horizontal level
    {rank=same; Treatment; Outcome;}
  }
  ```
- E.g.,
  - Competitive offers
  - Distance to store
  - Amount of product
  - Time to consume product

- A confounder can:
  - Make it difficult to understand the relationship between variables
  - Mute or inflate a relationship

* Marketing Example: Effect of Confounder
- E.g.,
  - Intervention = a marketing campaign to sell winter jackets
  - A confounding variable can be "running the campaign in the middle of the
    winter, after customers have already purchased their jackets"
  - A confounding var can be "a warm winter"

  ```graphviz[width=50%]
  digraph CausalDiagram {
    rankdir=TB;
    node [shape=box, style="rounded,filled"];

    W [label="W = Confounder"];
    M [label="M = Price"];
    X [label="X = Behavior"];
    Y [label="Y = Outcome"];

    W -> X;
    W -> Y;
    M -> X;
    M -> Y;
    X -> Y;
  }
  ```

* Step 4: What Are the Factors Creating the Effects and Changes?

- Total causal effect
  - Effect of all factors in the environment or model that modify the outcome
- Direct effect
  - Effect introduced through an intervention
- Indirect effect
  - Effect introduced by environment or it is a byproduct of the intervention in
    a way that was not planned

* Step 5: Build Causal DAG

- Causal models
  - Simplify complex systems without losing key relationships
  - Focus on essential variables and their interactions
- Visual models (e.g., DAGs) help abstract complexity into interpretable formats
  - Highlight direction and strength of influence between variables
- Simplicity
  - Aids communication between technical and non-technical stakeholders
  - Promotes shared understanding and collaborative refinement
  - Reduces cognitive overload by excluding irrelevant details or noise
  - Guides data collection by identifying the most impactful variables
  - Supports hypothesis testing through counterfactual and intervention
    scenarios
- Balance is key
  - Too much simplicity loses insight
  - Too much complexity loses clarity

* Step 7: Data Acquisition and Integration
- You can use the data collected for correlation-based ML

- Data collection can be done specifically for causal AI
  - Treating, conditioning, transforming data

* Step 8: Model Modification
- Once the DAG is designed, use software packages to build models
  - Refine the initial DAG and causal model to reduce bias and improve
    reliability
  - Clarify variables as confounders, mediators, or outcomes
- Avoid common pitfalls:
  - Do not control for mediators or effects, which can distort results
  - Control for direct and indirect confounders to prevent biased estimates
- Implementation tools:
  - Use libraries to operationalize models
  - Test models against technical and business objectives
- D-separation:
  - Identifies conditional independence relationships in a DAG
  - Determines necessary controls to isolate causal effects
  - Based on Judea Pearl's definition: independence = separation in the graph
  - Prevent unintentional inclusion of bias
  - Ensure causal assumptions align with data and domain logic
  - Improve model interpretability and predictive power
- Goal: Ensure final models are technically valid and business-relevant before
  proceeding to data transformation and testing stages

* Step 10: Data Transformation

- Prepare the data to match the refined causal model
  - Clean, normalize, and align data with model assumptions

- Transformations include:
  - Mapping observed variables to nodes in the DAG
  - Encoding categorical variables appropriately
  - Handling missing or unobserved data (e.g., imputation or exclusion)
  - Normalizing or scaling values to align with model expectations
- Control for bias and confounding:
  - Apply methods like propensity score matching or stratification
  - Exclude or adjust for variables that introduce bias per d-separation
    insights

- Goal: Ensure the data structure supports causal estimation
  - Consistent with assumptions made in model refinement
  - Aligned with theoretical model
  - Fit for downstream tasks like estimation, inference, and simulation

* Step 11: Preparing for Deployment in Business

- Operationalize the causal model within a business context
  - Transition from experimentation to integration with decision-making
    processes
  - Validate the model against real-world business data and outcomes
  - Ensure stakeholders understand and trust the causal logic and assumptions
- Model packaging:
  - Develop user-friendly interfaces or dashboards for business users
  - Automate data pipelines for timely updates and monitoring
  - Embed the model within decision-support tools or policy engines
- Governance and monitoring:
  - Establish metrics for performance tracking and drift detection
  - Create feedback loops to refine and improve models post-deployment
- Documentation and training:
  - Provide clear model documentation for auditors and users
  - Train stakeholders on interpreting causal results and making informed
    decisions
- Goal: A deployable causal AI solution that supports strategic decisions and
  delivers measurable business value

## #############################################################################
## Roles
## #############################################################################

* Why ML / AI Projects Fail?
- AI projects fail because they approach problems only from a ML perspective
  - Data scientists:
    - Use data to create models
    - Work in isolation from business users and internal data teams
  - Black-box models unable to produce solutions to real-world problems

* How to Make ML / AI Project Succeed

1. Create a hybrid team
   - Organizations are complex in structure and offerings
   - A single group lacks the knowledge / skills to tackle difficult problems
   - Need an hybrid team:
     - Represents all aspects of the business problem
     - Uses a collaborative framework
     - Communicates with a single language (e.g., through DAGs)
     - Team size depends on company size and project complexity
2. Meet regularly to ensure project continuity
3. Find an executive sponsor for the project
   - Someone who understands the project's goals and potential
4. Initial pilot
   - Small team for a targeted problem
   - Demonstrate the merit of the AI approach

* Roles in Hybrid Teams

\begingroup \small

| Role                   | Responsibilities                            |
| ---------------------- | ------------------------------------------- |
| Business Strategists   | Align modeling with business goals          |
|                        | Sponsor projects                            |
|                        | Communicate insights to stakeholders        |
| Subject-Matter Experts | Provide domain expertise                    |
|                        | Identify relevant variables and assumptions |
|                        | Validate DAGs                               |
| Data Experts           | Source and clean data                       |
|                        | Map data to model variables                 |
|                        | Handle missing values                       |
| Data Scientists        | Construct and validate DAGs                 |
|                        | Apply causal inference methods              |
|                        | Simulate decisions                          |
| Software Developers    | Build tools and interfaces                  |
|                        | Create data pipelines                       |
| IT Professionals       | Provide infrastructure and governance       |
|                        | Ensure model execution                      |
|                        | Integrate with enterprise systems           |
| Project Managers       | Coordinate collaboration and timelines      |
|                        | Manage documentation                        |
|                        | Ensure alignment with strategic goals       |

\endgroup

* Steps for a Hybrid Team Project

- **Establish a Phased, Collaborative Approach**
  - Align strategic goals with technical efforts
  - Emphasize early stakeholder engagement and shared ownership
- **Strategic Kickoff Meeting**
  - Unite business, technical, and operational roles
  - Clarify problems, outcomes, responsibilities, success metrics
- **Define Team Goals**
  - Use SMART objectives aligned with business strategy
  - Focus on outcomes and supported decisions, not tools
- **Target a Project**
  - Choose a bounded, feasible, high-value use case
  - Prioritize early wins for trust and momentum
- **Define the Hypothesis**
  - Translate business problems into testable causal assumptions
  - Build a preliminary DAG with experts' input
- **Incremental Model Development**
  - Build the model in small, reviewable stages
  - Iterate with regular feedback, refining scope and variables
- **Embrace Iteration and Continuous Refinement**
  - Keep progress collaborative and aligned with business needs
  - Add complexity gradually to manage risk and enhance understanding

* The Importance of Explainability

- Managers rely on AI systems to automate decision-making
  - Decisions rely on complex algorithms and data
- Understanding AI-based models is growing in importance
  - How do ML models make decisions?
  - How can they be trusted?
  - Are they biased?
- Management often faces demands to prove code validity
  - Loss of trust, regulation violations, fines, additional development costs,
    lawsuits
  - E.g., a false negative in a medical screening for cancer
- Well-designed AI systems must foster trust, transparency, and user confidence

- **Humans in the loop**
  - AI systems lack true reasoning and contextual understanding
  - Human involvement ensures interpretation and context are considered

* Techniques for Interpretability
- Local Interpretable Model-agnostic Explanations (LIME)
  - Focuses on a single prediction (local fidelity)
  - Approximates the model locally with an interpretable model
  - Perturbs input data and observes changes in predictions
- Partial Dependence Plots (PDP)
  - Show the marginal effect of one or two features on the predicted outcome
  - Vary the value of one feature while keeping others constant
  - Plot feature values against the average predicted outcome
- Individual Conditional Expectation (ICE)
  - Show the relationship between a feature and the prediction for individual
    instances

- SHapley Additive exPlanations (SHAP)
  - Quantify the contribution of each feature to a specific prediction

* Causal AI in Interpretable AI
- Causal AI helps understand causes, effects, and potential solutions
  - Uses causal graphical models to present variables, relationships, and
    strengths
  - Counterfactual analysis predicts outcomes of different actions or policies
    before deployment
  - Model output is understandable to humans and non-experts
  - Removing confounding variables prevents skewed causal estimates due to
    hidden influences
  - Hybrid teams (technical + domain experts) enhance context awareness and
    reduce blind spots

//// # Chap 7: Tools, practices, and techniques (p. 168)
//
//* The Iterative Causal AI Pipeline
//
//- Similar to the DevOps infinity loop, the causal AI pipeline has 8 stages,
//  which are circular and iterative
//
//// TODO: Add picture
//
//1. Define business objectives
//   - Identify goals and key performance indicators (KPI)
//2. Model development
//   - Establish causal relationships among variables
//   - Develop causal AI model using graphical models, potential outcomes,
//     do-calculus
//3. Data identification and collection
//   - Gather data with causal relationships and potential confounders
//4. Model validation
//   - Test and validate model to ensure it provides accurate causal insights,
//     e.g.,
//     - Measure performance against KPIs
//     - Assess model generalizability
//5. Deployment / production
//   - Integrate validated AI model into existing systems for decision-making
//6. Monitor and evaluate
//   - Track performance of deployed model
//   - Assess impact on decision-making process
//7. Update and iterate
//   - Refine model based on real-time performance evaluation
//8. Continuous learning
//   - Create culture of learning and improvement
//   - Share knowledge and best practices for causal AI
//   - Stay updated on latest advancements in causal AI
//
//* Define Business Objectives
//- Use one of the systems
//  - SMART goals
//  - MoSCoW method
//  - Kano model
//
//- Goals
//  - Establish a shared understanding of the project's goals among stakeholders,
//    business leaders, data scientists, and exports
//  - Enhances clarity, focus, and motivation
//  - Improves performance tracking and outcome evaluation
//  - Facilitates effective communication and collaboration
//
//* SMART Goals
//- SMART is an acronym for Specific, Measurable, Achievable, Relevant, Time-bound
//- **Specific**
//  - Clearly defines what is to be achieved
//  - E.g., "Increase website traffic by 15%"
//- **Measurable**
//  - Includes criteria to track progress and success
//  - E.g., "Track visits using Google Analytics"
//- **Achievable**
//  - Realistic based on resources and constraints
//  - E.g., "Based on historical growth rates, 15% is attainable"
//- **Relevant**
//  - Aligns with broader organizational or personal objectives
//  - E.g., "Traffic increase supports lead generation goals"
//- **Time-bound**
//  - Specifies a deadline or timeframe
//  - E.g., "Achieve by end of Q3"
//
//* Moscow Method
//- Framework for prioritization of features to:
//  - Focus on critical features
//  - Ensure stakeholders are aligned on priorities
//  - Support agile and iterative development by delivering value in phases
//
//\begingroup \scriptsize
//
//| Priority    | Description                                   | Example                                         |
//| ----------- | --------------------------------------------- | ----------------------------------------------- |
//| Must-have   | Essential requirements                        | A login system for a banking app                |
//| Should-have | Important but not critical features           | Multi-factor authentication to enhance security |
//| Could-have  | Enhance user experience but are not essential | Dark mode for a mobile app                      |
//| Won't-have  | Excluded features due to low priority         | AI-powered chat assistant                       |
//
//\endgroup
//
//* Kano Model
//- Framework for product development and customer satisfaction analysis to
//  prioritize features
//
//\begingroup \scriptsize
//
//| Category         | Description                                              | Examples                           |
//| ---------------- | -------------------------------------------------------- | ---------------------------------- |
//| Must-have        | Expected features; absence causes dissatisfaction        | Keyboard and screen on a laptop    |
//|                  |                                                          | Calling and texting on a cellphone |
//| Performance      | The better, the more satisfied customers are             | Faster battery charging            |
//|                  |                                                          | Longer battery life                |
//| Delighters       | Unexpected features that wow customers and build loyalty | Free cloud storage with a laptop   |
//|                  |                                                          | Novel camera                       |
//| Indifferent      | Features that do not affect customer satisfaction        | 5 power adapter options for laptop |
//|                  |                                                          | Color of phone internals           |
//| Reverse features | Features liked by some but disliked by others            | Touchscreen instead of touchpad    |
//|                  |                                                          | Excessive automation               |
//
//\endgroup
//
//* Decision Matrix
//- Tool to evaluate options based on criteria for objective comparison
//
//- **Goals**
//  - Eliminate biases by quantifying decisions
//  - Clarify priorities
//  - Enable comparison between options
//
//- **Approach**
//  1. List alternatives
//     - E.g., laptop A, laptop B
//  2. Define criteria
//     - E.g., price, performance, battery life
//  3. Assign weight to each criterion
//  4. Score each option
//     - Rate 1-5 for each criterion
//  5. Calculate weighted score
//     - Weighted sum of scores for each alternative
//  6. Compare totals
//     - E.g., choose the alternative with the highest score
//
//* Key Performance Indicators (KPIs): General Overview
//
//- **What are KPIs?**
//  - Quantitative metrics that reflect how well an individual, team, or
//    organization is achieving key objectives
//
//- **Purpose of KPIs**
//  - Provide focus for strategic and operational improvement
//  - Serve as benchmarks for progress and performance evaluation
//
//- **Categories of KPIs**
//  - **Financial KPIs**: Revenue, profit margin, cost per acquisition
//  - **Customer KPIs**: Customer satisfaction, churn rate
//  - **Process KPIs**: Efficiency, cycle time, error rate
//  - **People KPIs**: Employee engagement, turnover rate, training hours
//
//- **Characteristics of Effective KPIs**
//  - Specific and clearly defined
//  - Measurable with reliable data
//  - Achievable and realistic
//  - Relevant to the business goals
//  - Time-bound for periodic evaluation
//
//- **Developing KPIs**
//  - Align with strategic goals
//  - Involve key stakeholders in the design process
//  - Establish baseline and targets
//
//- **Using KPIs**
//  - Monitor performance regularly
//  - Identify trends and root causes
//  - Inform decisions and prioritize actions
//
//- **Continuous Improvement**
//  - Review and refine KPIs as objectives and environments evolve
//
//* The Importance of Synthetic Data in Causal AI
//
//- Synthetic data is artificially generated to simulate real-world data
//  properties
//  - Created using algorithms or simulations based on real data distributions
//
//- **Key Benefits**
//  - Enhances data availability when real data is limited, biased, or sensitive
//  - Enables experimentation without risking privacy breaches
//  - Protects identities by not using real personal data
//  - Helps comply with data privacy regulations like GDPR and HIPAA
//  - Allows rebalancing of underrepresented groups or scenarios
//  - Facilitates fairer model training and evaluation
//
//- **Use in Causal Modeling**
//  - Simulates counterfactual scenarios to test causal hypotheses
//  - Assists in validating causal assumptions and interventions
//
//- **Challenges**
//  - Ensuring synthetic data accurately reflects key patterns and relationships
//  - Avoiding distortion of causal dependencies or overfitting to synthetic
//    features
//
//- **Best Practices**
//  - Combine with real data for validation
//  - Use domain knowledge to guide synthetic data generation and use
//
//* Digital Twin
//- A digital twin is a virtual representation of a physical system or process
//  created using data and software
//  - Used to simulate and identify potential problems under different conditions
//  - Used to generate synthetic data

* The Future of Causal AI
- Causal AI:
  - Is moving out of academia into the commercial world
  - Is a departure from the 2000 approach of a purely data-driven AI systems
  - Is a reflection of the reality of how humans think, analyze, and make
    decisions and how the real world works

- Causal, traditional AI, deep learning, and generative techniques will merge

// ###########################################################################


// ### 13.5.2, The back-door criterion


// TODO(gp): Add d-separation

// TODO(gp): Add Facuce?

// Do-operator

//* Causal Discovery
//
//- **Definition**
//  - Causal discovery identifies **cause–effect relationships** among variables
//  - Understand **which variables influence others** and **how interventions
//    change outcomes**
//
//- **Key Idea**
//  - Networks like $Fire \to Smoke$ and $Smoke \to Fire$ can represent the same
//    probability, but only one shows the **true causal direction**
//  - Causal discovery infers this direction from **observational or experimental
//    data**
//
//- **Interventions and the $do()$ Operator**
//  - Introduced by Judea Pearl for **interventions**
//  - $\Pr(X_i | do(X_j = x_j))$ describes the effect of **actively setting**
//    $X_j$ to a value
//
//- **Learning from Data**
//  - Causal discovery algorithms recover the **causal graph** structure using:
//    - **Conditional independence tests**
//    - **Score-based search**
//    - **Hybrid methods**
//  - Determine which links in a Bayesian network represent **true causal
//    influences**
//
//- **Relation to Causal Inference**
//  - Causal discovery identifies **the structure** of causality
//  - Causal inference predicts **effects of actions or interventions**
//
//- **Example**
//  - Observe **Sprinkler** and **Rain** affecting **WetGrass**
//  - Causal discovery determines if **Rain causes WetGrass directly**
//    **through influencing Sprinkler**.
//
