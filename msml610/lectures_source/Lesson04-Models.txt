::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Machine Learning Models}}$$**
\endgroup

::: columns
:::: {.column width=75%}
\vspace{1cm}

**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`

**References**:

- Burkov: _"Machine Learning Engineering"_ (2020)

- Hastie et al.: _"The Elements of Statistical Learning"_ (2nd ed, 2009)

::::
:::: {.column width=20%}

![](msml610/lectures_source/figures/book_covers/Book_cover_Hundred_page_ML_book.jpg){ height=20% }

![](msml610/lectures_source/figures/book_covers/Book_cover_The_Elements_of_Statistical_Learning.jpg){ height=20% }

::::
:::

# ##############################################################################
# Models
# ##############################################################################

## #############################################################################
## Naive Bayes
## #############################################################################

* Naive Bayes
- Predict classes $H_1, ..., H_n$ using evidence $\vE$:
  - Use Bayes' rule of conditional probability to decide output class
    $H_1, ... H_n$ given evidence $\vE$:
    $$
    \Pr(H_j|\vv{E}) = \frac{\Pr(\vv{E}|H_j) \Pr(H_j)}{\Pr(\vv{E})}
    $$
    where $\vv{E}$ is the vector of features
  - Training data estimates joint probability $\Pr(H_i, \vE)$

- **Naive Bayes assumptions:**
  1. Features are independent
  2. Features are equally important (or at least all relevant)

- **Naive Bayes model:**
  - Called "naive" due to the simplifying assumption of independence, even if
    not true
  - Works surprisingly well

* Naive Bayes: Weather Prediction Example

::: columns
:::: {.column width=55%}

- **Problem**
  - Predict if kids play outside using past weather observations

- **ML formulation**
  - Supervised learning
  - Predictor vars:
    - `outlook = {sunny, overcast, rainy}`
    - `temperature = {hot, mild, cold}`
    - `humidity = {high, normal}`
    - `windy = {true, false}`
  - Response var:
    - `play = {yes, no}`
  - Training set:
    - Samples for predictors and response from observations
    - Possible noise in data (e.g., kids have different preferences, some are
      sick illness, some have homework)

::::
:::: {.column width=40%}

\begingroup \tiny
| **Outlook** |**Temp**|**Humidity**|**Windy**|**Play**|
|-----------|-------------|----------|-------|------|
| Overcast  | Cold        | Normal   | True  | Yes  |
| Overcast  | Hot         | High     | False | Yes  |
| Overcast  | Hot         | Normal   | False | Yes  |
| Overcast  | Mild        | High     | True  | Yes  |
| Rainy     | Cold        | Normal   | False | Yes  |
| Rainy     | Cold        | Normal   | True  | No   |
| Rainy     | Mild        | High     | False | Yes  |
| Rainy     | Mild        | High     | True  | No   |
| Rainy     | Mild        | Normal   | False | Yes  |
| Sunny     | Cold        | Normal   | False | Yes  |
| Sunny     | Hot         | High     | False | No   |
| Sunny     | Hot         | High     | True  | No   |
| Sunny     | Mild        | High     | False | No   |
| Sunny     | Mild        | Normal   | True  | Yes  |
\endgroup
::::
:::

* Naive Bayes: Weather Prediction Example
- Use Bayes' rule to decide class $H_j$:
  $$
  \Pr(H_j|\vv{E}) = \frac{\Pr(\vv{E}|H_j) \Pr(H_j)}{\Pr(\vv{E})}
  $$
  where:
  - $H_j$: event to predict
    - E.g., `play = yes`
  - $\vv{E}$: event with feature values
    - E.g., `outlook=sunny, temperature=high, humidity=high, windy=yes`

* Naive Bayes: Weather Prediction Example
- The **model** is:
  \begingroup \scriptsize
  $$
  \Pr(H_j|\vv{E}) = \frac{\Pr(\vv{E}|H_j) \Pr(H_j)}{\Pr(\vv{E})}
  $$
  \endgroup
  where:
  - $\Pr(H_j)$: **prior probability** (probability of the outcome before evidence
    $\vv{E}$)
    - E.g., $\Pr(\text{play = yes})$
    - Computed from training set as:
      \begingroup \scriptsize
      $$\Pr(H_j) = \sum_{k=1}^N \Pr(H_j \land \vv{E}_k)$$
      \endgroup

  - $\Pr(\vv{E})$: **probability of the evidence**
    - Computed from training set
    - Not needed as it is common to the probability of each class

  - $\Pr(\vv{E}|H_j)$: **conditional probability**
    - Computed as independent probabilities (the "naive" assumption):
      \begingroup \scriptsize
      \begin{alignat*}{2}
      \Pr(\vv{E}|H_j)
      &= \Pr(E_1=e_1, E_2=e_2, ..., E_n=e_n | H_j) \\
      &\approx \Pr(E_1=e_1|H_j) \cdot ... \cdot \Pr(E_n=e_n|H_j) \\
      \end{alignat*}
      \endgroup

- **Interpretation of Bayes theorem**
  - The prior is modulated through the conditional probability and the
    probability of the evidence

* 1-Rule
- Aka "tree stump", i.e., a decision tree with a single node

- **Algorithm**
  - Pick a single feature (e.g., `outlook`):
    - Most discriminant
    - Based on expert opinion
  - Choose the most frequent output for the feature's current value

// TODO(gp): Add picture of tree stump

- **Weather problem example**:
  - Pick `outlook` as single feature
  - Know predictor vars, e.g., `outlook = sunny`
  - Compute probability of `play = yes` given `outlook = sunny` using training
    set:
    $$
    \Pr(\texttt{play = yes} | \texttt{outlook = sunny})
    $$
  - Output the predicted variable


* Naive Bayes: Why Independence Assumption
- **Independence assumption** factors joint probability into marginal
  probabilities:
  \begin{alignat*}{2}
  \Pr(\vv{E}|H_j)
  &= \Pr(E_1=e_1, E_2=e_2, ..., E_n=e_n | H_j) \\
  &\approx \Pr(E_1=e_1|H_j) \cdot ... \cdot \Pr(E_n=e_n|H_j) \\
  \end{alignat*}

- **Pros**:
  - Simplifies probability computation
  - Aids generalization due to underlying model and so fewer samples needed

- **Cons**:
  - Assumption may not be true

* Estimating Probabilities: MLE
- **Maximum Likelihood Estimate** (MLE) estimates event probability by counting
  occurrences among all possible events:
  $$
  \Pr(\vv{E} = \vv{e}') = \frac{\# I(\vv{E} = \vv{e}')}{K}
  $$

- For Naive Bayes, we need to estimate probability of each feature:
  $$
  \Pr(E_i = e') = \frac{\# I(E_i = e')}{\sum_{k=1}^K \# I(E_i = e_k)}
  $$

- **Problem**
  - Value $e'$ of feature $E_i$ not in training set with output class $H_j$
  - Estimated probability $\Pr(E_i=e'|H_j) = 0$
  - Plugging $\Pr(E_i=e'|H_j) = 0$ into
    $$
    \Pr(H_j|\vv{E}) \approx \Pr(E_1=e_1|H_j) \cdot ... \cdot \Pr(E_n=e_n|H_j)
    $$
  - Yields $\Pr(H_j|\vv{E}) = 0$ $\to$ impossible to decide output

* Estimating Probabilities: Laplace Estimator
- Use Laplace estimator for events not in training set instead of MLE

- **Maximum likelihood estimate** (MLE)
  $$
  \Pr(E_i = e') = \frac{\# I(E_i = e')}{\sum_{k=1}^K \# I(E_i = e_k)}
  $$

- **Laplace estimator**
  - Adds 1 to each count and $V$ (number of feature values) to denominator:
    $$
    \Pr(E_i = e')
    = \frac{1 + \# I(E_i = e')}{\sum_{j=1}^V (1 + \# I(E_i = e'))}
    = \frac{1 + \# I(E_i = e')}{V + \sum_{j=1}^V \# I(E_i = e')}
    $$
  - Blends prior of equiprobable feature values with MLE estimates

## #############################################################################
## Decision Trees
## #############################################################################

// AIMA 19.3 Learning Decision Trees

* Decision Tree
- **Characteristics**
  - Supervised learning
  - Classification and regression
  - Non-parametric (i.e., no functional form)

- **Model**
  - Decision rules organized in a tree

- **Training**
  - Infer decision rules from data
  - Worst case complexity: $O(2^n)$ with number of variables
  - Greedy divide-and-conquer improves average complexity

- **Evaluation**
  - Evaluate model from root to leaves
  - Prediction cost: $O(log(n))$ with number of training points

* Typical Decision Trees
- **Each node** tests a feature against a constant
  - Split input space with hyperplanes parallel to axes

- **Each feature**:
  - Tested once
  - Labeled with $(x^{(j)}, {<, =, >}, t)$
  - Checks feature $x^{(j)}$ against threshold $t$
  - Result determines branch to follow

- **Leaves** represent decisions:
  - Class labels (e.g., classification)
    - Predict class or probability
  - Regression function

- Deeper tree, more complex decision rules, fitter model
  - No re-converging paths: it's a tree!
  - Trees are non-linear models using variable interaction in `OR-AND` form:
    $$
    y_i = (x_1 \ge x_1') \land (x_2 \ge x_2') ...) \lor (...)
    $$

* Decision Trees: Pros
- Both for **regression and classification**

- **Simple to understand and interpret**
  - White box model: explainable decisions
  - Visualizable
  - Can be created by hand

- Requires **little data preparation**
  - Invariant under feature scaling
    - No data normalization
    - No dummy variables
  - Handles numerical and categorical data
  - Robust to irrelevant features (implicit feature selection)
  - Missing values are treated:
    - As their own value; or
    - Assigned the most frequent value (i.e., imputation)

- **Scalable**
  - Performs well with large datasets

* Decision Trees: Cons
- **Learning a decision tree** is NP-complete
  - Much worse than complexity of OLS
  - Algorithms use heuristics (e.g., greedy algorithms)

- **Risk of overfitting**
  - Solutions:
    - Pruning
    - Minimum samples at a leaf node
    - Max depth of trees

- **Some training sets are hard to learn**
  - E.g., `XORs`, parity

- **Unstable**
  - Small data variations greatly influence the tree
  - Solutions:
    - Ensemble learning / randomization

* Handling of Missing Values
- Missing values are treated:
  - As their own value; or
  - Assigned the most frequent value (i.e., imputation)

// This should be moved somewhere else
//* Multi-output problem with no correlation
//- If there is no correlation between the outputs
//  - Classification outputs can always be encoded as one-hot and one can use
//    one-vs-all approach
//  - Build $n$ independent models
//- For regression this is not possible
//
//* Multi-output problem with correlation
//- Typically outputs are correlated
//  - E.g., predict the two coordinates of a circle with noise
//  - E.g., predict lower half of a face from the top half
//- Build a single model for all outputs
//  - Lower training time
//  - Greater generalization accuracy
//- A tree can store all outputs in each leaf, instead of one class
//  - The splitting is done considering the average reduction across all $n$
//    outputs

* Learning Decision Trees: Intuition
- **Several algorithms**
  - ID3
  - C4.5
  - CART (Classification And Regression Trees)

- Typically, the problem has a **recursive formulation**
  - Consider the current "leaf"
  - Find the variable/split ("node") that best separates outcomes into two
    groups ("leaves")
    - "Best" = samples with same labels grouped together
  - Continue splitting until:
    - Groups are small enough
    - Maximum depth is reached
    - Sufficiently "pure"

// TODO: Add example. Think of two coordinates $x_1, x_2$ with various examples
// with 3 labels

* Splitting at One Node: Problem Formulation
- Consider the $m$-th node of a decision tree
  - Given $p_i = (\vx_i, y_i)$ where $i = 1, ..., N_m$, with training vectors
    $\vx_i$ and labels $y_i$
  - Candidate splits are $\theta = (j, t_m)$ with feature $j$ and threshold $t_m$
  - Each split partitions data into subsets:
    \begin{alignat*}{2}
    Q_{L}(j, t_m) & = \{ p_i = (\vx_i, y_i): x_j \le t_m \} \\
    Q_{R}(j, t_m) & = \{ p_i \notin Q_{L} \} \\
    \end{alignat*}
  - Impurity of a split is defined as:
    $$
    H(j, t_m) = \frac{n_{L}}{N_m} H(Q_{L}) + \frac{n_{R}}{N_m} H(Q_{R})
    $$
  - Find split $(j, t_m)^*$ minimizing $H(j, t_m)$

- **Recurse** on $Q_{L}(j, t_m)^*$ and $Q_{R}(j, t_m)^*$

* Measures of Node Impurity for Classification
- **Measures of node impurity:**
  - Based on probabilities of choosing objects of different classes
    $k = 1, ..., K$ in the $m$-th node, i.e., $p_k$
  - Smaller impurity values are better
  - Less impurity means smaller probability of misclassification

- **Examples**
  1. Misclassification error
  2. Gini impurity index
  3. Information gain

* Probability of Classification in a Node
- Assume $m$-th node has $N_m$ objects $x_i$ in $K$ classes
- Compute class probability in $m$-th node as:
  $$
  \hat{f}_{m, k}
  \defeq \Pr(\text{pick object of class $k$ in $m$-th node })
  = \frac{1}{N_m} \sum_{x_i \in \text{ node }} I(c(x_i) = k)
  $$
  where $y_i = c(x_i)$ is the correct class for element $x_i$

- E.g., if there are $N_m = 10$ objects belonging to $K=3$ classes
  - 3 red, 6 blue, 1 green
  - The probability of classification $\hat{f}_{m, k}$ are:
    - Red = 3/10
    - Blue = 6/10
    - Green = 1/10

* Misclassification Error: Definition
- You have several class probabilities $p_i$ need a single probability
  - Consider worst case: most common class $k'$ in node

- **Misclassification error**
  $$
  H_M(p) \defeq 1 - \max_{k}{p_k}
  $$

- **Binary classifier**
  - Best case (perfect purity)
    - Only one class in node
    - $H_M(p) = 0$
  - Worst case (perfect impurity)
    - 50-50 split between classes in node
    - $H_M(p) = 0.5$

- **Multi-class classifier** with $K$ classes
  - Misclassification error has upper bound: $1 / K$

* Gini Impurity Index: Definition
- **Gini index** $H_G(p)$ is probability of picking an element randomly and
  classifying it incorrectly
  - By using the law of total probability:
    \begingroup \small
    \begin{alignat*}{2}
    H_G(p)
    & = \Pr(\text{pick elem of $k$-th class}) \cdot
    \Pr(\text{misclassification | elem of $k$ class}) \\
    & = \sum_{k=1}^K p_k \cdot (1 - p_k)
    \end{alignat*}
    \endgroup

- **Binary classifier**
  - $H_G(p)$ is between 0 (perfect purity) and 0.5 (perfect impurity)

- **Multi-class classifier** with $K$ classes
  - $H_G(p)$ has upper bound: $1 - K \frac{1}{K}^2 = 1 - \frac{1}{K}$

* Information Gain: Definition
- Aka cross-entropy (remember entropy is $- p \log p$)

- **Information gain**
  $$
  H_{IG} = - \sum_{k=1}^K p_k \cdot \log_2(p_k)
  $$

- **Binary classifier**
  - $H_{IG}$ varies between 0 (perfect purity) and 1 (perfect impurity)

- **Multi-class classifier** with $K$ classes
  - $H_{IG}$ has upper bound: $\log{K}$

* Measures of Impurity: Examples
- Consider the case of 16 elements in a node , belonging to 2 classes

- If all elements are of the same class:
  - Misclassification error $= 0$
  - Gini index $= 1 - (1 - 0) = 0$
  - Information gain $= - (1 \cdot \log_2(1) - 0 \cdot \log_2(0)) = 0$

- If one element is of one class:
  - Misclassification error =
    $1 - \max(\frac{1}{16}, \frac{1}{15}) = \frac{1}{16}$
  - Gini index $= 1 - ((\frac{1}{16})^2 + (\frac{1}{15})^2) = 0.12$
  - Information gain
    $= -(\frac{1}{16} \log_2(\frac{1}{16}) +
    \frac{15}{16} \log_2(\frac{15}{16})) = 0.34$

- If elements are split in the two classes equally:
  - Misclassification error $= \frac{8}{16} = 0.5$
  - Gini index $= 1 - (\frac{8}{16}^2 + \frac{8}{16}^2) = 0.5$
  - Information gain
    $= -( \frac{8}{16} \log_2(\frac{8}{16}) +
  \frac{8}{16} \log_2(\frac{8}{16})) = 1$

// TODO: Convert into a table / widget

* Measures of Impurity for Regression
- For continuous variables with $N_m$ observations at a node, minimize mean
  squared error:

  $$
  \begin{aligned}
  & c_m = \frac{1}{N_m} \sum_{i \in N_m} y_i & \text{ (average class)}\\
  & H = \frac{1}{N_m} \sum_{i \in N_m} (y_i - c_m)^2 & \text{ (variance)}\\
  \end{aligned}
  $$

* Tips for Using Trees
- **Decision trees overfit** with **many features**
  - Get right ratio of training samples to features

- **Solutions**
  - Use dimensionality reduction (PCA, feature selection) to remove
    non-discriminative features

  - Use maximum tree depth to prevent overfitting

  - Control minimum number of examples at a leaf node / split
    - Small number $\to$  overfitting
    - Large number $\to$  no learning

- **Balance dataset before training** to avoid **bias**
  - Normalize sum of sample weights for each class

* Feature Selection with Trees
- **Intuition**
  - Top tree features predict more samples

- **Importance of a variable**
  - Feature's control over samples estimates importance
  - Feature's depth as a decision node assesses importance

- **Trees are unstable**
  - Reduce estimation variance by averaging variable depth over multiple
    randomized trees (random forest)

* Embeddings with Trees
- **Intuition**
  - Learning a tree is like a non-parametric density estimation
  - Neighboring data points likely lie within the same leaf

- **Embedding: unsupervised data transformation**
  - Tree encodes data by indices of leaves a data point belongs to
  - Index encoded one-hot for high-dimensional, sparse, binary coding

- Use **number of trees and max depth per tree** to control space size

## #############################################################################
## Random Forests
## #############################################################################

* From Decision Trees to Random Forests
- Decision trees are **high capacity models**:
  - Low bias
  - High variance

- Idea: apply ensemble methods to trees $\rightarrow$ "random forests"

- **Bagging**
  - Reduces variance in "unstable" non-linear models
    - Learning trees is unstable
  - Best with complex models (e.g., fully grown trees)
    - Boosting works best with weak models (e.g., shallow decision trees, aka
      tree stumps)
  - Works for regression and classification
  - Customizable for trees
    - Different types of randomization in trees

- **Bias-variance trade-off** in random forests
  - Forest bias could increase compared to a single non-random tree
  - Forest variance reduced by averaging, usually compensating for bias increase

* Randomization in Trees
- **Bagging** (bootstrap aggregate) / perturb-and-combine techniques designed for
  trees
  1. Training samples (with replacement)
  2. Picking features (random subspaces)
  3. Decision split thresholds
  4. All the above

- **Random forests**
  - Each tree built from samples drawn with replacement (bootstrap sample)
  - Split picked as best among random subset of features

- **Extremely randomized trees** (aka "Extra-Trees")
  - Thresholds randomized
  - More randomness than random forests
  - Trade off more bias for variance

- **Combine random forests**
  - Majority vote on class
  - Averaging prediction probability
  - Averaging prediction

* Random Forests: Pros and Cons
- Pros and cons are the same as ensemble learning

- **Pros**
  - Increased accuracy

- **Cons**
  - Lower training and evaluation speed
  - Loss of interpretability
  - Overfitting (cross-validation is needed)

## #############################################################################
## Linear Models
## #############################################################################

* Linear Regression Model
- **Data set**
  - $(\vx_1, y_1), ... , (\vx_N, y_N)$
  - $N$ examples
  - $P$ features, $\vx_i \in \bbR^P$

- **Linear regression model**
  $$
  h(\vx) = \sum_{i=1}^{P} w_i x_i = \vw^T \vx \in \bbR
  $$

- Add **bias term** $w_0$ to model with $x_0 = 1$ to data
  $$
  h(\vx) = w_0 + \sum_{i=1}^{P} w_i x_i = \sum_{i=0}^{P} w_i x_i = \vw^T \vx
  $$

* Linear Regression: In-sample Error
- For regression, use **squared error for in-sample error**:
  $$
  E_{in}(h) = \frac{1}{N} \sum_{i=1}^{N} (h(\vx_i) - f(\vx_i)) ^ 2
  $$

- Squared error for **linear regression**
  $$
  E_{in}(h)
  = E_{in}(\vw)
  = \frac{1}{N} \sum_{i=1}^{N} (\vw^T \vx_i - y_i) ^ 2
  $$
- Squared error **in vector form**
  $$
  E_{in}(h)
  = \frac{1}{N} \| \mX \vw - \vy \| ^ 2
  = \frac{1}{N} (\mX \vw - \vy)^T (\mX \vw - \vy)
  $$
  where:
  - $\mX$ is the matrix with examples $\vx_i^T$ as rows ("design matrix")
  - $\mX$ is a tall matrix with few parameters ($P$) and many examples ($N$)
  - $\vy$ is the column vector with all outputs (target vector)

* Linear Regression: Find Optimal Model 
- You want to minimize $E_{in}(\vw) = (\mX \vw - \vy)^T (\mX \vw - \vy)$ with
  respect to $\vw$
  \begingroup \small
  \begin{alignat*}{1}
  & \nabla E_{in}(\vw^*) = \vv{0} \\
  & \frac{2}{N}\mX^T(\mX \vw^* - \vy) = \vv{0} \\
  & \mX^T \mX \vw^* = \mX^T \vy \\
  \end{alignat*}
  \endgroup
- If the square matrix $\mX^T \mX$ is invertible:
  $$
  \vw^* = (\mX^T \mX)^{-1} \mX^T \vy = \mX^\dagger \vy
  $$

- The matrix $\mX^\dagger \defeq (\mX^T \mX)^{-1} \mX^T$ is called
  **pseudo-inverse**
  - It generalizes the inverse for non-square matrices, in fact:
    - $\mX^\dagger \mX = \mI$
    - If $\mX$ is square and invertible, then $\mX^\dagger = \mX^{-1}$

* Complexity of One-Step Learning
- Learning with the pseudo-inverse is **one-step learning**
  - Contrasts with iterative methods, e.g., gradient descent

- Inverting a square matrix of size $P$ is related to the number of parameters,
  not examples $N$
  - Complexity of one-step learning is $O(P^3)$

* Linear Models Are Linear in What?
- A **model is linear** when the signal $s = \sum_{i=0}^P w_i x_i = \vw^T \vx$ is
  linear **with variables**
  - Unknown variables: weights $w_i$
  - Inputs $x_i$ are fixed

- Applying a **non-linear transform to inputs** $z_i = \Phi(x_i)$ keeps the model
  linear, e.g.,
  - Positive/negative part (e.g., $z_i = RELU(x_i), RELU(-x_i)$)
  - Waterfall (conditioning model to different feature ranges)
  - Thresholding (e.g., $z_i = \min(x_i, T)$)
  - Indicator variables (e.g., $z_i = I(x_i > 0)$)
  - Winsorizing (replace outliers with a large constant value)

- Applying a **non-linear transform to weights** $z_i = \Phi(w_i)$ makes the
  model non-linear

* Non-Linear Transformations in Linear Models
- **Transform variables**
  - Use $\Phi: \calX \rightarrow \calZ$
  - Transform each point $\vx_n \in \calX = \bbR^d$ into a point in feature space
    $\vz_n = \Phi(\vx_n) \in \calZ = \bbR^{\tilde{d}}$ with $d \ne \tilde{d}$

- **Learn**
  - Learn linear model in $\calZ$, obtaining $\vv{\tilde{w}}$ for separating
    hyperplane

- **Predict**
  - Evaluate model on new point in $\calZ$:
    $$
    y = \sign(\vv{\tilde{w}}^T \Phi(\vx))
    \text{or} \quad y = \vv{\tilde{w}}^T \Phi(\vx)
    $$

- Compute **decision boundary**
  - In $\calX$ if $\Phi$ is invertible; or
  - By classifying any $\vx \in \calX$ in $\calZ$

## #############################################################################
## Perceptron
## #############################################################################

* Example of Classification Problems
- **Binary classification problem**
  - $y \in \{0, 1\}$
    - Typically assign 1 to what you want to detect
  - Email: `spam`, `not_spam`
  - Online transaction: `fraudulent`, `valid`
  - Tumor: `malignant`, `benign`

- **Multi-class classification problem**
  - $y \in \{0, 1, 2, ... , K\}$
  - Email tagging: `work`, `family`, `friends`
  - Medical diagnosis: `not_ill`, `cold`, `flu`
  - Weather: `sunny`, `rainy`, `cloudy`, `snow`

* Linear Regression for Classification
- Use **linear regression for classification**
  - Transform outputs into $\{+1, -1\} \in \bbR$
  - Learn $\vw^T \vx_n \approx y_n = \pm 1$
  - Use $\sign(\vw^T \vx_n)$ as model (perceptron)

- **Not optimal**: outliers influence fit due to square distance metric
  - Use weights from linear regression to initialize a learning algorithm for
    classification (e.g., PLA)

* Perceptron Learning Algorithm (PLA)
- First machine learning algorithm discovered

- **Algorithm**
  - Training set $\calD = \{(\vx_1, y_1), ..., (\vx_n, y_n) \}$
  - Initialize weights $\vw$
    - Random values
    - Use linear regression for classification as seed
  - Pick a misclassified point $\sign(\vw^T \vx_i) \ne y_i$ from training set
    $\calD$
  - Update weights: $\vw(t + 1) = \vw(t) + y_i \vx_i$
    - Like stochastic gradient descent
  - Iterate until no misclassified points

- Algorithm **converges** (slowly) for linearly separable data

- **Pocket version of PLA**
  - Idea
    - Continuously update solution
    - Keep best solution "in the back pocket"
  - Have a solution if stopping after max iterations

* Non-Linear Transformations for Classifications
- Classification problems have **varying degrees of non-linear boundaries**:
  1. Non-linearly separable data
     - E.g., $+$ in center, $-$ in corners in a 2-feature scatter plot
  2. Mostly linear classes with few outliers
  3. Higher-order decision boundary
     - E.g., quadratic
  4. Non-linear data-feature relationship
     - E.g., variable threshold

// TODO: Add examples

## #############################################################################
## Logistic Regression
## #############################################################################

* Logistic Regression Is a Probabilistic Classifier
- **Logistic regression** learns:
  - The probability of each class $\Pr(y | \vx)$ given input $\vx$
  - Instead of predicting class $y$ directly

- **Parametric approach**: assume $\Pr(y | \vx; \vw)$ has a known functional
  form
  $$
  \Pr(y=1|\vx; \vw) = \text{logit}(\vw^T \vx)
  $$

- **Optimize parameters** $\vw$ using maximum likelihood
  $$
  \vw^* = \argmax_{\vw} \Pr(\calD; \vw)
  $$

- **Predict** by outputting class with highest probability
  $$
  h_{\vw}(\vx) =
  \begin{cases}
  +1 & \Pr(y=1|\vx; \vw) \ge 0.5 \\
  -1 & \Pr(y=1|\vx; \vw) < 0.5 \\
  \end{cases}
  $$

* Logistic Regression: Example
- Assume $y$ is:
  - The event $y_i$ _"patient with characteristics $\vx$ had heart attack"_
  - Function of parameters $\vx$ (e.g., age, gender, diet)

- In data set $\calD$:
  - No samples of $\Pr(y | \vx)$
  - Have realizations:
    - _"Patient with $\vx_1$ had a heart attack"_
    - _"Patient with $\vx_2$ didn't"_
    - ...

- Learn $\Pr(y | \vx)$
  - Find best parameters $\vw$ for logistic regression model to explain data
    $\calD$

* Logistic Function
- Aka "sigmoid"
- **Logistic function** $\text{logit}(s)$ is defined as
  $$
  \text{logit}(s) \defeq \frac{e^s}{1 + e^s} = \frac{1}{1 + e^{-s}}
  $$
  - Varies in [0, 1]
  - Crosses the origin at 0.5
  - Asymptotes at 0 and 1
  - It is a soft version of $\sign()$

// TODO(gp): Add picture

* Logistic Regression vs Linear Classifier
- **Functional form is similar**
  - **Logistic regression**: $h(\vw) = \text{logit}(\vw^T \vx$)
  - **Linear classifier** (perceptron): $h(\vw) = \sign(\vw^T \vx)$

- **Difference** in probabilistic interpretation and fitting method
  - Logistic regression** lacks samples of probability function to interpolate
    - Uses realizations of random variable
    - Seeks model parameters maximizing data likelihood
  - **Linear classification** assumes class value is linear function of inputs

* Error for Probabilistic Binary Classifiers
- For probabilistic binary classification, use **log-probability error** as
  point-wise error
  $$
  e(h(\vx), y) \defeq -\log(\Pr(y = h(\vx)| \vx; \vw))
  $$
  - Negate for positive errors: $\log([0, 1]) \in [-\infty, 0)$

- Log probability generalizes 0-1 error
  - Case $y = 1$
    - Output $h(\vx)$ close to 1 $\implies$ probability 1 $\implies \log(1) = 0$
      $\implies e(\cdot) = 0$
    - Output close to 0 $\implies e() = -\log(0) \to +\infty$
  - Similar behavior for $y = 0$

// TODO: Add a picture

* One-Liner Error for Probabilistic Binary Classifiers
- **Point-wise error** for example $(\vx, y)$ for probabilistic binary
  classifiers is defined as:
  \begingroup \small
  \begin{alignat*}{2}
  e(h(\vx), y) & \defeq -\log(\Pr(h(\vx) = y | \vx)) \\
  & = \begin{cases}
  -\log(\Pr(y=1|\vx)) & y = 1\\
  -\log(\Pr(y=0|\vx)) & y = 0\\
  \end{cases}
  \\
  & = \begin{cases}
  -\log(\Pr(y=1|\vx)) & y = 1\\
  -\log(1 - \Pr(y=1|\vx)) & y = 0\\
  \end{cases}
  \\
  \end{alignat*}
  \endgroup
  \vspace{-1cm}

- Any function of a binary variable:
  \begingroup \small
  $$
  \begin{aligned}
  y
  &= \begin{cases}
  a & x = 1\\
  b & x = 0\\
  \end{cases}\\
  \end{aligned}
  $$
  \endgroup
  can be written as one-liner: $y = x \cdot a + (1 - x) \cdot b$

- Point-wise error can be written independently of $\Pr(y=1|\vx)$:
  \begingroup \small
  $$
  e(h(\vx), y) = -y \log(\Pr(y=1|\vx) - (1 - y) \log(1 - \Pr(y=1|\vx))
  $$
  \endgroup

* One-Liner Error for Logistic Regression
- The point-wise error for a binary classifier is:
  $$
  e(h(\vx), y) = -y \log(\Pr(y=1|\vx) - (1 - y) \log(1 - \Pr(y=1|\vx))
  $$

- Simplify further **with logit function**:
  \begin{alignat*}{3}
  e(h(\vx), y)
  & \defeq - \log \Pr(h(\vx)=y)
  &
  \\
  & \text{ ... a bit of math manipulation ...}
  &
  \\
  &
  = -\log \text{logit}(y \vw^T \vx)
  &
  \text{ since } \text{logit}(s) = \frac{1}{1 + e^{-s}}
  \\
  &
  = \log(1 + \exp(-y \vw^T \vx))
  &
  \\
  \end{alignat*}
  \vspace{-1cm}

- Point-wise error for logistic regression equals **cross-entropy error**

* Cross-Entropy Error
- **Point-wise error for logistic regression** has expression:
  $$
  e(h(\vx), y) = \log(1 + \exp(- y \cdot \vw^T \vx))
  $$
  - It is called **cross-entropy error**
  - Note: no $-$ before $\log(\cdot)$ but before $y \cdot \vw^T \vx$

- Cross-entropy error **generalizes 0-1 error**
  - If $\vw^T \vx$ agrees with $y$ in sign and $|\vw^T \vx|$ is large $\implies$
    error goes to 0
  - If they disagree in sign $\implies$ error goes towards $\infty$

- Define **in-sample error on training set** as average of point-wise errors:
  $$
  E_{in} \defeq \frac{1}{N} \sum_n e(h(\vx_n), y_n)
  $$

* Fitting Logistic Regression
- A plausible error measure of a hypothesis is based on **likelihood of data**
  $$
  \Pr(\calD | h = f)
  $$
  - _"How likely is the data $\calD$ under the assumption that $h = f$?"_
  - _"How likely is that the data $\calD$ was generated by $h$?"_

- **Maximize likelihood** $\calD$ generated from logistic regression
  $$\Pr(y = 1| \vx; \vw)$$

- It can be proved that this is equivalent to **minimizing in-sample error** on
  training set **using cross-entropy error**

* Fitting Logistic Regression (Optional)
- Find $\vw$ that maximizes likelihood for data set
  $\calD = \{ (\vx_1, y_1), ... , (\vx_N, y_N) \}$ generated by model $h(\vx)$:
  $$
  \Pr(D | \vw)
  = \Pr(y_1 = h(\vx_1) \land ... \land y_N = h(\vx_N))
  = \Pr(y_1 = y_1' \land ... \land y_N = y_N')
  $$

- Model form:
  $$
  y' = h(\vx) =
  \begin{cases}
  +1 & \text{ if logit}(\vw^T \vx) > 0.5\\
  -1 & \text{ otherwise}
  \end{cases}
  $$

- Assuming independence among training examples:
  $$
  \Pr(D | \vw) = \prod_{i=1}^N \Pr(y_i = y_i' | \vx_i)
  $$

- Fold $y_n$ in expression:
  - When $y_n = 1$, $\Pr(y_n = y_n') = \text{logit}(\vw^T \vx_n)$
  - When $y_n = -1$, $\Pr(y_n = y_n') = \text{logit}(-\vw^T \vx_n)$

- Thus, $\Pr(y_n = y_n') = \text{logit}(y_n \vw^T \vx_n)$

* Fitting Logistic Regression (Optional)
- Given:
  $$
  \Pr(D | \vw) = \prod_{i=1}^N \text{logit}(y_n \vw^T \vx_n)
  $$

- Re-write optimization as minimizing sum of point-wise errors
  $E_{in} = \sum e(h(\vx_n), y_n)$
  - Maximize $\log(...)$ with respect to $\vw$ since log argument $> 0$ and
    $\log()$ is monotone

- Equivalently, minimize:

  \begin{alignat*}{3}
  - \frac{1}{N} \log(...)
  &= - \frac{1}{N} \log(\prod(...))
  = - \frac{1}{N} \sum(\log(...))
  & 
  \text{ since } \text{logit}(s) = \frac{1}{1 + e^{-s}}
  \\
  &= \frac{1}{N} \sum \log(\frac{1}{\text{logit}(y_n \vw^T \vx_n)})
  &
  \\
  &= \frac{1}{N} \sum \log(1 + \exp(- y_n \vw^T \vx_n))
  = \frac{1}{N} \sum e(h(\vx_n), y_n) = E_{in}(\vw)
  &
  \\
  \end{alignat*}

* Gradient Descent for Logistic Regression
- Gradient descent requires two inputs:
  - Gradient of the cost function $\frac{\partial E}{w_j} \text{ for all } j$
  - Cost function $E_{in}(\vw)$

- The cost function is:
  $$
  E_{in}(\vw) = \frac{1}{N} \sum_i e(h(\vx_i; \vw), y_i)
  $$
- The cost function for logistic regression:
  $$
  E_{in}(\vw) = \frac{1}{N} \sum_i \log(1 + \exp(-y_i \vw^T \vx_i))
  $$

- Thus gradient descent converges to global minimum
  - It can be shown that $E_{in}(\vw)$ is convex in $\vw$
  - In fact sum of exponentials and flipped exponentials is convex and log is
    monotone

* One-Vs-All Multi-Class Classification
- Aka "one-vs-rest" classifier

- **Problem**: you have $n$ classes $c_1, ... , c_n$ to distinguish given $\vx$

- **Learn**
  - Create $n$ binary classification problems where we classify $c_i$ vs
    $c_{-i}$ (everything but $i$)
  - Learn $n$ classifiers with optimal $\vw_i$, each estimating
    $\Pr(y=i | \vx; \vw_i)$

- **Predict**
  - Evaluate the $n$ classifiers
  - Pick the class $y = i$ with the maximum $\Pr(y = i | \vx)$

* Cost Function for Multi-Class Classification (Optional)
- The cost function for logistic regression is:
  $$
  E_{in}(\vw)
  = - \frac{1}{N}
  \sum_{i=1}^N (y_i \log \Pr(y=1|\vx_i) + (1 - y_i) \log (1 - \Pr(y=1|\vx_i)))
  $$

- Encode expected outputs $\vy_i$ one-hot
  - $j$-th element $\vy_i|_j$ is 1 if correct class is $j$-th
  - E.g., for $k = 4$ `1000`

- Using $\vh(\vx)$ as model outputs and $\Pr(y = 1|\vx) = p(\vx)$:
  $$
  E_{in}(\vw)
  = - \frac{1}{N}
  \sum_{i=1}^N \sum_{k=1}^K \left(
  \vy_i \log(\vp(\vx_i)) +
  (1 - \vy_i) \log(1 - \vp(\vx_i)) \right)\big|_k
  $$

- Innermost summation considers error on each class/digit
  - E.g., for $k = 4$ `1000` vs `0100`
  - Equal digits don't contribute to error
  - Different digits give positive contribution

## #############################################################################
## LDA, QDA
## #############################################################################

* Basic Idea of Parametric Models
- Assume a model generates data
  - Known functional form
  - Parametrized with unknown parameters to estimate

- **Pros**
  - Utilize data structure
  - Easy to fit: few parameters
  - Accurate predictions if assumptions correct

- **Cons**
  - Strong data assumptions
  - Low accuracy if assumptions incorrect

* Linear and Quadratic Discriminant Analysis
- Aka LDA and QDA

- **Parametric models**
  - Assume each class generating process is multivariate Gaussian
  - Classifiers with linear and quadratic decision surface

- **Pros**
  - Closed-form solutions easy to compute (sample mean and covariance)
  - Inherently multiclass
  - No hyperparams to tune

- **Cons**
  - Strong assumptions about the data

* LDA / QDA: Model Form
- Both LDA and QDA assume class generating process
  $f_k(\vx; \vv{\mu_k}, \mat{\Sigma_k})$ is **multivariate Gaussian**

- **Linear discriminant analysis (LDA)** model:
  $$
  f_k(\vx; \vv{\mu_k}, \mat{\Sigma_k})
  \sim \calN(\vv{\mu_k}, \mSigma))
  $$
  - Means $\vv{\mu_k}$ differ for all $k$ classes
  - Covariance matrix $\mSigma$ same for all $k$ classes
  - Classes separated by linear decision boundaries

- **Quadratic discriminant analysis (QDA)** model:
  - Classes $k$ have different covariance matrix $\mat{\Sigma_k}$
  - Classes separated by quadratic boundaries

* Bayes Theorem for LDA / QDA
- Consider a classification setup with multi-class output $Y \in \{1, ..., K\}$

- Build a **parametric model for the conditional distribution**:
  $$
  \Pr(Y = k | X = \vx)
  $$

- Use **Bayes theorem**:
  $$
  \Pr(Y = k | X = \vx) = \frac{\Pr(X = \vx | Y = k) \cdot \Pr(Y = k)}{\Pr(X = \vx)}
  $$
  where:
  - $\Pr(X = \vx | Y = k)$: given a class, estimate probability of $\vx$
  - $\Pr(Y = k) = \pi_k$: probability of each class (prior)
  - $\Pr(X = \vx)$: probability of each input

- **Estimate probabilities** from data

* LDA / QDA: Boundary Decision (Optional)
- Consider the ratio between the probabilities of $Y = k$ vs $Y = j$:
  $$
  r = \frac{\Pr(Y = k | X = \vx)}{\Pr(Y = j | X = \vx)}
  $$

- Using the model assumption and Bayes theorem:
  \begin{alignat*}{2}
  \Pr(Y = k | X = \vx)
  &\propto \Pr(X = \vx | Y = k) \cdot \Pr(Y = k)
  \\
  &= f_k(\vx; \vv{\mu_k}) \cdot \pi_k
  \\
  \end{alignat*}
  where:
  $$
  f_k(\vx) = c \cdot
  \exp \left(
  -\frac{1}{2}
  (\vx - \vv{\mu_k})^T \mSigma^{-1} (\vx - \vv{\mu_k})
  \right)
  $$

- Apply $\log(\cdot)$ as a monotone transformation
  $$
  r = \log \frac{f_k(\vx)}{f_j(\vx)} + \log \frac{\pi_k}{\pi_j}
  $$

* LDA / QDA: Boundary Decision (Optional)
- Apply $\log(\cdot)$ as a monotone transformation
  $$
  r = \log \frac{f_k(\vx)}{f_j(\vx)} + \log \frac{\pi_k}{\pi_j}
  $$

- **Second term** independent of $\vx$ so you can ignore it

- **First term** proportional to:
  $$
  (\vx - \vv{\mu_k})^T \mSigma^{-1} (\vx - \vv{\mu_k}) -
  (\vx - \vv{\mu_j})^T \mSigma^{-1} (\vx - \vv{\mu_j})
  $$

- Expand, simplify $\vx^T \mSigma^{-1} \vx$, and note:
  $$
  2 \vx^T \mSigma^{-1} (\vv{\mu_k} - \vv{\mu_j})
  $$
  Resulting in a linear relationship in $\vx$
  $$
  - \frac{1}{2} (\vv{\mu_k} + \vv{\mu_j})^T \mSigma^{-1} (\vv{\mu_k} -
  \vv{\mu_j})
  + \vx^T \mSigma^{-1}(\vv{\mu_k} - \vv{\mu_j})
  $$

* LDA / QDA: Learn Model (Optional)
- In practice:
  - Ignore $\Pr(X = \vx)$; it's common for all classes
  - Know or estimate prior $\Pr(Y = k) = \pi_k$ from data
  - Estimate conditional probability $\Pr(X = \vx | Y = k)$

- Model assumes Gaussian distribution for conditional probability:
  $$
  \Pr(X = \vx | Y = k) = f_k(\vx; \vv{\mu_k}, \mat{\Sigma_k})
  $$
  where:
  $$
  f_k(\vx) = \frac{1}{(2 \pi)^n |\mat{\Sigma_k}|^{1/2}}
  \exp \left( -\frac{1}{2}(\vx - \vv{\mu_k})^T \mat{\Sigma_k}^{-1}
  (\vx - \vv{\mu_k}) \right)
  $$

- Estimate parameters $\vv{\mu_k}$, $\mat{\Sigma_k}$ using sample mean and
  covariance

* Evaluating LDA / QDA
- For new $\vx = \vx'$, compute for each class $Y = k$
  $$
  \Pr(Y = k | X = \vx) \propto f_k(\vx; \vv{\mu_k}, \mat{\Sigma_k}) \cdot \pi_k
  $$

- Choose $k$ that maximizes posterior probability

- If $f_i(\vx)$ is from a multivariate Gaussian distribution with diagonal
  $\mSigma$ (feature independence), you can simplify expressions further

## #############################################################################
## Kernel Methods
## #############################################################################

* Kernel: Definition

- Consider a transformation $\Phi: \calX \rightarrow \calZ$
  - E.g., transform features in space $\calX$ non-linearly into
    higher-dimensional space $\calZ$

- **Kernel of transformation** $\Phi$ yields inner product of two points
  $\vx, \vx' \in \calX$ in transformed space $\calZ$
  $$
  K_{\Phi}(\vx, \vx')
  \defeq \langle \Phi(\vx), \Phi(\vx') \rangle
  = \Phi(\vx)^T \Phi(\vx')
  = \vz^T \vz'
  $$

::: columns
:::: {.column width=50%}

- Why doing this?

::::
:::: {.column width=45%}

![](msml610/lectures_source/figures/Lesson04_Kernel_Trick.png)
::::
:::

* Kernel: Expression From the Transform
- If you have an expression for $\Phi$, compute a closed formula for the kernel

- E.g., if transformation is $\Phi: \bbR^2 \to \bbR^6$, it introduces interaction
  terms:
  $$
  \vz = \Phi(\vx) = \Phi(x_1, x_2) = (1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)
  $$
- Kernel of $\Phi$ is:
  \begin{alignat*}{2}
  & K_{\Phi}(\vx, \vx')
  & = (1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)^T
    (1, {x'}_1, {x'}_2, {x'}_1^2, {x'}_2^2, {x'}_1 {x'}_2) \\
  & 
  & = 1 + x_1 x'_1 + x_2 x_2' + x_1^2 {x'}_1^2 + x_2^2 {x'}_2^2
    + x_1 x_2 {x'}_1 {x'}_2 \\
  \end{alignat*}

* Gaussian Kernel
- Aka "exponential kernel" or "Radial Basis Function" (RBF) kernel
- A **Gaussian kernel** has the form:
  $$
  K(\vx, \vx')
  = \exp(- \gamma \|\vx - \vx'\|^2)
  = \exp(- \frac{\|\vx - \vx'\|^2}{\sigma^2})
  $$
- It can be shown to be an inner product in an infinite dimension $\calZ$

* Kernel as Way to Measure Similarity
- **Intuition**: The Gaussian kernel
  $$
  K(\vx, \vx')
  = \exp(- \gamma \|\vx - \vx'\|^2)
  $$
  measures "similarity" of point $\vx$ to point $\vx_i$:
  - $K(\vx, \vx')$ is 1 when points are the same
  - Value is 0 when points are distant
  - Effect strength depends on $\gamma$

- Using kernels to compute features:
  - Kernels often rely on distance between vectors
    - E.g., euclidean norm $\|\vx - \vx'\|^2$
  - Need to scale features for similar effects among coordinates

* Linear Kernel
- Consider the transformation $\Phi$ as the identity function $\Phi(\vx) = \vx$
- The kernel function is:
  $$
  K_{\Phi}(\vx, \vx') = \vx^T \vx'
  $$
  - A **linear kernel** means using no kernel
  - It is just a "pass-through"

* Polynomial Kernel
- Given a point $\vx \in \bbR^n$, consider the function with two parameters $k$
  and $d$
  $$
  K_{\Phi}(\vx, \vx') = (k + \vx^T \vx')^d
  $$
- It is called **polynomial** since if you expand the dot product you get a
  polynomial
- It can be proved that this is always a kernel

* Kernel: Identifying a Function as a Kernel
- **Problem**:
  - You have a certain function $K(\vx, \vx')$ and you want to show that $K(\cdot)$
    is an inner product in the form for some function $\Phi(\cdot)$
    $$
    K(\vx, \vx') = \Phi(\vx)^T \Phi(\vx') \quad \forall \vx, \vx'
    $$
    for a certain $\Phi$ and $\calZ$

- In theory, a given function $K(\vx, \vx')$ is a valid kernel iff:
  - It is a symmetric, and
  - Satisfies the Mercer's condition: the matrix $K(\vx_i, \vx_j)$ is definite
    semi-positive

* Kernel: Example of Identifying a Kernel
- Let's show that:
  $$K(\vx, \vx') = (k + \vx^T \vx')^d$$
  is a **kernel** for any $n, k, d$

- According to the definition you need to show that there is always a transform
  $\Phi$:
  $$
  \Phi: \calX = \bbR^n \to \calZ = \bbR^q
  $$
  with $q \gg d$, such that $K_{\Phi} = (k + \vx^T \vx')^d$

- **Example**
  - $\calX = \bbR^2$, $K(\vx, \vx') = (1 + \vx^T \vx')^2 = (1 + x_1 x'_1 + x_2 x'_2)^2$
  - Compute the full expression in terms of the coordinates:
    $$
    K(\vx, \vx')
    = (1 + x_1^2 {x'}_1^2 + x_2^2 {x'}_2^2 + 2 x_1 x'_1 + 2 x_2 x'_2 + 2 x_1 x'_1 x_2 x'_2)
    $$
  - Choose:
    - $\calZ = \bbR^6$
    - $\Phi(x_1, x_2) = (1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)$
  - This is a particular case of the polynomial kernel

* A Kernel Is a Computational Shortcut
- In literature, the **kernel trick** is a **computational shortcut** for the dot
  product of transformed vectors

- Compare 2 ways to compute the inner product of transformed vectors for a
  polynomial kernel
  1. **Using definition**: compute images of vectors, then inner product in
     transformed space:
     $$
     (1, x_1, x_2, \sqrt{2} x_1 x_2, x_1^2, x_2^2, ...)^T \cdot (1, x'_1, ...)
     $$
     - Requires combinatorial powers and a large dot product
  2. **Kernel trick**: use kernel function for dot product in transformed space
     $$
     (k + \vx^T \vx')^d
     $$
     - Requires inner product of small vectors, then power of a number

- Kernel trick is more computationally efficient for inner product computation

## #############################################################################
## Support Vector Machines (Optional)
## #############################################################################

* Support Vector Machines (SVM)
- Arguably one of the most successful classification algorithm, together with
  neural networks and random forests

- **Idea**: find a separating hyperplane that maximizes the distance from the
  class points (aka "margin")

- **All the rage in 2005-2015**
  - Robust classifier handling outliers automatically
  - Strong theoretical justification of out-of-bound error
  - Strong link with VC dimension
  - Cool geometric interpretation
  - Solve a very complex optimization problem with some neat tricks
  - Works for both regression and classification

- SVM for classification:
  - Does not output probabilities (like logistic regression), but predicts
    directly the class
  - Has a notion of confidence, as distance from the margin

* SVM Is a Large Margin Classifier

::: columns
:::: {.column width=50%}
- Why **large margin** classifier is good?

- Given a linearly separable data set, the optimal separating line maximizes the
  margin:
  - More robust to noise
  - Large margin reduces VC dimension of hypothesis set

::::
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson04_SVM.png)

::::
:::

* SVM: Notation and Conventions

- Assume that:
  1. Outputs are encoded as $y_i \in \{-1, 1\}$
  2. Pull out $w_0$ from $\vw$
     - The bias $w_0 = b$ plays a different role
     - $\vw = (w_1, ... , w_d)$ and there is no $x_0 = 1$
     - $\vw^T \vx + b = 0$ is the equation of the separating hyperplane
  3. $\vx_n$ is the closest point to the hyperplane
     - It can be multiple points from different classes

- Normalize $\vw$ and $b$ to get a canonical representation of the hyperplane
  imposing $|\vw^T \vx_n + b| = 1$

* SVM: Original Form of Problem
- The SVM problem is:

  \begin{alignat*}{3}
  \text{find }
  & \vw, b
  &
  \\
  \text{maximize }
  & \frac{1}{\|\vw\|}
  & \text{(max margin) }
  \\
  \text{subject to }
  & \min_{i=1, ... , n} |\vw^T \vx_i + b| = 1
  & \text{(hyperplane)}
  \\
  \end{alignat*}

- This problem is not friendly to optimization since it has norm, min, and
  absolute value

* Primal Form of SVM Problem
- You can rewrite it as:

  $$
  \begin{aligned}
  \text{find $\vw, b$} \eqspace & \\
  \text{minimize } \eqspace & \frac{1}{2} \vw^T \vw\\
  \text{subject to } \eqspace & y_i (\vw^T \vx_i + b) \ge 1 \; \forall i = 1, ..., n\\
  \end{aligned}
  $$

- Note that under $\vw$ minimal and linear separable classes, it is guaranteed
  that for at least one $\vx_i$ in the second equation will be equal to $1$ (as
  in the original problem)
  - In fact otherwise we could scale down $\vw$ and $b$ (which does not change the
    plane) to use the slack, against the hypothesis of minimality of $\vw$

* Dual (Lagrangian) Form of SVM Problem

  $$
  \begin{aligned}
  \text{minimize with respect to } \valpha \eqspace &
  \calL(\valpha) =
  \sum_{i=1}^N \alpha_i -
  \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N y_i y_j \alpha_i \alpha_j \vx_i^T \vx_j \\
  %
  \text{subject to } \eqspace &
  \valpha \ge \vv{0}, \sum_{i=1}^N \alpha_i y_i = 0 \\
  %
  & \vw = \sum_{i=1}^N \alpha_i y_i \vx_i \\
  \end{aligned}
  $$

- The equation for $\vw$ is not a constraint, but it computes $\vw$ (the plane)
  given $\valpha$, while $b$ is given by
  $\min |\vw^T \vx_i + b| = 1$

* Dual Form of SVM as QP Problem
- The dual form of SVM problem is a convex quadratic programming problem, in the
  form:

  $$
  \begin{aligned}
  \text{minimize with respect to } \valpha \eqspace &
  \vv{1}^T \valpha - \frac{1}{2} \valpha^T \mQ \valpha\\
  \text{subject to } \eqspace &
  \valpha \ge 0, \vy^T \valpha = 0\\
  \end{aligned}
  $$

  where:
  - the matrix is $\mQ = \{ y_i y_j \vx_i^T \vx_j \}_{ij}$
  - $\valpha$ is the column vector $(\alpha_1, \ldots , \alpha_N)$

* Solving Dual Formulation of SVM Problem (1/2)
- **Solving convex problem** for $\alpha$
  - Feeding this problem to a QP solver, you get the optimal vector $\valpha$

- **Compute hyperplane** $\vw$

  - From $\valpha$ recover the plane $\vw$ from the equation:
    $\vw = \sum_{i=1}^N \alpha_i y_i \vx_i$
  - Looking at the optimal $\alpha_i$, you can observe that many of them are $0$
  - This is because when you applied the Lagrange multipliers to the inequalities:
    $y_i (\vw^T \vx_i + b) \ge 1$, you got the KKT condition:
    $$
    \alpha_i (y_i (\vw^T \vx_i + b) - 1) = 0
    $$
  - From these equations, either
    - $\alpha_i = 0$ and $\vx_i$ is an \textit{interior point} since it has
      non-null distance from the plane (i.e., slack) from the plane; or
    - $\alpha_i \ne 0$ and the slack is $0$, which implies that the $\vx_i$ point
      touches the margin, i.e., it is a \textit{support vector}

* Solving Dual Formulation of SVM Problem (2/2)
::: columns
:::: {.column width=50%}
- Thus the hyperplane is only function of the support vectors:
  $$
  \vw
  = \sum_{i=1}^N \alpha_i y_i \vx_i
  = \sum_{\vx_i \in \text{SV}} \alpha_i y_i \vx_i
  $$
  since only for the support vectors $\alpha \ne 0$
  - The $\alpha_i \ne 0$ are the real degree of freedom

::::
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson04_Support_Vectors.png)

::::
:::

- **Compute** $b$
  - Once $\vw$ is known, you can use any support vector to compute $b$:
    $$
    y_i (\vw^T \vx_i + b) = 1
    $$

* Support Vectors and Degrees of Freedom for SVM
- The number of support vectors is related to the degrees of freedom of the
  model
- Because of the VC dimension, you have an in-sample quantity to bound the
  out-of-sample error:
  $$
  E_{out} \le E_{in} + c \frac{\text{num of SVs}}{N - 1}
  $$
- You are "guaranteed" to not overfit

* Non-Linear Transform for SVM
- $\Phi: \calX \to \calZ$ transforms $\vx_i$ into
  $\vz_i = \Phi(\vx_i) \in \bbR^{\tilde{d}}$ with $\tilde{d} > d$

- Transform vectors through $\Phi$ and apply SVM machinery

- Dual SVM formulation in $\calZ$ space:
  $$
  \calL(\valpha) =
  \sum_{i=1}^N \alpha_i -
  \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j \vz_i^T \vz_j
  $$

- Note:
  - Optimization problem has same number of unknowns as original space (number
    of points $N$)
  - Support vectors live in $\calZ$: they have $\alpha = 0$. In $\calX$, they
    are pre-images of support vectors
  - Decision boundary and margin can be represented in original space (not
    linear)

* Non-Linear Transforms for SVM vs Others
- In SVM the non-linear transform does not change the number of unknowns and
  degrees of freedom of the model
- This is different from transforming the variables in a linear problem, since
  in that case the number of unknowns changes

* SVM in Higher Dimensional Space
- **Pros**
  - You don't pay the price in terms of complexity of optimization problem
    - Number of unknowns is still $N$ (different than a linear problem)
  - You don't pay the price in terms of increased generalization bounds
    - Number of support vectors is $\le N$
    - This is because each hypothesis $h$ can be complex but the cardinality of
      the hypothesis set $\calH$ is the same

- **Cons**
  - You pay a price to compute $\Phi(\vx_i)^T \Phi(\vx_j)$, since $\Phi$ could be
    very complex
    - The kernel trick will remove this extra complexity by doing
      $\Phi(\vx_i)^T \Phi(\vx_j) = K_{\Phi}(\vx_i, \vx_j)$

* Non-Linear Transform in SVM vs Kernel Trick
- The trivial approach is:
  - Transform vectors with $\Phi(\cdot)$
  - Apply all SVM machinery to the transformed vectors
  - **Cons**: $\Phi$ might be very complex, e.g., potentially exponential number
    of terms

- You can express the SVM problem formulation and the prediction in terms of a
  kernel
  $$
  K_{\Phi}(\vx, \vx') = \Phi(\vx)^T \Phi(\vx') = \vz^T \vz'
  $$
  - You only need the kernel $K_{\Phi}(\vx, \vx')$ of the transformation
    $\Phi(\cdot)$ and not $\Phi(\cdot)$ itself

* SVM in Terms of Kernel: Optimization Step
- When you build the QP formulation for the Lagrangian to compute the $\alpha$ we
  can use $K_{\Phi}(\vx_i, \vx_j)$ instead of $\vz_i^T \vz_j$
  $$
  \calL(\valpha) =
  \sum_{n=1}^N \alpha_n -
  \frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N y_n y_m \alpha_n \alpha_m
  K_{\Phi}(\vx_n, \vx_m)
  $$
- $\vz_n$ does not appear in the constraints

  $$
  \valpha \ge \vv{0}, \valpha^T \vy = 0
  $$

* SVM in Terms of Kernel: Prediction Step
- You need only inner products to compute a prediction for a given $\vz$
- In fact to make predictions, you replace the expression of
  $\tilde{\vw} = \sum_{i : \alpha_i > 0} \alpha_i y_i \vz_i$ in
  $h(\vx) = \sign(\vw^T \Phi(\vx) + b)$, yielding:

  $$
  h(\vx)
  = \sign(\sum_{i : \alpha_i > 0} \alpha_i y_i K_{\Phi}(\vx_i, \vx) + b)
  $$

  where $b$ is given by $y_i(\vw^T \vz_i + b) = 1$ for any support vector
  $\vx_m$ and thus

  $$
  b = \frac{1}{y_m} - \sum_{i: \alpha_i > 0} \alpha_i y_i K_{\Phi}(\vx_i, \vx_m)
  $$

* Implications of Kernel Trick in SVM
- The "kernel trick" is a computational shortcut:
  - Use the kernel of the transformation instead of the transformation itself

- To use SVMs, compute inner products between transformed vectors $\vz$

- The kernel trick implies:
  - No need to compute $\Phi()$: use the kernel $K_{\Phi}$, not the
    transformation $\Phi$
  - No need to know $\Phi$: with function $K_{\Phi}$ as an inner product, use
    SVM machinery without knowing $\calZ$ space or transformation $\Phi$
  - $\Phi$ can be impossible to compute: $K_{\Phi}$ can correspond to a
    transformation $\Phi$ to an infinite dimensional space (e.g., Gaussian
    kernel)

* Non-Linearly Separable SVM Problem

::: columns
:::: {.column width=75%}

- In general there are 2 types of non-separable data sets:
  - Slightly non-separable
    - Few points crossing the boundary $\implies$ use soft margin SVMs
  - Seriously non-separable
    - E.g., the class inside the circle $\implies$ use non-linear transforms /
      kernels

::::
:::: {.column width=50%}

![](msml610/lectures_source/figures/Lesson04_Slightly_non_separable_dataset.png)

![](msml610/lectures_source/figures/Lesson04_Slightly_non_separable_dataset2.png)

![](msml610/lectures_source/figures/Lesson04_Non_separable_dataset.png)

::::
:::

- In practice, both issues are present and one can combine soft margin SVM and
  non-linear transforms

* Soft-Margin SVM for Better Generalization on Linearly-Separable Data Sets
- Sometimes, even if the data is linearly separable, one can get better
  $E_{out}$ using soft margin SVM at the cost of worst $E_{in}$
  - Usual trade off between in-sample and out-of-sample performance
  - E.g., in the data set there are a few of outliers that are forcing a smaller
    margin than what we could obtain if we ignore them, in order to get all the
    points classified correctly
- If $C$ parameter is very large the SVM optimization requires to make the error
  very small, and this might trade off a large margin with getting all the
  classification right

* Primal Formulation for Soft Margin SVM
- We want to introduce an error measure based on the margin violation for each
  point, so instead of the constraint:
  $$
  y_i (\vw^T \vx_i + b) \ge 1 \text{ (hard margin)}
  $$
  we use:
  $$
  y_i (\vw^T \vx_i + b) \ge 1 - \xi_i, \text{ where } \xi_i \ge 0
  \text{ (soft margin)}
  $$
- The cumulative margin violation is $C \sum_{i=1}^N \xi_i$

- The soft margin SVM optimization (primal form) is:

  $$
  \begin{aligned}
  \text{find $\vw, b, \vxi$} & \\
  \text{minimize } \eqspace & \frac{1}{2} \vw^T \vw + C \sum_{i=1}^N \xi_i\\
  \text{subject to } \eqspace & y_i(\vw^T \vx_i + b) \ge 1 - \xi_i \; \forall i\\
  & \xi_i \ge 0
  \end{aligned}
  $$

* Classes of Support Vectors for Soft Margin SVM
- There are 3 classes of points:
- \textit{margin support vectors}: they are exactly on the margin defining it
  - In primal form: $y_i (\vw^T \vx_i + b) = 1 \iff \xi_i = 0$
  - In dual form: $0 < \alpha_i < C$
- \textit{non-margin support vectors}: they are inside the margin and classified
  correctly or not
  - In primal form: $y_i (\vw^T \vx_i + b) < 1 \iff \xi_i > 0$
  - In dual form: $\alpha_i = C$
- \textit{non-support vectors}, i.e., interior points:
  - In primal form: $y_i (\vw^T \vx_i + b) > 1$
  - In dual form: $\alpha_i = 0$

* Intuition for C in SVM
- $C$ represents how much penalty we incur for passing the margin
- If $C$ is large, then SVM will try to fit all the points to avoid being
  penalized (lower bias / higher variance)
  - $C \to \infty$ which yields hard-margin SVM
- If $C$ is small, then we allow margin violations (higher bias / lower
  variance)
- From another point of view $C \propto \frac{1}{\lambda}$, so large $C$ means
  small $\lambda$ and thus small regularization

- $C$ is chosen through cross validation, like any regularization parameter

* Multi-Class Classification for SVM
- Often SVM packages have built-in multi-class classification
- Otherwise use the one-vs-all method:
  - Train $K$ SVMs distinguishing each class from the rest using one-hot
    encoding, getting SVM parameters $(\vw_1, b_1), ..., (\vw_K, b_k)$
  - For a new example $\vx$ compute $\vw_i^T \vx + b_i$ for all the models
  - Pick the model that gives the largest positive value (i.e., more confident
    about its class vs the rest of the classes)

## #############################################################################
## Similarity-Based Models
## #############################################################################

* Similarity-Based Models: Intuition
- Idea: the model evaluated in one point $h(\vx)$ is affected by:
  - Other data points in the training set $(\vx_n, y_n) \in D$
  - The effect is based on the distance $d(\vx, \vx_n) = \|\vx - \vx_n\|$

- In other words, the model is the sum of the effect of each point in the
  training set, scaled down by the distance
  - The model is a superposition of effects
  $$
  h(\vx)
  = \sum_i \text{ effect of } h(\vx_i) \text{ scaled by } d(\vx, \vx_i)
  $$

- This approach allows to define complex decision boundaries

* Similarity-Based Models: Gaussian Kernels
- Consider a Gaussian kernel with a "landmark" point $\vx_i$ and a similarity
  distance defined as:
  $$
  K(\vx, \vx_i) = \exp(- \frac{\| \vx - \vx_i \| ^ 2}{2 \sigma^2})
  $$

- E.g., the hypothesis model has the form:
  $$
  h(\vx)
  = \sum_{i=1}^3 y_i K(\vx, \vx_i)
  = y_1 K(\vx, \vx_1) + y_2 K(\vx, \vx_2) + y_3 K(\vx, \vx_3)
  $$
  - The response is weighting the responses $y_i = \{0, 1\}$ through the
    similarity of $\vx$ from the landmark points
  - This can be seen by plotting $h(\vx)$ on a plane

// TODO: Add plot

* Radial Basis Function Model
- Aka RBF

- The model form for **regression** is:
  $$
  h(\vx) = \sum_{i=1}^{N} w_i \exp(-\gamma \|\vx - \vx_i\| ^2)
  $$
  where:
  - If $\gamma$ is small, the exponential falls off slowly, and multiple
    training points affect a point between them
  - If $\gamma$ is large, there are spikes centered in the training points and
    nothing outside

// TODO: Add plot

- For **classification** use a similar approach to "linear regression for
  classification"
  - Fit a regression model:
    $$
    s(\vx) = \sum_{i=1}^{N} w_i \exp(-\gamma \|\vx - \vx_i\| ^2)
    $$
  - Take the sign to make predictions:
    $$
    h(\vx) = \sign(s(\vx))
    $$

* RBF: Block Diagram
- One can represent graphically an RBF model
  - The (fixed by learning) params, one for each training point
  - The weights depending on the distance of the input to the examples
  - The weighted params are summed together

// TODO

* RBF: Reducing Model VC Dimension
- Some variants for RBF:
  - Add a bias term
  - Use different $\gamma_i$ for each point, i.e., different influence of
    different points
  - Then the number of degrees of freedom increases even more

- In RBF there are many parameters:
  - There are as many parameters $\vw$ as data points $N$ (e.g., $N = 10^{9}$)
  - One parameter $w_i$ per training point (e.g., $N$ can be $10^6$)
  - Cons: negative consequences on generalization error

- To reduce number of parameters
  - Pick $K \ll N$ centers $\vmu_1, ... , \vmu_K$ instead of $\vx_1, ... , \vx_N$
    - We can use $k$-means clustering to find the centers
    - Note: this doesn't burn the training set since this is unsupervised
      learning and we don't use the labels
  - Same as RBF model using the distances from the centers of the clusters:
    $$
    h(\vx) = \sum_{i=1}^{K} w_i \exp (-\gamma \|\vx - \vmu_i\|^2)
    $$

- Still a lot of parameters because:
  - $K$ (scalar) weights $w_k$
  - $K$ reference points $\vmu_k$ ($d$-dimensional vectors)

* RBF: Learning Models
- We want to learn $w_i, \gamma$, with fixed centers $\vmu_i$, for an RBF model:
  $$
  h(\vx) = \sum_{i=1}^{K} w_i \exp (-\gamma \|\vx - \vmu_i\|^2)
  $$

- **Minimize:**
  $$
  E_{in} = \sum_i (h_{\vw, \gamma}(\vx_i) - y_i)^2 = f(\vw, \gamma)
  $$
  - Use an iterative approach (e.g., EM, coordinate descent)

* Learning RBF Models
- Use iterative approach (similar to EM algorithm):
  - Fix $\gamma$, solve for $\vw$ (using one-step learning)
  - Fix $\vw$, solve for $\gamma$ (with gradient descent)

- **Step 1**
  - Assume that $\gamma$ is known and fixed
  - Learn $\vw$

- We can impose perfect interpolation:
  $$E_{in} = \frac{1}{n} \sum (h(\vx_i) - y_i)^2 = 0$$
  and get the problem:
  $$
  h(\vx_j)
  = \sum_i w_i \exp(-\gamma \|\vx_i - \vx_j\| ^2)
  = \sum_i w_i \phi_{i,j}
  = \vphi_j^T \vw = y_i
  $$
  - We have $N$ equations (one per point) and $N$ unknowns $\vw$
  - $\mat{\Phi}$ is known since it is function of the data set and $\gamma$
- The problem in matrix form is like: $\mat{\Phi} \cdot \vw = \vy$

* Learning RBF Models

- The problem in matrix form is like
  $$\mat{\Phi} \cdot \vw = \vy$$
  - If $\mat{\Phi}$ is invertible, then $\vw = \mat{\Phi}^{-1} \vy$
    - We have the desired values on the training points and the exponential
      interpolates in the other points
  - If $\mat{\Phi}$ is not invertible, optimize the problem in a least square
    sense:
    $$\argmin_{\vw} E_{in} = \sum_i (h(\vx_j) - y_i)^2$$
    - Compute the pseudo-inverse (assuming $\mat{\Phi}^T \mat{\Phi}$ is
      invertible)
    - Assign the weights as:
      $$\vw = (\mat{\Phi}^T \mat{\Phi})^{-1} \mat{\Phi}^T \vy$$

- **Step 2**
  - Assume that $\vw$ is known and fixed
  - Learn $\gamma$

* RBF Network vs Neural Networks
- Consider the case of regression model for Neural Networks and RBF model
  - RBF: 
    $$h(\vx)
    = \sum_i w_i e^{-\gamma \|\vx - \vx_i\|^2}
    = \vw^T \vv{phi}$$
  - Neural networks:
    $$h(\vx)
    = \Theta(\vw^{(L)}T \vx^{(L)})
    = \Theta(\vw^{(L)}T \vv{Theta}(\mW^{(L-1)} ...))$$

- Difference:
  - RBF has a single layer
  - Neural networks have multiple layers

- Similarities:
  - Combine features together with weights with a dot product
  - Have features extracted from the inputs
    - For RBF features are $e^{-\gamma \|\vx - \vx_i\|^2}$ fixed and always $> 0$
    - For NN hidden layers synthesize features that can be $> 0$ or $< 0$

* RBF Network vs SVM
- The model form is the same:
  - RBF:
    $$h(\vx) = \sign(\sum_i w_i e^{-\gamma \|\vx - \vx_i\|^2}$$
  - SVM:
    $$h(\vx) = \sign(\vw^T \vx + b)$$ 

- The interpretation is completely different (interpolation vs large margin)
  - In RBF all vectors (or centers of few clusters) contribute to the model
  - In SVM only support vectors contribute to the model

* K-Nearest Neighbor (KNN) Model
- The model is like:
  $$
  h_{\vw}(\vx) = \frac{1}{n} \sum_{\vx_i \text{closest to } \vx} w_i
  $$

- **Idea**:
  - Closeness implies a distance (e.g., euclidean metric) or similarity (e.g., a
    kernel)
  - Consider the $k$ closest points to the evaluation point $\vx$
  - Take an average of their response

* KNN: Intuition of Number Degrees of Freedom
- Nearest neighbor model ($k = 1$)
  - Adopt the response of the closest point to the point we are evaluating $\vx$
  - Like Voronoi tessellations: each point has a region for which it is the
    closest point and assigns its output to that region

- One could think there is a single parameter for KNN, i.e., $k$, since this is
  the hyperparameter we need to learn
  - For $k = 1$ there are $N$ neighborhoods, one around each point of the
    training set
  - For $k = N$ there is a single neighborhood
  - So the intuition is that the effective number of parameters is $\frac{N}{k}$
    since one can imagine there are $N / k$ non-overlapping neighborhoods

* KNN: Assumptions on the Data
- KNN makes no assumption on the data
  - This is the opposite of the linear model where there is a strong assumption on
    the data
- KNN assumes locality in parameter space
  - The model is constant in the neighborhood of an example
  - E.g., $k = 1$ we consider the Voronoi tesselation low-bias / high-variance
  - E.g., $k = N$ we consider the average value (high-bias / low-variance)

* Training and Test Error for KNN
- For $k=1$ a KNN model
  - Makes no error on the training set (assuming a non-noisy target), since it
    memorizes the training set (low bias / high variance)
  - $E_{out}$ is larger than $E_{in}$
- If $k$ increases the training error $E_{in}$ increases but the test error
  $E_{out}$ decreases until it starts increasing again
  - We have the typical behavior of model complexity as in bias-variance diagrams

* KNN vs RBF Models
- **Similarities**
  - K-Nearest Neighbor is a \textit{discrete} version of the RBF model

- **Differences:**
  - Consider only the $k$ \textit{closest examples} to the point $\vx$ (not all
    examples in the training set)
  - Use a \textit{constant kernel} (responses are not weighted by distance)

## #############################################################################
## Clustering
## #############################################################################

* K-Means Clustering: Problem Formulation
- We have $N$ \textit{unlabeled} points $\{\vx_1, \vx_2, ..., \vx_N\}$
- We want to partition the points into $K$ clusters $S_1, ..., S_K$
  - Each cluster is defined by its center $\vmu_k$
  - Each point $\vx_i$ is assigned to cluster $c(\vx_i)$
  - The unknowns are: $c(\vx_1), ..., c(\vx_N), \vmu_1, ..., \vmu_K$

- We want to minimize the distance between each $\vx_i$ and the assigned center
  $\vmu_k$ where $k = c(\vx_i)$:
  \begin{alignat*}{3}
  J(c_1, ..., c_N, \mu_1, ..., \mu_K) 
  &= \sum_{k=1}^K \sum_{\vx_n \in S_k} \|\vx_n - \vmu_k\| ^ 2
  & \text{(scanning the clusters)}
  \\
  &= \sum_{i=1}^N \| \vx_i - \vmu_{c(\vx_i)} \| ^ 2
  & \text{(scanning the points)} \\
  \end{alignat*}

- K-means clustering is NP-hard (combinatorial) and thus intractable
  - In fact there are $K^N$ possible assignments

* K-Means Clustering: Lloyd'S Algorithm
- We start picking a random assignment of $N$ points to the $K$ clusters
  - Better than picking randomly the centroids of the clusters
- Each iteration does 2 steps

- **Step 1**: Move centroid
  - We move the centroid of each cluster to the mean point of the current cluster
  - This loop iterates over the $K$ clusters
  - $\vmu_k \leftarrow \frac{1}{|S_k|}\sum_{\vx_n \in S_k}\vx_n$

- **Step 2**: Cluster assignment

  - Each $\vx_n$ is assigned to the closest cluster based on its center
  - This loop iterates over the $N$ points
  - $S_k \leftarrow
    \{\vx_n: \| \vx_n - \vmu_k \| \le \|\vx_n - \vmu_l\| \;
    \forall l \ne k \}$

* K-Means Clustering: Convergence
- K-means algorithm converges since:
  - There is a finite (though large $K^N$) number of possible partitionings, and
    thus a finite number of possible values of the objective functions
  - The objective function $J(\cdot)$ is always decreased

- The objective function is always decreasing
  - The cost function $J(\vmu_1, ..., \vmu_K, c_1, ..., c_N)$ can be seen as a
    function of:
    - The centroids $c_1, .., c_N$
    - The point assignments $\vmu_1, ..., \vmu_K$
  - At each step, K-means minimizes $J$ with respect to:
    - The centroids (keeping the assignments fixed); then
    - The assignments (keeping the centroids fixed)
  - It is like coordinate descent

- Generally, it converges to a local minimum
  - Run K-means multiple times using different random initializations
  - Pick the best result

* K-Means Clustering: Non-Separable Clusters
- For simplicity, we imagine a clear separation between clusters
  - In practice, clusters (especially in high dimensions) are not obviously
    separable

- We can use K-means on data that is not obviously separated
  - E.g., market segmentation
  - E.g., t-shirt sizing
    - Collect height and width of a population of customers
    - Run K-means
    - Find the optimal way to split the population into 3 sizes (S, M, L)

* Choosing the Number of Clusters
- It's often unclear how many clusters $K$ exist in the data
  - E.g., visual analysis can be inconclusive with 2D or 3D data
  - Even more difficult in high dimensional spaces

1. Elbow Method
   - Vary the number of clusters $K$
   - Compute the optimal cost function $J(\cdot)$
   - Choose $K$ at the "elbow" point if visible
   - The elbow is absent if the curve resembles a hyperbole $\approx 1/K$

2. End-to-end approach
   - Choose $K$ to optimize later processing stages
   - E.g., More t-shirt sizes (i.e., more clusters) $\implies$
     - Satisfy customers
     - Complicates manufacturing
     - Increases stocking and inventory management

* Clustering: Interpretation of Clusters
- Often we want to give a meaning to clusters

- Cluster meaning is difficult to automate: it must be interpreted manually
  - Examine the cluster centroids
    - Centroid values show the "typical" point in each cluster
    - High, low, or zero feature values highlight key characteristics

  - Analyze the distribution of features per cluster
    - Plot histograms or boxplots for each feature
    - Identify features that vary sharply across clusters

  - Visualize clusters in 2D or 3D
    - E.g., PCA, t-SNE, UMAP
    - Helps understand separation and internal structure

  - Identify common traits in each cluster
    - For categorical features, count dominant categories

  - Compare clusters to external labels if available
    - See if clusters align with known real-world groups

  - Train a classifier like decision tree
    - Important features for predicting cluster reveal their meaning

- Example: Customer Segmentation
  - Features: (Age, Annual Income, Spending Score)
  - Cluster 1
    - $(25 \text{ yrs}, 30K, 90)$ $\to$ "Young Big Spenders"
  - Cluster 2:
    - $(50 \text{ yrs}, 80K, 40)$ $\to$ "Comfortable
    Mid-Lifers"
  - Cluster 3:
    $(35 \text{ yrs}, 120K, 20)$ $\to$ "High Income, Low Spending Customers"

## #############################################################################
## Anomaly Detection
## #############################################################################

* Anomaly Detection: Problem Formulation
- **Problem**:
  - We have $\{\vx_1, ..., \vx_N\}$ examples with features $\vx \in \bbR^P$ for
    good / non-anomalous instances
  - We want to find a way to detect bad / anomalous instances

- **Algorithm**:
  - We don't know what makes a _"bad instances"_
  - We learn what _"good instances"_ have in common using unsupervised learning
    - I.e., find the distribution for _"good instances"_ $\vx_i$, 
      $\Pr(\vx \text{ is good})$
  - Pick features
    - The goal is to find "sensitive" features, i.e., features that might take
      large or small values in case of an anomaly
    - E.g., ratio between CPU load and network traffic
  - Estimate the distribution $\Pr(\vx \text{ is good})$
  - Choose the threshold $\varepsilon$
  - For a new instance $\vx_{new}$, if
    $$\Pr(\vx_{new} \text{ is good}) \le \varepsilon$$
    we flag it as an anomaly

* Anomaly Detection: Example of Aircraft Engines
- **Problem**
  - Test aircraft engines to identify anomalies in a new engine

- **Solution**:
  - Features $\vx_i$ can be:
    - Heat generated
    - Vibration intensity
    - ...
  - Collect data for all engines
  - Model a PDF $\Pr(\vx \text{ is good})$
  - Decide if a new engine is acceptable $\Pr(\vx_{good}) \le \varepsilon$
    or needs more testing

* Anomaly Detection: Example of Hacked Account
- **Problem**
  - Find if an account for a given user $i$ was hacked

- **Solution**:
  - Model features that represent "user $i$ activity" 
  - Features $\vx_i$ can be:
    - How many times s/he logs a day
    - How many times s/he fails to enter the password
    - How fast s/he types
    - How many pages s/he visits
    - How many times s/he posts comments
    - ...
  - Model a PDF $\Pr(\vx \text{ is good})$
  - Identify unusual users by checking $\Pr(\vx_{new}) \le \varepsilon$

* Anomaly Detection: Example of Computers in Data Center
- **Problem**
  - Monitor servers in a data center to find malfunctioning or hanged servers

- **Solution**:
  - Features $\vx_i$ can be:
    - Memory in use
    - CPU load
    - Network traffic
    - Number of reads/writes per sec
    - CPU load / network activity
    - ...
  - Model a PDF $\Pr(\vx \text{ is good})$
  - Identify unusual users by checking $\Pr(\vx_{new}) \le \varepsilon$

* Using a Gaussian Model for Anomaly Detection
- Aka "density estimation"
- Given $N$ examples $\vx_1, ..., \vx_N \in \bbR^p$
- Ensure that the features have a Gaussian distribution
  - If not, we can apply some transformations, e.g., $\log(x_i + k)$
- Estimate the parameters of the Gaussian model $f_X(\vx)$

- Given a new example $\vx_{new}$, compute:
  $$
  \Pr(\vx_{new} \text{ is good}) \le \varepsilon
  $$
  to flag an anomaly

* Estimate Univariate Gaussian Model
- We have $N$ (scalar) examples $\vx_1, ..., \vx_N \in \bbR$ for "good instances"

- Assume the data is generated by a Gaussian distribution
  $$X \sim \calN(\mu, \sigma)$$
  which has a PDF:
  $$
  f_X(x; \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} \exp(- \frac{(x - \mu)^2}{2 \sigma^2})
  $$

- Estimate mean and sigma with maximum likelihood:
  \begin{alignat*}{2}
  &\mu = \frac{1}{N}\sum_i x_i \\
  &\sigma^2 = \frac{1}{N-1}\sum_i (x_i - \mu)^2 \\
  \end{alignat*}

* Estimate Multivariate Independent Gaussian Model
- We have $N$ examples $\vx_1, ..., \vx_N \in \bbR^p$ for "good instances"

- Assume independence of the features, the PDF of a multi-variate Gaussian $X$
  is:
  $$
  f_X(\vx; \vmu, \vsigma)
  = \prod_{i=1}^p f_{X_i}(x_i; \mu_i, \sigma_i)
  $$

- Infer the parameters $\mu_i$ and $\sigma_i$ using discrete formulas to get the
  complete model
- Vectorize the computation

* Estimate a Multi-Variate Gaussian Model
- **Problem**:
  - Sometimes features vary together (e.g., network use and CPU load), so using
    the independent assumption might cause misclassifications
  - E.g., the components of an example $\vx_{new}$ are within the expected range
    but together make no sense
    - E.g., low network use with high CPU load

- **Solution 1**:
  - Engineer features to create features that are high in case of an anomaly
  - This bridges the gap for the correlation between variables (that can't be
    modeled in independent Gaussian models)

- **Solution 2**:
  - Estimate the entire multivariate model, instead of assuming independence and
    estimating one marginal Gaussian at a time

* Estimate a Multi-Variate Gaussian Model
- The PDF of a multi-variate Gaussian is:
  $$
  f_X(\vx; \vmu, \mSigma) =
  \frac{1}{(2 \pi)^\frac{n}{2} |\mSigma|^\frac{1}{2}}
  \exp(-\frac{1}{2} (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu))
  $$

- We estimate:
  $$
  \vmu \in \bbR^d = \frac{1}{N}\sum_{k=1}^N \vx_k
  $$
  and
  $$
  \mSigma \in \bbR^{d \times d}
  = \{s_{ij}\}
  = \frac{1}{N - 1} \sum_{k=1}^N (\vx_k - \vmu) (\vx_k - \vmu)^T
  $$

- This model requires more examples to train since there are more parameters to
  fit

- If there is independence between the variables, then the multivariate Gaussian
  is decomposed into a product of Gaussian distributions

* Evaluate Anomaly Detection Systems
- To evaluate models one needs to:
  - Compare different models
  - Tune hyperparameters (e.g., $\varepsilon$) of models
  - Estimate out-of-sample error

- As always we should use a single real number for comparison
  - Use any classification metric, e.g.,
    - True/false positive/negative rate
    - Precision or recall
  - F-score

- _Labeled_ data is still needed to rate models
  \begin{alignat*}{2}
  y = 0 & \text{good} \\
  y = 1 & \text{anomalous} \\
  \end{alignat*}

* Evaluate Anomaly Detection Systems
- Often the number of anomalous examples $y = 1$ is much smaller than good
  examples with $y = 0$
  - E.g., 10,000 good vs 20 bad examples
  - Important to address class imbalance for accurate model performance

- **Algorithm**:
  - Pick 60% of data with $y = 0$ to train (only on the good examples)

  - Split the remaining data $y = 0$ and $y = 1$ into validation and test sets
    - Ensure both sets are representative of the overall dataset
    - Train, validation, and test sets should have no overlap but have the same
      characteristics
    - This helps in evaluating the model's performance accurately

  - Use the validation set to compare models, estimate the hyper parameters
    - E.g., $\varepsilon$ is the threshold for anomaly detection 

  - Use the test set to evaluate the final model
    - The model is trained on normal data and tested on both normal and anomalous
      data

- **Example**:
  - In aircraft engine example there are 10,000 good engines ($y = 0$), 20 bad
    engines ($y=1$) 
  - Train set: 6,000 $y=0$ examples
  - Validation set: 2,000 $y=0$ and 10 $y=1$
  - Test set: 2,000 $y=0$ and 10 $y=1$

* Anomaly Detection vs Supervised Learning
- Even in unsupervised learning, we need \textit{labeled} data for model
  evaluation

- What's the difference with supervised learning?
  - In anomaly detection/unsupervised learning, we train only on good examples
  - In supervised learning, we train on both good and bad examples

- Use:
  - Anomaly detection/unsupervised learning:
    - When learning only from good examples due to few anomalous examples
    - When having a strong prior on the model
    - When future anomalous examples are unknown (no prior)
  - Supervised learning when the training set has less skewed classes
  - It is not a clear-cut decision
