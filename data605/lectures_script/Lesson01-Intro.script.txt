# ##############################################################################
# Slide 1: Invariants of a Class Lecture
# ##############################################################################

- Invariants are key concepts that remain constant throughout the course. The
  focus is on understanding these concepts intuitively, which means grasping the
  basic ideas without getting bogged down in complex details. This approach
  helps students build a strong foundation in the subject matter.

- The course includes interactive Jupyter notebook tutorials. These tutorials
  are designed to be completed at home, allowing students to learn at their own
  pace. Over time, videos will be added to these tutorials to enhance
  understanding and provide additional guidance.

- The class flow is dynamic, alternating between different teaching methods.
  This includes using slides for structured information, the whiteboard for
  spontaneous explanations, and tutorials for hands-on practice. This variety
  keeps the class engaging and caters to different learning styles.

- Labs are an essential part of the course. They provide an opportunity to
  review complete class project examples, which helps students understand how
  theoretical concepts are applied in real-world scenarios. Collaboration on
  class projects is encouraged, fostering teamwork and communication skills.

Transition: Now, let's explore the resources that will support your learning
journey in this class.

# ##############################################################################
# Slide 2: Books of the Class
# ##############################################################################

- The slides used in this class are carefully curated from various sources,
  including books, technical articles, and the Internet. This ensures that the
  content is comprehensive and up-to-date with the latest developments in the
  field.

- The slides are designed to be self-sufficient, meaning they contain all the
  necessary information for understanding the topics covered. This allows
  students to rely on them as a primary resource for studying and revising the
  material.

Transition: With a solid understanding of the resources, let's delve into the
learning outcomes you can expect from this course.

# ##############################################################################
# Slide 3: Learning Outcomes
# ##############################################################################

- By the end of this course, students will be able to model and reason about
  data effectively. This involves understanding how data is structured and how
  it can be used to draw meaningful conclusions.

- Students will learn to process and manipulate data using tools like Python and
  Pandas. These skills are crucial for handling large datasets and extracting
  valuable insights.

- The course introduces a variety of data models, such as relational, NoSQL, and
  graph databases. Students will learn to choose the appropriate data model for
  different applications, which is essential for efficient data management.

- Students will gain experience using data management systems like PostgreSQL,
  MongoDB, and HBase. They will learn to select the right system for specific
  scenarios, ensuring optimal performance and scalability.

- Building data processing pipelines is another key outcome. Students will use
  tools like Docker and Airflow to automate and streamline data workflows.

- Finally, students will build a big-data system from start to finish as part of
  a class project. This hands-on experience is invaluable for understanding the
  complexities of big data systems. Additionally, students will have the
  opportunity to contribute to an open-source project, enhancing their practical
  skills and professional portfolio.

# ##############################################################################
# Slide 1: Tools We Will Learn to Use
# ##############################################################################

In this course, we will explore a variety of tools essential for machine
learning and big data. We'll start with programming languages, focusing on
Python, which is widely used for data analysis and machine learning tasks.
Development tools are crucial, and we'll cover Bash/Linux OS for command-line
operations, Git for version control, and GitHub for collaboration through Pull
Requests and issue tracking. Jupyter notebooks will be used for interactive
coding, and Docker will help us create consistent development environments.

For big data, we'll delve into Extract-Transform-Load (ETL) pipelines, which are
vital for data processing. We'll explore relational databases like PostgreSQL
and NoSQL databases such as HBase, MongoDB, Couchbase, and Redis, which are
designed for different data storage needs. Graph databases like Neo4j and
computing frameworks like Hadoop and Spark will also be covered. Airflow will be
introduced as a workflow manager, and we'll explore cloud services like AWS for
scalable computing resources. Tutorials will be provided to help you get
hands-on experience with these tools.

Transitioning to the next slide, let's discuss what you need to do to prepare
for the course.

# ##############################################################################
# Slide 2: Todos
# ##############################################################################

To succeed in this course, there are several tasks you need to complete. First,
study the slides and materials provided. Make sure to enable notifications on
the DATA605 ELMS/Canvas site to stay updated. Familiarize yourself with the
contact information for me and the TAs. Check the DATA605 Schedule and GitHub
repo for important resources and updates. The FAQs document is also a valuable
resource for common questions.

Setting up your computing environment is crucial. Install Linux or VMware and
Docker on your laptop, following the instructions in the class repo. Bringing
your laptop to class is important for hands-on activities. While lessons are
recorded, attending class when possible is beneficial for interactive learning.

Now, let's move on to how your performance will be evaluated in this course.

# ##############################################################################
# Slide 3: Grading
# ##############################################################################

Your grade in this course will be based on quizzes and a final project. Quizzes
make up 40% of your grade and are designed to encourage consistent study
throughout the semester. Each quiz will have 20 multiple-choice questions
covering the previous two lessons, and you'll have 20 minutes to complete them.
There will be 4-5 quizzes in total.

The final project is a significant part of your grade, accounting for 60%. It
requires a comprehensive application of the course concepts in a big data
project using Python. You can choose a topic from a provided list and work
individually or in a group. This project is an opportunity to demonstrate your
understanding and skills in a practical setting.

With this understanding of the grading structure, you're now equipped to focus
on your learning journey in this course.

# ##############################################################################
# Slide 1: Class Projects
# ##############################################################################

In this slide, we discuss the structure and expectations for class projects. The
main idea is to build a project titled "Build X with Y," where X represents a
use case and Y is the technology used. The first step is to study and describe
the chosen technology Y. This involves understanding its features, capabilities,
and how it can be applied to solve problems. Next, students are expected to
implement the use case X using the technology Y, which means applying
theoretical knowledge to practical scenarios. To showcase the project, students
should create Jupyter notebooks that demonstrate their work. This is a great way
to present data and code interactively. Additionally, students are encouraged to
commit their code to GitHub and contribute to open-source repositories, which
helps in building a professional portfolio. Writing a blog entry about the
project allows students to reflect on their learning and share insights with a
broader audience. Finally, presenting the project in a video helps in developing
communication skills and provides a platform to explain the project to others.
Students can choose from a list of use cases and technologies, such as big data
or large language models. Projects can be done individually or in groups of up
to three people, with varying levels of difficulty to accommodate different
skill levels.

Transition: Now, let's explore the soft skills necessary for success in the
workplace.

# ##############################################################################
# Slide 2: Soft Skills to Succeed in the Workplace
# ##############################################################################

This slide emphasizes the importance of soft skills in preparing for the
workplace. The goal is to model class projects in a way that mirrors real-world
work environments. Working in a team is crucial, as it helps in learning
collaboration and communication. Designing software architecture using
principles like Object-Oriented Programming (OOP), Agile methodologies, and
design patterns is essential for creating scalable and maintainable software.
Commenting code and writing external documentation such as tutorials, manuals,
and how-tos are important for making the code understandable to others and to
your future self. Reading others' code is a valuable skill that helps in
understanding different coding styles and improving one's own coding abilities.
Following code conventions like PEP8 or Google Code ensures consistency and
readability. Clear communication through emails or platforms like Slack is vital
for effective collaboration. Filing and reproducing bug reports are part of
maintaining software quality. Having an intuition for computer science constants
and a basic understanding of operating systems, including virtual memory and
processes, are foundational skills that enhance problem-solving abilities.

Transition: Let's move on to learn more about the instructor and their
background.

# ##############################################################################
# Slide 3: Yours Truly
# ##############################################################################

This slide provides an introduction to the instructor, GP Saggese. GP Saggese
completed their PhD and Postdoc at the University of Illinois at
Urbana-Champaign between 2001 and 2006. They are currently a lecturer at the
University of Maryland, teaching courses such as Big Data Systems and Advanced
Machine Learning. In the real world, GP Saggese has worked as a research
scientist at companies like NVIDIA, Synopsys, Teza, and Engineers' Gate. They
have also founded three AI and fintech startups: ZeroSoft, June, and Causify AI.
With over 20 academic papers and two US patents, GP Saggese brings a wealth of
knowledge and experience to the classroom. Students can connect with them via
LinkedIn or email for further engagement and learning opportunities. The slide
also includes a picture of GP Saggese in a coding state, providing a personal
touch and visual connection.

Transition: With this background, let's dive into the world of Big Data.

# ##############################################################################
# Slide 1: Data Science
# ##############################################################################

Data science offers several promises that can significantly benefit businesses.
Firstly, it provides a competitive advantage by enabling companies to leverage
data for strategic insights. This means businesses can make more informed
decisions, both strategically and tactically, which can lead to better outcomes.
Additionally, data science helps in optimizing business processes, making them
more efficient and effective.

It's important to note that data science is not a new concept. In the past, it
was known by different names such as operation research in the 1970s and 1980s,
decision support and business intelligence in the 1990s, and predictive
analytics in the early 2010s. The evolution of these terms reflects the growing
importance and application of data-driven decision-making over the years.

What has changed in recent times is the accessibility and ease of learning and
applying data science. Unlike before, there is no longer a need to hire
expensive consulting firms to implement data science solutions. The availability
of open-source tools, such as Python and its associated libraries like numpy,
scipy, Pandas, and sklearn, has democratized access to data science.
Furthermore, the availability of large datasets and affordable computing
resources, such as cloud computing services like AWS and Google Cloud, as well
as GPUs, have made it easier for individuals and organizations to harness the
power of data science.

Transitioning to the next slide, let's explore the motivation behind the growing
importance of data science.

# ##############################################################################
# Slide 2: Motivation: Data Overload
# ##############################################################################

Data science is recognized as a key driver of economic growth, as highlighted by
McKinsey in 2013. The explosion of data across various domains is a significant
factor contributing to this growth. Sensing devices and networks continuously
monitor processes, providing a constant stream of data. For example, sensors can
track the temperature of a room, monitor vital signs, or measure pollution
levels in the air. Additionally, the widespread use of sophisticated
smartphones, owned by 80% of the global population, generates vast amounts of
data.

The internet and social networks have made it easier than ever to publish and
share data. The Internet of Things (IoT) further connects everyday objects to
the internet, from power supplies to toasters, generating even more data. This
phenomenon, known as datafication, transforms various aspects of life into data
streams, such as tracking what you like or enjoy through your online
interactions.

However, this data explosion presents challenges. One major challenge is how to
effectively handle the increasing volume of data. Another is how to extract
actionable insights and scientific knowledge from this data to drive meaningful
outcomes.

As we move forward, let's delve into the scale of data size and understand the
magnitude of data we are dealing with.

# ##############################################################################
# Slide 3: Scale of Data Size
# ##############################################################################

Understanding the scale of data size is crucial in the context of data science.
Starting with the smallest unit, a megabyte (MB) is approximately one million
bytes, which is roughly the size of a typical English book. Moving up, a
gigabyte (GB) is equivalent to one billion bytes or 1,000 MB, which can store
about half an hour of video or the compressed text of Wikipedia without media,
which is around 22GB.

A terabyte (TB) is a significant jump, equating to one million MB. This amount
of storage can hold the human genome, approximately 100,000 photos, and costs
around $50 for a 1TB hard drive or $23 per month on AWS S3. A petabyte (PB) is
1,000 TB, which can store 13 years of high-definition video and costs about
$250,000 per year on AWS S3.

As we scale further, an exabyte (EB) is one million TB, which was the global
yearly internet traffic in 2004. A zetabyte (ZB) is one billion TB, representing
the global yearly internet traffic in 2016 and would fill 20% of Manhattan with
data centers. A yottabyte (YB) is an astronomical figure, costing $100 trillion
and would fill Delaware and Rhode Island with a million data centers. Finally, a
brontobyte is even larger, at 10^27 bytes.

Understanding these scales helps us appreciate the vast amounts of data
generated and the challenges in managing and analyzing such data.

# ##############################################################################
# Slide 1: Constants Everybody Should Know
# ##############################################################################

This slide provides a list of constants that are crucial for understanding the
performance of computing systems. These constants give us an idea of how fast
different operations are in a computer. For instance, a CPU running at 3GHz can
execute an instruction in about 0.3 nanoseconds. This is incredibly fast, but as
we move to accessing data from different levels of memory, the time increases.
An L1 cache reference takes about 1 nanosecond, while an L2 cache reference
takes 4 nanoseconds. Accessing the main memory is significantly slower, taking
about 100 nanoseconds. Reading 1MB from memory can take anywhere from 20 to 100
microseconds, depending on the system. When it comes to storage, an SSD random
read takes about 16 microseconds, which is much faster than a disk seek
operation that takes around 2 milliseconds. Sending data over a network is even
slower; for example, sending 1KB over a network takes about 1 millisecond.
Finally, a packet round-trip from California to the Netherlands takes
approximately 150 milliseconds. Understanding these constants helps in designing
efficient systems and applications by highlighting the importance of minimizing
data movement and optimizing data access patterns.

Transition: Now, let's explore how these constants impact big data applications.

# ##############################################################################
# Slide 2: Big Data Applications
# ##############################################################################

This slide focuses on the application of big data in personalized marketing.
Personalized marketing aims to target each consumer individually by analyzing
their behavior and preferences. For example, Amazon uses a variety of data
points such as shopping history, search, click, and browse activity, as well as
trends and reviews, to personalize product suggestions for each user. This
approach helps brands understand the relationship between customers and
products. Sentiment analysis plays a crucial role here, as it allows companies
to gauge consumer sentiment from social media, online reviews, blogs, and
surveys. By categorizing sentiment as positive, negative, or neutral, companies
can tailor their marketing strategies accordingly. In 2022, a staggering $600
billion was spent on digital marketing, highlighting the significant investment
in personalized marketing strategies. The power of personalization is evident in
its ability to enhance customer engagement and drive sales by delivering
relevant content and offers to consumers.

Transition: Let's continue by examining another big data application in mobile
advertisement.

# ##############################################################################
# Slide 3: Big Data Applications
# ##############################################################################

This slide discusses the role of big data in mobile advertisement. With mobile
phones being ubiquitous, reaching 80% of the world's population and accounting
for 6.5 billion smartphones, they present a massive opportunity for targeted
advertising. By integrating online and offline databases, companies can gather
data such as GPS location, search history, and credit card transactions to
deliver personalized advertisements. For instance, if someone buys a new house
and searches for renovation ideas, watches related shows, and visits certain
locations, their phone can track this activity. Consequently, Google might send
them coupons for the nearest Home Depot, making it seem as though Google is
following them. This level of personalization is possible due to the vast amount
of data collected and analyzed, allowing companies to deliver timely and
relevant advertisements to consumers. The integration of big data in mobile
advertising enhances the effectiveness of marketing campaigns by ensuring that
the right message reaches the right audience at the right time.

# ##############################################################################
# Slide 1: Big Data Applications - Biomedical Data
# ##############################################################################

In the realm of big data applications, biomedical data plays a crucial role.
Personalized medicine is a significant advancement where treatments are tailored
specifically to individual patients. This customization is based on various
factors such as genetics, daily activities, environment, and personal habits. By
considering these elements, healthcare providers can offer more effective
treatments that are specifically suited to each patient. Genome sequencing is
another critical aspect of biomedical data. It involves analyzing an
individual's genetic code to understand their unique biological makeup, which
can inform personalized treatment plans. Health technology, including personal
health trackers like smart rings and smartphones, is becoming increasingly
popular. These devices collect data on various health metrics, providing users
and healthcare professionals with valuable insights into an individual's health
status. This data can be used to monitor health trends, predict potential health
issues, and make informed decisions about treatment and lifestyle changes.
Overall, the integration of big data in biomedical applications is transforming
the healthcare industry by enabling more precise and personalized care.

Transition: Let's now explore how big data is shaping the development of smart
cities.

# ##############################################################################
# Slide 2: Big Data Applications - Smart Cities
# ##############################################################################

Smart cities are another exciting application of big data, where an
interconnected mesh of sensors is utilized to enhance urban living. These
sensors include traffic sensors, camera networks, and satellites, all working
together to collect and analyze data. The primary goals of smart cities are to
monitor air pollution, minimize traffic congestion, provide optimal urban
services, and maximize energy savings. By analyzing data from these sensors,
city planners and administrators can make informed decisions to improve the
quality of life for residents. For example, traffic data can be used to optimize
traffic flow, reducing congestion and commute times. Air quality sensors can
help monitor pollution levels, enabling timely interventions to improve air
quality. Additionally, energy consumption data can be analyzed to identify areas
where energy savings can be maximized, contributing to a more sustainable urban
environment. Overall, the use of big data in smart cities is paving the way for
more efficient, sustainable, and livable urban spaces.

Transition: Now, let's delve into the overarching goal of data science and its
transformative potential.

# ##############################################################################
# Slide 3: Goal of Data Science
# ##############################################################################

The primary goal of data science is to transform data into wisdom. This
transformation process involves several stages. It begins with data, which are
raw bytes collected from various sources. This raw data is then organized and
structured to become information. From information, we derive knowledge through
learning and analysis. Finally, wisdom is achieved when we gain a deep
understanding of the insights derived from the data. These insights are crucial
as they enable informed decisions and actions. Data science often involves
combining streams of big data to generate new data, which can itself be
considered "big data." This iterative process of data transformation and
analysis is what drives innovation and progress in various fields. By harnessing
the power of data science, organizations can uncover hidden patterns, predict
future trends, and make strategic decisions that lead to better outcomes.
Ultimately, the goal of data science is to empower individuals and organizations
with the knowledge and wisdom needed to navigate an increasingly data-driven
world.

# ##############################################################################
# Slide 1: the Six V'S of Big Data
# ##############################################################################

In this slide, we explore the six key characteristics of big data, often
referred to as the Six V's. These characteristics help us understand the
complexities and challenges associated with managing and analyzing large
datasets.

- Volume: This refers to the sheer amount of data generated every day. With the
  exponential growth of data, organizations need to manage vast quantities of
  information efficiently.

- Variety: Data comes in different forms and formats. It can be structured, like
  spreadsheets, semi-structured, like text documents, or unstructured, like
  images and videos. Handling this variety is crucial for effective data
  analysis.

- Velocity: The speed at which data is generated and processed is critical. Some
  data needs to be analyzed in real-time, such as data from sensors, while other
  data can be processed offline.

- Veracity: This aspect deals with the quality and trustworthiness of data. It
  involves addressing biases, noise, and abnormalities in data to ensure
  accurate analysis.

- Valence: This refers to the interconnectedness of data, often represented in
  graph forms. Understanding these connections can provide deeper insights.

- Value: Ultimately, data must provide value to an organization. It should offer
  insights that lead to better decision-making and tangible benefits.

Let's delve deeper into the first two V's: Volume and Variety.

# ##############################################################################
# Slide 2: the Six V'S of Big Data - Volume and Variety
# ##############################################################################

This slide focuses on the first two V's: Volume and Variety, providing more
detailed insights into these aspects.

- Volume: The amount of data generated is staggering. For instance, 2.5 exabytes
  of data are created daily, with 90% of the world's data generated in the last
  two years. This rapid growth means data doubles approximately every 1.2 years.
  Examples include 500 million tweets per day on Twitter, 8.5 billion daily
  queries on Google, and 4 petabytes of data processed daily by Meta. Walmart
  alone handles 2.5 petabytes of unstructured data every hour.

- Variety: Data comes in various forms, which can be challenging to manage.
  Structured data includes organized formats like spreadsheets and databases.
  Semi-structured data includes text documents and sales receipts, while
  unstructured data encompasses photos and videos. Additionally, data can be
  stored in different formats such as binary, CSV, XML, and JSON. Managing this
  variety is essential for comprehensive data analysis.

Next, we will explore the concepts of Velocity and Veracity in big data.

# ##############################################################################
# Slide 1: Sources of Big Data
# ##############################################################################

In this slide, we are introduced to the concept of Big Data and its various
sources. Big Data can be categorized based on its origin, which includes
machines, people, and organizations. Each of these sources contributes to the
vast amount of data generated daily. Machines refer to devices and systems that
produce data through their operations, such as sensors and tracking systems.
People generate data through their interactions and activities online, like
social media and internet searches. Organizations contribute data through their
operations and transactions. Understanding the source of Big Data is crucial as
it helps in determining how to handle, process, and extract valuable insights
from it. The slide also includes an image to visually represent these sources,
which can help in better understanding the concept.

# ##############################################################################
# Slide 2: Sources of Big Data: Machines
# ##############################################################################

This slide focuses on machines as a source of Big Data. Machines generate data
through various means, such as real-time sensors found in aircraft like the
Boeing 787, cars, website tracking systems, personal health trackers, and
scientific experiments. The data produced by machines is highly structured,
which is a significant advantage as it makes processing and analysis more
straightforward. However, there are challenges associated with machine-generated
data. It is often difficult to move due to its volume, requiring computation to
be done in-place or centralized. Additionally, machine data is typically
streaming rather than batch, meaning it is continuously generated and needs
real-time processing. Understanding these pros and cons is essential for
effectively managing and utilizing machine-generated data.

Transition: Now, let's explore how people contribute to the generation of Big
Data.

# ##############################################################################
# Slide 3: Sources of Big Data: People
# ##############################################################################

This slide delves into how people generate Big Data through their activities.
People contribute data through social media platforms like Instagram, Twitter,
and LinkedIn, video-sharing sites such as YouTube and TikTok, blogging, website
comments, internet searches, text messages, and personal documents like Google
Docs and emails. The data generated by people is valuable for personalization
and business intelligence, offering insights into consumer behavior and
preferences. However, this data is often semi-structured or unstructured,
consisting of text, images, and videos, which presents challenges in processing
and extracting value. To derive insights, organizations must invest in
acquiring, storing, cleaning, retrieving, and processing this data.
Additionally, there are concerns about surveillance capitalism, where personal
data is used for commercial purposes without explicit consent. Understanding
these aspects is crucial for leveraging people-generated data effectively while
addressing privacy concerns.

# ##############################################################################
# Slide 1: Sources of Big Data: Orgs
# ##############################################################################

Organizations are major contributors to the generation of big data. They produce
data through various activities such as commercial transactions, credit card
usage, e-commerce activities, banking operations, medical records, and website
clicks. One of the advantages of data generated by organizations is that it is
highly structured, making it easier to analyze and interpret. However, there are
some challenges associated with this data. Organizations often store every event
with the aim of predicting future trends, but this can lead to missed
opportunities if the data is not utilized effectively. Additionally, data is
often stored in "data silos," where each department within an organization has
its own system and model for data storage. This can add complexity to data
management, as data may become outdated or not easily visible across
departments. Cloud computing solutions, such as data lakes and data warehouses,
offer ways to mitigate these issues by providing centralized and accessible data
storage options.

Transition: Now, let's explore whether data science is just a passing trend or
something more substantial.

# ##############################################################################
# Slide 2: Is Data Science Just Hype?
# ##############################################################################

The term "big data" or "data science" refers to any process where valuable
information is derived from data. Data scientists have been labeled as having
the "sexiest job" of the 21st century, but the term has become somewhat muddled
over time. This raises the question: is data science all hype? On one hand, data
science is not just hype. It involves extracting insights and knowledge from
data, and big data techniques have revolutionized various domains, including
education, food supply, and disease epidemic management. On the other hand, the
work of data scientists is similar to what statisticians have been doing for
years. The difference lies in the availability of more digital data, the
development of easy-to-use programming frameworks like Hadoop, and the cost
reduction offered by cloud computing services such as AWS. These advancements
allow large-scale data combined with simple algorithms to often outperform small
data with complex algorithms.

Transition: Let's delve deeper into what sets modern data science apart from
traditional statistical methods.

# ##############################################################################
# Slide 1: What Was Cool in 2012?
# ##############################################################################

In 2012, the technology landscape was buzzing with innovations that were
considered cutting-edge at the time. This was a period when cloud computing was
gaining significant traction, allowing businesses to store and process data over
the internet rather than on local servers. This shift enabled more flexibility
and scalability for companies. Big data was another hot topic, as organizations
began to realize the potential of analyzing large datasets to gain insights and
drive decision-making. Social media platforms were also on the rise,
transforming how people communicated and shared information globally. Mobile
technology was rapidly evolving, with smartphones becoming more powerful and
accessible, leading to a surge in mobile applications. These advancements
collectively shaped the way businesses operated and how individuals interacted
with technology.

# ##############################################################################
# Slide 2: What Was Cool in 2017?
# ##############################################################################

By 2017, the technological landscape had evolved significantly. Artificial
intelligence and machine learning were at the forefront, with businesses
exploring their potential to automate processes and enhance decision-making. The
Internet of Things (IoT) was becoming more prevalent, connecting everyday
devices to the internet and enabling smarter environments. Blockchain technology
was gaining attention for its potential to revolutionize industries with secure
and transparent transactions. Virtual and augmented reality were also making
waves, offering immersive experiences in gaming, education, and training. These
technologies were not just trends but were beginning to integrate into various
sectors, promising to transform industries and improve efficiencies.

Let's see how the technological landscape continued to evolve in 2022.

# ##############################################################################
# Slide 3: What Was Cool in 2022?
# ##############################################################################

In 2022, technology continued to advance at a rapid pace. Artificial
intelligence had become more sophisticated, with applications in natural
language processing, computer vision, and autonomous systems. The metaverse was
a buzzword, with companies investing in virtual worlds and digital experiences.
Quantum computing was making headlines, promising to solve complex problems
beyond the capabilities of classical computers. Cybersecurity was a major focus,
as the increase in digital interactions heightened the need for robust security
measures. Sustainability in technology was also a key trend, with efforts to
reduce the environmental impact of tech operations. These developments
highlighted the ongoing innovation in the tech industry and the potential for
future breakthroughs.

# ##############################################################################
# Slide 1: Key Shifts Before/After Big-Data
# ##############################################################################

In this slide, we explore the significant changes that have occurred with the
advent of big data. Initially, datasets were small, curated, and clean. This
meant that data collection was a meticulous process, often involving random
sampling and careful planning. Such datasets were costly to gather and limited
in scope, making detailed analysis challenging. However, with the rise of big
data, we can now easily collect vast amounts of information. This shift allows
us to feed large datasets into algorithms, where the sheer volume of data helps
to extract meaningful insights despite the presence of noise.

Another key shift is the move from focusing on causation to correlation.
Traditionally, the goal was to determine cause and effect, which is often
difficult to establish. Today, the emphasis is on identifying correlations,
which can be sufficient for many applications. For instance, the correlation
between the purchase of diapers and beer can be used for marketing strategies
without needing to understand the underlying causation.

Lastly, the concept of "data-fication" has emerged, where abstract concepts are
converted into data. This can be seen in how sensors can capture data about
sitting posture or how user preferences are quantified through likes. These
shifts highlight the transformative impact of big data on how we collect,
analyze, and interpret information.

Let's now look at a practical example of how big data has been used effectively
in election predictions.

# ##############################################################################
# Slide 2: Examples: Election Prediction
# ##############################################################################

This slide presents a compelling example of how big data has been utilized in
predicting election outcomes. Nate Silver's predictions for the 2008 and 2012 US
elections are highlighted. In 2008, he accurately predicted the outcomes in 49
out of 50 states, and in 2012, he achieved a perfect score by predicting all 50
states correctly. This remarkable accuracy can be attributed to several factors.

Firstly, the use of multiple data sources provided a comprehensive view of the
electoral landscape. By incorporating historical accuracy, Silver was able to
refine his predictions further. Statistical models played a crucial role in
analyzing the data, allowing for a better understanding of correlations between
various factors. Monte-Carlo simulations were employed to estimate electoral
probabilities, providing a probabilistic approach to prediction.

The focus on probabilities rather than certainties allowed for a more nuanced
understanding of potential outcomes. Effective communication of these
probabilities helped convey the predictions clearly to the public. This example
underscores the power of big data in making informed predictions and decisions,
showcasing its potential in various fields beyond just elections.

# ##############################################################################
# Slide 1: Examples: Google Flu Trends
# ##############################################################################

This slide discusses Google Flu Trends, a project that aimed to predict flu
outbreaks by analyzing search queries. The flu affects a significant portion of
the US population annually, with a considerable number of deaths. Early warnings
are crucial for prevention and control. Google Flu Trends attempted to provide
these early warnings by analyzing 45 specific search terms and using IP
addresses to determine the location of the searches. This approach allowed them
to predict regional flu outbreaks one to two weeks before the Centers for
Disease Control and Prevention (CDC). The project was active from 2008 to 2015.
However, the accuracy of Google Flu Trends declined over time. Initially, it
claimed a 97% accuracy rate, but its out-of-sample accuracy was lower, often
overshooting CDC data by 30%. One reason for this decline was that people often
searched for flu-related symptoms like "fever" and "cough" without having a
confirmed diagnosis. This highlights the limitations of relying solely on big
data for accurate predictions.

Transition: Now, let's explore the role of a data scientist in the context of
big data.

# ##############################################################################
# Slide 2: Data Scientist
# ##############################################################################

This slide introduces the concept of a data scientist, a term that is often
ambiguous and ill-defined. The slide references Drew Conway's Venn Diagram,
which is a popular framework for understanding the skills and knowledge areas
that a data scientist should possess. The Venn Diagram typically includes three
overlapping circles representing different skill sets: domain expertise,
programming skills, and statistical knowledge. A data scientist is someone who
sits at the intersection of these areas, combining them to extract meaningful
insights from data. The role of a data scientist is crucial in the era of big
data, as they are responsible for analyzing complex datasets and providing
actionable insights. However, the term "data scientist" can mean different
things in different contexts, and the specific skills required can vary
depending on the industry and the specific problems being addressed.

Transition: Let's delve into the typical workflow of a data scientist to
understand their role better.

# ##############################################################################
# Slide 3: Typical Data Scientist Workflow
# ##############################################################################

This slide outlines the typical workflow of a data scientist, which is essential
for understanding how they approach data analysis. The workflow usually starts
with data collection, where data scientists gather relevant data from various
sources. Next, they move on to data cleaning and preprocessing, which involves
removing errors and inconsistencies from the data to ensure its quality. After
that, data exploration and visualization help in understanding the data's
structure and identifying patterns or trends. The next step is model building,
where data scientists use statistical and machine learning techniques to create
predictive models. Once the models are built, they are evaluated to ensure their
accuracy and reliability. Finally, the insights gained from the models are
communicated to stakeholders, often through reports or visualizations. This
workflow is iterative, meaning data scientists may need to revisit previous
steps based on new findings or feedback. Understanding this workflow is crucial
for anyone looking to pursue a career in data science, as it provides a
structured approach to tackling complex data problems.

# ##############################################################################
# Slide 1: Where Data Scientist Spends Most Time
# ##############################################################################

- Data scientists spend a significant portion of their time, about 80-90%, on
  data cleaning and wrangling. This process involves preparing and organizing
  raw data into a usable format, which is crucial for accurate analysis and
  insights.
- This task is often referred to as "janitor work" in data science because it
  involves a lot of repetitive and meticulous tasks that are essential but not
  glamorous. Despite its unappealing nickname, this work is foundational to the
  success of any data-driven project.
- There are ongoing research directions in data wrangling aimed at making this
  process more efficient and less time-consuming. Innovations in this area could
  significantly reduce the time data scientists spend on these tasks, allowing
  them to focus more on analysis and interpretation.

Transition: Now that we understand where data scientists spend most of their
time, let's explore what skills they need to be effective.

# ##############################################################################
# Slide 2: What a Data Scientist Should Know
# ##############################################################################

- Data grappling skills are essential for data scientists. They need to be
  proficient in moving and manipulating data using programming languages like
  Python. Familiarity with data storage tools such as relational databases and
  key-value stores, as well as programming frameworks like SQL, Hadoop, and
  Spark, is crucial.
- Data visualization experience is important for creating informative visuals
  that help communicate data insights. Data scientists should know how to use
  tools like D3.js and various plotting libraries to effectively represent data.
- A solid understanding of statistics is necessary for interpreting data
  accurately. This includes knowledge of error-bars and confidence intervals,
  and proficiency in using statistical tools like Python libraries, Matlab, and
  R.
- Experience with forecasting and prediction is vital, as data scientists often
  use basic machine learning techniques to make predictions based on data.
- Communication skills are key for data scientists to tell the story behind the
  data and effectively communicate their findings to stakeholders.

Transition: With these skills in mind, let's delve into the concept of data
models and their importance in data science.

# ##############################################################################
# Slide 3: Data Models
# ##############################################################################

- Data modeling is the process of representing and capturing the structure and
  properties of real-world entities. It involves creating an abstraction that
  translates real-world scenarios into a representation that can be used for
  analysis and decision-making.
- A data model describes how data is represented and accessed. This includes
  different types of data models like relational and key-value models, and
  operations such as insertions and queries. A schema in a database is an
  example of a specific data collection described using a data model.
- Data models are essential because they provide a clear understanding of data
  structure, which is necessary for writing general-purpose code. They also
  facilitate data sharing across programs, organizations, and systems, and help
  integrate information from multiple sources. Additionally, data models are
  used to preprocess data for efficient access, such as building an index for
  faster retrieval.

# ##############################################################################
# Slide 1: Multiple Layers of Data Modeling
# ##############################################################################

- Physical Layer: This is about how data is stored on physical devices. It
  involves understanding the storage mechanisms like B-trees, which help in
  efficiently indexing and retrieving data. Knowing how data is physically
  stored is crucial for optimizing performance and ensuring data can be accessed
  quickly and reliably.

- Logical Layer: This layer focuses on the structure and organization of data.
  It involves defining entities (like tables in a database), their attributes
  (like columns in a table), and the types of information stored. It also
  includes understanding the relationships between these entities and
  attributes, which is essential for designing a coherent and functional
  database.

- Views: Views are used to control what data is accessible to users. They can
  restrict information flow to enhance security and make the system easier to
  use. By creating views, you can ensure that users only see the data they need,
  which helps in maintaining data privacy and simplifying user interactions with
  the database.

Transition: Now, let's delve deeper into the logical layer of data models.

# ##############################################################################
# Slide 2: Data Models: Logical Layer
# ##############################################################################

- Modeling Constructs: These are the basic building blocks used to represent
  data structures. They include concepts like entity types (which define what
  data is about), entity attributes (which describe the data), and the
  relationships between entities and attributes. Understanding these constructs
  is key to creating a logical representation of data that accurately reflects
  real-world scenarios.

- Integrity Constraints: These are rules that ensure data remains accurate and
  consistent. They help prevent errors and inconsistencies by enforcing
  conditions like non-empty fields or specific data types. Integrity constraints
  are vital for maintaining the reliability and trustworthiness of a database.

- Manipulation Constructs: These are operations that allow you to modify data,
  such as inserting new data, updating existing data, or deleting data. These
  constructs are essential for keeping the database up-to-date and relevant to
  the needs of its users.

Transition: With a solid understanding of the logical layer, let's explore
various examples of data models.

# ##############################################################################
# Slide 3: Examples of Data Models
# ##############################################################################

- Relational Model (SQL): This is a widely used model that organizes data into
  tables with rows and columns. It's known for its simplicity and powerful
  querying capabilities.

- Entity-Relationship (ER) Model: This model focuses on entities and their
  relationships, making it useful for designing databases that reflect complex
  real-world interactions.

- XML: A flexible, text-based format that is both human-readable and
  machine-readable, often used for data interchange.

- Object-Oriented (OO) and Object-Relational: These models integrate
  object-oriented programming concepts with database systems, allowing for more
  complex data representations.

- RDF and Property Graph: These models are used for representing data in a graph
  format, which is useful for capturing relationships and connections between
  data points.

- Serialization Formats: Formats like CSV, Parquet, JSON, Protocol Buffer,
  Avro/Thrift, and Python Pickle are used to serialize data for storage or
  transmission. They each have their strengths, such as simplicity, efficiency,
  or compatibility with specific programming environments. Understanding these
  formats is crucial for choosing the right one for your data needs.

# ##############################################################################
# Slide 1: Good Data Models
# ##############################################################################

- A good data model is essential for effectively capturing and utilizing
  real-world data. It should be expressive, meaning it can accurately represent
  the complexities and nuances of the data it models. This expressiveness
  ensures that the data model can handle a wide variety of data scenarios and
  use cases.
- Ease of use is another critical characteristic. A data model should be
  straightforward for users to interact with, minimizing the learning curve and
  reducing the potential for errors. This ease of use is crucial for both
  developers and end-users who need to query or manipulate the data.
- Performance is also a key consideration. A data model should perform well,
  meaning it should be efficient in terms of memory usage and processing time.
  However, there is often a tension between these characteristics. More powerful
  models can represent a broader range of datasets but may be more challenging
  to use and less efficient.
- The evolution of data modeling tools reflects the need to capture different
  data structures. Structured data led to the development of relational
  databases, which are highly organized and efficient for certain types of data.
  Semi-structured data, like web data, is often managed using formats like XML
  and JSON. Unstructured data, which lacks a predefined format, is typically
  handled by NoSQL databases, which offer more flexibility.

Transition: Understanding data independence is crucial for managing data
effectively across different systems and applications.

# ##############################################################################
# Slide 2: Data Independence
# ##############################################################################

- Data independence is a fundamental concept in database management, allowing
  changes to be made to the data representation without affecting the
  applications that use the data. This concept is divided into two types:
  logical and physical data independence.
- Logical data independence refers to the ability to change the data model or
  schema without altering the programs that access the data. For example, an API
  can abstract the backend data structure, allowing developers to modify the
  data representation without impacting the applications that rely on it.
- Physical data independence involves changing the way data is stored on disk
  without affecting the programs that use the data. This can include indexing
  data to improve retrieval speed, partitioning or distributing data across
  multiple locations for scalability, replicating data for redundancy,
  compressing data to save space, and sorting data to enhance query performance.
- Both types of data independence are crucial for maintaining flexibility and
  scalability in database systems. They allow organizations to adapt to changing
  requirements and technologies without disrupting existing applications or
  workflows.

Transition: Let's take a brief look at the history of databases to understand
how these concepts have evolved over time.

# ##############################################################################
# Slide 3: Databases: A Brief History
# ##############################################################################

- The history of databases dates back to the 1960s when computers began to gain
  traction as a valuable technology for enterprises. During this time, each
  application typically had its own data store, with data formats that were not
  accessible to other programs. This lack of interoperability led to
  inefficiencies and data silos.
- The term "database" emerged to describe shared data banks that multiple
  applications could access. This concept involved defining a data format and
  storing it as a "data dictionary" or schema. Database management software was
  developed to facilitate access to this shared data.
- Early databases faced several challenges, including how to write data
  dictionaries, how to access data efficiently, and who should control the data.
  These challenges included concerns about data integrity, security, and
  privacy.
- Over time, databases have evolved to address these issues, leading to the
  development of more sophisticated database management systems that support
  data independence, scalability, and security. Understanding this history helps
  us appreciate the complexities and capabilities of modern database systems.

# ##############################################################################
# Slide 1: Databases: A Brief History
# ##############################################################################

In the 1960s, databases were primarily organized using hierarchical and network
models. These models were designed to connect different types of records, such
as linking customer accounts with customer information. The network model was
particularly valued for its generality and flexibility, allowing for complex
relationships between data. A significant development during this time was IBM's
IMS Hierarchical Database, created in 1966 for the Apollo space program. This
database system was developed before the advent of hard disks and has been
widely adopted, with over 95% of the top Fortune 1000 companies using it. It is
capable of processing 50 billion transactions daily and managing 15 million
gigabytes of data. However, these early database models had their drawbacks.
They often exposed too much internal data, such as structures and pointers,
leading to what is known as a "leaky abstraction," where the complexity of the
system becomes apparent to the user.

Transition: As we move forward, let's explore the different database models that
emerged over time.

# ##############################################################################
# Slide 2: Relational, Hierarchical, Network Model
# ##############################################################################

The relational model, hierarchical model, and network model each offer unique
ways of organizing data. The relational model represents data as tuples within
relations and is accessed using SQL, a powerful query language. This model is
known for its simplicity and effectiveness in managing data. The hierarchical
model organizes data in a tree-like structure, where each parent can have
multiple children, connected through links. This model saw a resurgence in the
1990s with the rise of XML databases. The network model, on the other hand,
organizes data in a graph structure, allowing for multiple parents and children.
This model experienced a resurgence in the 2010s with the popularity of graph
databases. Each model has its strengths and is suited to different types of data
and use cases.

Transition: Now, let's delve into the evolution of the relational model in the
1970s.

# ##############################################################################
# Slide 3: Databases: A Brief History
# ##############################################################################

The 1970s marked the introduction of the relational model, developed by Ted
Codd. This model is based on set theory and first-order predicate logic,
providing an elegant and formal approach to data management. One of its key
advantages is data independence, meaning users do not need to worry about how
data is stored or processed. The relational model introduced a high-level query
language, SQL, which is based on relational algebra. This allowed users to
interact with data more intuitively. Additionally, the concept of normal forms
was introduced, enabling users to reason about data and relationships while
removing redundancies. Influential projects during this time included INGRES at
UC Berkeley and System R at IBM, both of which ignored compatibility with the
earlier IMS system. The introduction of the relational model sparked debates
between proponents of the relational and network models, each advocating for
their preferred approach to data management.

# ##############################################################################
# Slide 1: Entity-Relationship Model
# ##############################################################################

In 1976, Peter Chen introduced the Entity-Relationship (ER) Model, a framework
for organizing and describing knowledge in terms of entities and relationships.
Entities are essentially the "nouns" of the model, representing physical or
logical objects. For example, in a university database, entities could be
"Student" or "Course." Relationships, on the other hand, are the "verbs" that
connect these entities, such as "enrolls in" or "teaches." The ER model is
crucial because it provides a structured way to visualize and map out the data
and its connections. This model can be directly translated into a relational
database, where entities and relationships are represented as tables. This
mapping is fundamental in database design, as it helps in organizing data
efficiently and ensuring that the relationships between different data points
are maintained. The images accompanying this slide likely illustrate examples of
entities and relationships, providing a visual representation of how the ER
model is structured.

Transition: Now, let's explore the evolution of databases over time.

# ##############################################################################
# Slide 2: Databases: A Brief History
# ##############################################################################

The 1980s marked a significant period for databases with the widespread
acceptance of the relational model. This was largely due to the support from
IBM, which helped establish SQL as a standard language for managing and querying
relational databases. During this time, the relational model was enhanced to
include features like set-valued attributes and aggregation, which allowed for
more complex data manipulation. In the late 1980s, the emergence of
object-oriented databases introduced a new way of storing data, focusing on
objects rather than tables. This approach aimed to address the impedance
mismatch between programming languages and databases by allowing data to be
stored in a way that more closely resembles how it is used in applications.
Object-relational databases emerged as a hybrid, combining the benefits of
object-oriented databases with the relational model. They introduced
user-defined types, allowing for more flexibility in data representation.
However, despite these advancements, there was no significant expressive
difference from the pure relational model, meaning that the core capabilities of
relational databases remained largely unchanged.

Transition: Let's delve deeper into the object-oriented paradigm and its key
concepts.

# ##############################################################################
# Slide 3: Object-Oriented
# ##############################################################################

Object-oriented programming (OOP) is a data model that describes object behavior
through data, known as fields, and code, known as methods. One of the key
concepts in OOP is composition, also known as "has-a" relationships. For
example, an "Employee" class might have an "Address" class, indicating that an
employee has an address. Another important concept is inheritance, or "is-a"
relationships, where a class derives from another class. For instance, an
"Employee" class might derive from a "Person" class, inheriting its attributes
and behaviors. Polymorphism is another fundamental concept, allowing the same
interface to be used for different underlying forms. This means that the code
executed depends on the class of the object, such as a "draw()" method that
behaves differently for a "Circle" and a "Square," both of which descend from a
"Shape" class. Encapsulation is also crucial, as it involves restricting access
to certain parts of an object, such as private versus public fields or members.
This prevents external code from accessing the inner workings of an object,
promoting modularity and security in software design.

# ##############################################################################
# Slide 1: Databases: A Brief History
# ##############################################################################

In the late 90s, the internet began to grow rapidly, leading to the need for
more advanced data management systems. One significant development during this
time was the introduction of XML, or eXtensible Markup Language. XML is designed
to handle semi-structured data, which means it can manage data that doesn't fit
neatly into tables like traditional databases. It uses a tree-like structure,
allowing for a flexible schema that can adapt to different types of data. This
flexibility is crucial for web applications where data formats can vary widely.
The example provided shows a catalog of CDs, each with details like title,
artist, and price, demonstrating how XML can organize complex data in a readable
format. XML's ability to represent hierarchical data structures made it a
popular choice for web data exchange and storage.

Transition: Moving from XML, let's explore another framework for data
representation.

# ##############################################################################
# Slide 2: Resource Description Framework
# ##############################################################################

The Resource Description Framework, or RDF, is another method for organizing
data, particularly useful for representing information about resources on the
web. RDF uses a simple structure known as "subject-predicate-object" triples.
This means each piece of data is expressed as a relationship between a subject,
a predicate, and an object. For example, "sky has-the-color blue" is a simple
RDF triple. This structure maps to a labeled, directed multi-graph, which is
more general than a tree and allows for complex relationships between data
points. RDF data can be stored in traditional relational databases or
specialized "triple-stores" designed to handle these types of data structures.
The example provided illustrates how RDF can represent relationships between
people, dates, and interests, showing its versatility in linking diverse data
points.

Transition: Now, let's delve into another model that uses graphs to represent
data.

# ##############################################################################
# Slide 3: Property Graph Model
# ##############################################################################

The Property Graph Model is a way to represent data using graphs, which consist
of vertices (nodes) and edges (connections). Each vertex and edge can have
properties, which are key-value pairs that provide additional information about
the data. This model is particularly useful for representing complex networks,
such as social networks or transportation systems, where relationships between
data points are as important as the data points themselves. Property graphs can
be stored in traditional relational databases or specialized graph databases,
which are optimized for handling graph structures. The image provided shows a
visual representation of a property graph, highlighting how vertices and edges
can be used to model real-world relationships. This model's ability to handle
intricate connections makes it a powerful tool for analyzing and visualizing
complex data sets.
