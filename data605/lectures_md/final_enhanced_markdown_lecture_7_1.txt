::: columns
:::: {.column width=15%}
![](lectures_source/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605 - Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**Orchestration with Airflow
Data wrangling
Deployment 
**
\endgroup

::: columns
:::: {.column width=75%}
\vspace{1cm}

**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`**

**v1.1**
::::
:::: {.column width=20%}

::::
:::

UMD DATA605 - Big Data Systems

# ##############################################################################
# UMD DATA605 - Big Data Systems Orchestration with Airflow
# ##############################################################################

* UMD DATA605 - Big Data Systems Orchestration with Airflow
**UMD DATA605 - Big Data Systems** **Orchestration with Airflow**
Data wrangling
Deployment

Dr. GP Saggese
gsaggese@umd.edu

# ##############################################################################
# Orchestration - Resources
# ##############################################################################

* Orchestration - Resources
- Concepts in the slides
- Airflow tutorial
- Web resources
- Documentation
- Tutorial
- Mastery
- Data Pipelines with Apache Airflow

© UMD DATA605



![](images/lecture_7_1/lec_7_1_slide_2_image_1.png)

# ##############################################################################
# Workflow Managers (1)
# ##############################################################################

* Workflow Managers
- **Data pipelines** move/transform data across data stores
- **Orchestration problem** = data pipelines require to coordinate jobs across systems
  - Run tasks on a certain schedule
  - Run tasks in a specific order (dependencies)
  - Monitor tasks
    - Notify devops if a job fails
    - Retry on failure
    - Track how long it takes to run
  - Meet real-time constraints
  - Scale performance

© UMD DATA605



![](images/lecture_7_1/lec_7_1_slide_3_image_1.png)

# ##############################################################################
# Workflow Managers (2)
# ##############################################################################

* Workflow Managers
![](images/lecture_7_1/lec_7_1_slide_4_image_1.png)

- **E.g., live weather dashboard**
  - Fetch the weather data from API
  - Clean / transform the data
  - Push data to the dashboard/ website
- Problems
  - Tasks schedule
  - Tasks dependencies
  - Monitor functionality and performance
  - Quickly one wants to add machine learning
  - Quickly the complexity increases

© UMD DATA605



![](images/lecture_7_1/lec_7_1_slide_4_image_2.png)

![](images/lecture_7_1/lec_7_1_slide_4_image_3.png)

# ##############################################################################
# Workflow Managers (3)
# ##############################################################################

* Workflow Managers
- **Workflow managers address the orchestration problem**
  - E.g., Airflow, Luigi, Metaflow, make, cron ...
- **Represent data pipelines as DAGs**
  - Nodes are tasks
  - Direct edges are dependencies
  - A task is executed only when all the ancestors have been executed
  - Independent tasks can be executed in parallel
  - Re-run failed tasks incrementally
- **How to describe data pipelines**
  - Static files (e.g., XML, YAML)
  - Workflows-as-code (e.g., Python in Airflow)
- **Provide scheduling**
  - How to describe what and when to run
- **Provide backfilling and catch-up**
  - Horizontally scalable (e.g., multiple runners)
- **Provide monitoring web interface**

© UMD DATA605



![](images/lecture_7_1/lec_7_1_slide_5_image_1.png)

![](images/lecture_7_1/lec_7_1_slide_5_image_2.png)

![](images/lecture_7_1/lec_7_1_slide_5_image_3.png)

# ##############################################################################
# Airflow
# ##############################################################################

* Airflow
- Developed at AirBnB in 2015
  - Open-sourced as part of Apache project
- **Batch oriented framework** for building data pipelines (not streaming)
- **Data pipelines**
  - Represented as DAGs
  - Described as Python code
- **Scheduler with rich semantics**
- Web-interface for monitoring
- Large ecosystem
  - Support many DBs
  - Many actions (e.g., emails, pager notifications)
- **Hosted and managed solution**
  - Run Airflow on your laptop (e.g., in tutorial)
  - Managed solution (e.g., AWS)

© UMD DATA605



![](images/lecture_7_1/lec_7_1_slide_6_image_1.png)

# ##############################################################################
# Airflow: Execution Semantics
# ##############################################################################

* Airflow: Execution Semantics
- **Scheduling semantic**
  - Describe when the next scheduling interval is
    - E.g., “every day at midnight”, “every 5 minutes on the hour”
  - Similar to **cron**
- **Retry**
  - If a task fails, it can be re-run (after a wait time) to recover from intermittent failures
- **Incremental processing**
  - Time is divided into intervals given the schedule
  - Execute DAG only for data in that interval, instead of processing the entire data set
- **Catch-up**
  - Run all the missing intervals up to now (e.g., after a downtime)
- **Backfilling**
  - Execute DAG for historical schedule intervals that occurred in the past
  - E.g., if the data pipeline has changed one needs to re-process data from scratch

© UMD DATA605



# ##############################################################################
# Airflow: What Doesn’t Do Well
# ##############################################################################

* Airflow: What Doesn’t Do Well
- **Not great for streaming pipelines**
  - Better for recurring batch-oriented tasks
  - Time is assumed to be discrete and not continuous
    - E.g., schedule every hour, instead of process data as it comes
- **Prefer static pipelines**
  - DAGs should not change (too much) between runs
- **No data lineage**
  - No tracking of how data is transformed through the pipeline
  - Need to be implemented manually
- **No data versioning**
  - No tracking of updates to the data
  - Need to be implemented manually

© UMD DATA605


![](images/lecture_7_1/lec_7_1_slide_8_image_1.png)
```

```
# ##############################################################################
# Airflow: Components
# ##############################################################################

* Airflow: Components
![](images/lecture_7_1/lec_7_1_slide_9_image_1.png)

- **Users (DevOps)**
- **Web-server**
  - Visualize DAGs
  - Monitor DAG runs and results
- **Metastore**
  - Keep the state of the system
  - E.g., what DAG nodes have been executed
- **Scheduler**
  - Parse DAGs
  - Keep track of completed dependencies
  - Add tasks to the execution queue
  - Schedule tasks when time comes
- **Queue**
  - Tasks ready for execution
  - Tasks picked up by a pool of Workers
- **Workers**
  - Pick up tasks from Queue
  - Execute the tasks
  - Register task outcome in Metastore

© UMD DATA605



# ##############################################################################
# Airflow: Concepts
# ##############################################################################

* Airflow: Concepts
- Each DAG run represents a data interval, i.e., an interval between two times
  - E.g., a DAG scheduled **@daily**
  - Each data interval starts at midnight for each day, ends at midnight of next day
- DAG scheduled after data interval has ended
- Logical date
  - Simulate the scheduler running DAG / task for a specific date
  - Even if it is physically run now



# ##############################################################################
# Airflow: Tutorial (1)
# ##############################################################################

* Airflow: Tutorial
- Follow Airflow Tutorial in README
- From the tutorial for Airflow

© UMD DATA605



# ##############################################################################
# Airflow: Tutorial (2)
# ##############################################################################

* Airflow: Tutorial
- The script describes the DAG structure as Python code
  - There is no computation inside the DAG code
  - It only defines the DAG structure and the metadata (e.g., about scheduling)
- The **Scheduler** executes the code to build DAG
- **BashOperator** creates a task wrapping a Bash command

© UMD DATA605



![](images/lecture_7_1/lec_7_1_slide_12_image_1.png)

# ##############################################################################
# Airflow: Tutorial (3)
# ##############################################################################

* Airflow: Tutorial
- Dict with various default params to pass to the DAG constructor
  - E.g., different set-ups for dev vs prod
- Instantiate the DAG

© UMD DATA605



![](images/lecture_7_1/lec_7_1_slide_13_image_1.png)

![](images/lecture_7_1/lec_7_1_slide_13_image_2.png)

# ##############################################################################
# Airflow: Tutorial (4)
# ##############################################################################

* Airflow: Tutorial
- DAG defines tasks by instantiating **Operator** objects
  - The default params are passed to all the tasks
  - Can be overridden explicitly
- One can use a Jinja template
- Add tasks to the DAG with dependencies

© UMD DATA605


![](images/lecture_7_1/lec_7_1_slide_14_image_1.png)

![](images/lecture_7_1/lec_7_1_slide_14_image_2.png)

![](images/lecture_7_1/lec_7_1_slide_14_image_3.png)

**UMD DATA605 - Big Data Systems**
Orchestration with Airflow
**Data wrangling (Pandas)**
Deployment

Dr. GP Saggese
gsaggese@umd.edu

© UMD DATA605

# ##############################################################################
# Resources
# ##############################################################################

* Resources
- Pandas tutorial
- Class project
- Web
  - https://pandas.pydata.org
  - Onslaught of free resources
- Mastery
  - https://wesmckinney.com/book
  - Read cover-to-cover and execute all examples 2-3x time to really *master*



![](images/lecture_7_1/lec_7_1_slide_16_image_1.png)

© UMD DATA605
```

```
# ##############################################################################
# Overview (1)
# ##############################################################################

* Overview
- **Data wrangling**
  - Aka “data preparation”, “data munging”, “data curation”
  - Get data into a structured form suitable for analysis
  - Often it is the step where majority of time (80-90%) is spent
- **Key steps**
  - **Scraping**: extract information from sources (e.g., webpages)
  - **Data cleaning**: remove inconsistencies / errors
  - **Data transformation**: get data into the right structure
  - **Data integration**: combine data from multiple sources
  - **Information extraction**: extract structured information from unstructured / text sources



© UMD DATA605

# ##############################################################################
# Overview (2)
# ##############################################################################

* Overview


![](images/lecture_7_1/lec_7_1_slide_18_image_1.png)

© UMD DATA605

# ##############################################################################
# Overview (3)
# ##############################################################################

* Overview
- Many of the data wrangling problems are not easy to formalize, and have seen little research work, e.g.,
  - Data cleaning: mainly statistics, outlier detection, imputation
  - Data transformation, i.e., put the data in the “right” structure (e.g., tidy data)
  - Information extraction: feature computation, highly domain specific
- Others aspects have been studied in depth, e.g.,
  - Schema mapping
  - Data integration
- In an ETL process
  - Data extraction is the E step
  - Data wrangling is the T step



© UMD DATA605

# ##############################################################################
# Overview (4)
# ##############################################################################

* Overview


![](images/lecture_7_1/lec_7_1_slide_20_image_1.png)

- From Data Cleaning: Problems and Current Approaches
- Paper somewhat old: data is mostly coming from structured sources
- Today unstructured/semi-structured are equally important

© UMD DATA605

# ##############################################################################
# Data Extraction
# ##############################################################################

* Data Extraction
- Data may reside in a wide variety of different sources
  - Files (e.g., CSV, JSON, XML)
  - Many databases
  - Spreadsheets
  - AWS S3 buckets
  - ...
  - Most analytical tools support importing data from such sources through adapters
- Web scraping
  - In some cases there may be APIs, in other cases data may have to be explicitly scraped
  - Scraping data from web sources is tough
    - Can be fragile
    - Throttling
    - It’s cat-and-mouse game between scrapers and website
  - Often pipelines are set up to do this on a periodic basis
  - Several tools out there to do this (somewhat) automatically
    - E.g., import.io, portia, ...



© UMD DATA605

# ##############################################################################
# Tidy Data
# ##############################################################################

* Tidy Data
- Tidy data, Wickham, 2014
  - Each variable forms a column
  - Each observation forms a row
- Wide vs long format



![](images/lecture_7_1/lec_7_1_slide_22_image_1.png)

![](images/lecture_7_1/lec_7_1_slide_22_image_2.png)

![](images/lecture_7_1/lec_7_1_slide_22_image_3.png)

![](images/lecture_7_1/lec_7_1_slide_22_image_4.png)

“Messy” data

Tidy data

Wide format

Long format

![](images/lecture_7_1/lec_7_1_slide_22_image_5.png)

© UMD DATA605

# ##############################################################################
# **Data Quality Problems
# ##############################################################################

* **Data Quality Problems


![](images/lecture_7_1/lec_7_1_slide_23_image_1.png)

© UMD DATA605

# ##############################################################################
# Single-Source Problems
# ##############################################################################

* Single-Source Problems
- Depends largely on the source
- Databases can enforce constraints
- Data extracted from spreadsheets is often “clean”
  - At least there is a schema
- Logs are messy
- Data scraped from web-pages is much more messy
- Types of problems:
  - Ill-formatted data
  - Missing or illegal values, misspellings, use of wrong fields, extraction issues (e.g., not easy to separate out different fields)
  - Duplicated records, contradicting information, referential integrity violations
  - Unclear default/missing values
  - Evolving schemas or classification schemes (for categorical attributes)
  - Outliers



© UMD DATA605

# ##############################################################################
# Data Quality Problems
# ##############################################################################

* Data Quality Problems
**Data Quality Problems
**



![](images/lecture_7_1/lec_7_1_slide_25_image_1.png)

© UMD DATA605

# ##############################################################################
# Multi-Source Problems
# ##############################################################################

* Multi-Source Problems
## Multi-Source Problems

- Different data sources are:
  - Developed separately
  - Maintained by different people
  - Stored in different systems
  - ...
- Schema mapping / transformation
  - Mapping information across sources
  - Naming conflicts: same name used for different objects, different names for same objects
  - Structural conflicts: different representations across sources
- Entity resolution
  - Matching entities across sources
- Data quality issues
  - Contradicting information
  - Mismatched information
  - ...



© UMD DATA605

# ##############################################################################
# Data Cleaning: Outlier Detection
# ##############################################################################

* Data Cleaning: Outlier Detection
## Data Cleaning: Outlier Detection

- Quantitative Data Cleaning for Large Databases, Hellerstein, 2008
  - Focuses on numerical data (i.e., integers/floats that measure some quantities of interest)
- Sources of errors in data
  - Data entry errors: users putting in arbitrary values to satisfy the form
  - Measurement errors: especially sensor data
  - Distillation errors: errors that pop up during processing and summarization
  - Data integration errors: inconsistencies across sources that are combined together



© UMD DATA605

# ##############################################################################
# Univariate Outlier Detection
# ##############################################################################

* Univariate Outlier Detection
## Univariate Outlier Detection

- A set of values can be characterized by metrics such as
  - Center (e.g., mean)
  - Dispersion (e.g., standard deviation)
  - Higher momenta (e.g., skew, kurtosis)
- Use statistics to identify outliers
  - Must watch out for "masking": one extreme outlier may alter the metrics sufficiently to mask other outliers
  - Robust statistics: minimize effect of corrupted data
  - Robust center metrics:
    - Median
    - k%-trimmed mean (i.e., discard lowest and highest k% values)
  - Robust dispersion:
    - Median absolute deviation (MAD)
    - Median distance of values from the median value



![](images/lecture_7_1/lec_7_1_slide_28_image_1.png)

![](images/lecture_7_1/lec_7_1_slide_28_image_2.png)

© UMD DATA605

# ##############################################################################
# Outlier Detection
# ##############################################################################

* Outlier Detection
## Outlier Detection

- For Gaussian data
  - Any data points 1.4826x MAD away from median
  - May need to eyeball the data (e.g., plot a histogram) to decide if this is true
- For non-Gaussian data
  - Estimate generating distribution (parametric approach)
  - Distance-based methods: look for data points that do not have many neighbors
  - Density-based methods:
    - Define *density* to be average distance to *k* nearest neighbors
    - *Relative density* = density of node/average density of its neighbors
    - Use relative density to decide if a node is an outlier
- Most of these techniques start breaking down as the dimensionality of the data increases
  - *Curse of dimensionality*
    - You need an O(e^n) points with n dimensions to estimate
    - “In high dimensional spaces, data is always is sparse”
  - Can project data into lower-dimensional space and look for outliers there
    - Not as straightforward
- Wikipedia article on Outliers



© UMD DATA605

# ##############################################################################
# Multivariate Outliers
# ##############################################################################

* Multivariate Outliers
## Multivariate Outliers

- One set of techniques *multivariate Gaussian distribution* data
  - Defined by a *mean* μ and a *covariance matrix* Σ
- Mean / covariance are not *robust* (sensitive to outliers)
- Robust statistics analogous to univariate case
- Iterative approach
  - Mahalanobis distance of a point is the square root of (x - μ)'Σ-1(x - μ)
  - Measures how far the point x is from a multivariate normal distribution
  - Outliers are points that are too far away according to Mahalanobis distance
  - Remove outlier points
  - Recompute the mean and covariance
- Often volume of data is too much
  - Approximation techniques often used
- Need to try different techniques based on the data



![](images/lecture_7_1/lec_7_1_slide_30_image_1.png)

![](images/lecture_7_1/lec_7_1_slide_30_image_2.png)

© UMD DATA605

# ##############################################################################
# Time Series Outliers
# ##############################################################################

* Time Series Outliers
## Time Series Outliers

- Often data is in the form of a time series
- A **time series** is a sequence of data points recorded at regular time intervals tracking a variable over time
  - Stock prices
  - Sales revenue
  - Website traffic
  - Inventory levels
  - Energy consumption
  - Market demand
  - Social media engagement
  - Hourly energy usage
  - Customer satisfaction ratings over time
  - Weekly retail foot traffic
  - …
- Rich literature on *forecasting* in time series data
- Can use the historical patterns in the data to flag outliers
  - Rolling MAD (median absolute variation)



![](images/lecture_7_1/lec_7_1_slide_31_image_1.png)

**Time
**

© UMD DATA605

# ##############################################################################
# Split-Apply-Combine
# ##############################################################################

* Split-Apply-Combine
## Split-Apply-Combine

- The Split-Apply-Combine Strategy for Data Analysis, Wickam, 2011
- Common data analysis pattern
  - Split: break data into smaller pieces
  - Apply: operate on each piece independently
  - Combine: combine the pieces back together
- Pros
  - Code is compact
  - Easy to parallelize
- E.g.,
  - group-wise ranking
  - group vars (e.g., sums, means, counts)
  - create new models per group
- Supported by many languages
  - Pandas
  - SQL GROUP BY operator
  - Map-Reduce



![](images/lecture_7_1/lec_7_1_slide_32_image_1.png)

© UMD DATA605

# ##############################################################################
# Serialization Formats (1)
# ##############################################################################

* Serialization Formats
- Programs need to send data to each other (on the network, on disk)
  - E.g., Remote Procedure Calls (RPCs)
  - Several recent technologies based around schemas
    - JSON, YAML, Protocol Buffer, Python Pickle
- Serialization formats are data models



© UMD DATA605

# ##############################################################################
# Comma Separated Values (CSV)
# ##############################################################################

* Comma Separated Values (CSV)
- CSV stores data row-wise as text without schema
  - Each line of the file is a data record
  - Each record consists of one or more fields, separated by commas
- **Pros**
  - Very portable
    - It's text
    - Supported by every tool
  - Human-friendly
- **Cons**
  - Large footprint
    - Compression
  - Parsing is CPU intensive
  - No easy random access
  - No read only a subset of columns
  - No schema / types
    - Annotate CSV files with schema
  - Mainly read-only, difficult to modify



![](images/lecture_7_1/lec_7_1_slide_35_image_1.png)
![](images/lecture_7_1/lec_7_1_slide_35_image_2.png)

© UMD DATA605

# ##############################################################################
# (Apache) Parquet
# ##############################################################################

* (Apache) Parquet
- Parquet allows to read tiles of data
  - That’s what the name comes from
- Supports multi-dimensional and nested data
  - A generalization of dataframes
- Column-storage
  - Each column is stored together, has uniform data type, and compressed (efficiently)
- Queries can be executed by IO layer
  - Only the necessary chunks of data is read from disk
- **Pros**
  - 10x smaller than CSV
  - 10x faster (with multi-threading)
  - You can read only a subset of columns and rows
- **Cons**
  - Binary, non-human friendly
  - Need ingestion step converting the inbound format to Parquet
  - Mainly read-only, difficult to modify



![](images/lecture_7_1/lec_7_1_slide_36_image_1.png)

© UMD DATA605

# ##############################################################################
# JSON
# ##############################################################################

* JSON
- JSON = JavaScript Object Notation
- Data is nested dictionaries and arrays
- Very similar to XML
  - More human-readable
  - Less boilerplate
  - Executable in JavaScript (and Python)

**{     "firstName": "John",     "lastName": "Smith",     "isAlive": true,     "age": 25,     "height_cm": 167.6,     "address": {        "streetAddress": "21 2nd Street",        "city": "New York",        "state": "NY",        "postalCode": "10021-3100"     }, 
**   "phoneNumbers": [        {        "type": "home",        "number": "212 555-1234”       },        {        "type": "office",        "number": "646 555-4567"        }     ],     "children": [],     "spouse": null  }**


© UMD DATA605

# ##############################################################################
# Protocol Buffers
# ##############################################################################

* Protocol Buffers
- Developed by Google
- Open-source
- Represent data structures in:
  - Language agnostic
  - Platform agnostic
  - Versioning
- Schema is mostly relational
  - Optional fields
  - Types
  - Default values
  - Structures
  - Arrays
- Schema specified using a **.proto** file
- Compiled by **protoc** to produce C++, Java, or Python code to initialize, read, serialize objects

**import addressbook_pb2
**person = addressbook_pb2.Person()
**person.id = 1234
**person.name = "John Doe"
**person.email = "jdoe@example.com"
**phone = person.phones.add()
**phone.number = "555-4321"
**phone.type = addressbook_pb2.Person.HOME
**

**message Person {
**  optional string name = 1;
**  optional int32 id = 2;
**  optional string email = 3;
**  enum PhoneType {
**    MOBILE = 0;
**    HOME = 1;
**    WORK = 2;
**  }
**  message PhoneNumber {
**    optional string number = 1;
**    optional PhoneType type = 2;
**  }
**  repeated PhoneNumber phones = 4;
**}
**



© UMD DATA605

# ##############################################################################
# Serialization Formats (2)
# ##############################################################################

* Serialization Formats
- Avro
  - Richer data structures
  - JSON-specified schema
- Thrift
  - Developed by Facebook
  - Now Apache project
  - More languages supported
  - Supports exceptions and sets

**{        "namespace": "example.avro",        "type": "record",        "name": "User",        "fields": [                   {"name": "name", "type": "string"},                   {"name": "favorite_number", "type": ["int", "null"]},                   {"name": "favorite_color", "type": ["string", "null"]}        ] }**



© UMD DATA605

# ##############################################################################
# Remote Procedure Call
# ##############################################################################

* Remote Procedure Call
- **Remote Procedure Call** (RPC) is a protocol to request a service from a program located in another computer abstracting the details of the network communication
- **Goal**: similar to how procedure calls are made within a single process, without having to understand the network's details
- **Problems**
  - Can’t serialize pointers
  - Asynchronous communication
  - Failures and retry
- Used in distributed systems
  - E.g., microservices architectures, cloud services, and client-server applications
- Can be synchronous or asynchronous



© UMD DATA605

![](images/lecture_7_1/lec_7_1_slide_40_image_1.png)
```

# Content processing error - slide missing

```
# ##############################################################################
# RPCs: Internals
# ##############################################################################

* RPCs: Internals
- **Client procedure call**: Client calls a stub function, providing the necessary arguments
- **Request marshalling**: Client stub serializes the procedure's arguments into a format suitable for transmission over the network
- **Server communication**: Client's RPC runtime sends the procedure request across the network to the server
- **Server-side unmarshalling**: Server's RPC runtime receives the request and deserializes the arguments
- **Procedure execution**: Server calls the actual procedure on the server-side
- **Response marshalling**: Once the procedure completes, the return values are marshaled into a response message
- **Client communication **/ **response unmarshalling** / **return to client**: Return values are passed back to the client's original stub call, and execution continues as if the call were local.



© UMD DATA605

![](images/lecture_7_1/lec_7_1_slide_41_image_1.png)

# ##############################################################################
# Software testing (1)
# ##############################################################################

* Software testing
- Evaluate the functionality, reliability, performance, and security of a product and ensure it meets specified requirements
  - Software testing is a critical phase in the development process
- Adage:
  - “If it’s not tested, it doesn’t work”
  - “Debugging is 2x harder than writing code”
    - Corollary: if I’m doing my best to write code, how I can possibly debug it?
- **Different types of testing
**
- **Unit testing**: test individual components to ensure that each part functions correctly in isolation
- **Integration testing**: ensure that components work together as expected (e.g., detect interface defects)
- **System testing**: evaluate a fully integrated system's compliance with its specified requirements

© UMD DATA605



# ##############################################################################
# Software testing (2)
# ##############################################################################

* Software testing
- **Smoke/sanity testing**: A quick, non-exhaustive run-through of the functionalities to ensure that the main functions work as expected
  - E.g., decide if a new build is stable enough to use
  - E.g., the application doesn’t crash upon launching
- **Regression testing**: ensure that the new changes have not adversely affected existing functionality
  - Confusing: regressing in the sense of “getting worse”
- **Acceptance testing**: final phase of testing before the software is released
  - More used in waterfall workflows than Agile development
- **Performance testing**: load testing, stress testing, and spike testing
- **Security testing**: identify vulnerabilities, threats, and risks in the software
- **Usability testing**: assess how easy it is for end-users to use the software application
  - E.g., UI/UX
- **Compatibility testing**: check if the software is compatible with different browsers, database versions, operating systems, mobile devices, ...

© UMD DATA605


# ##############################################################################
# CI / CD
# ##############################################################################

* CI / CD
- **Continuous integration (CI)
**
  - Devs merge their code changes into a central repository, often multiple times a day
  - Automated build and test code after each change
  - **Goal**: Detect and fix integration errors quickly
  - Need unit tests! Add code together with tests!
- **Continuous deployment (CD)
**
  - Automatically deploy all code changes to a production environment
    - without human intervention
    - after the build and test phases pass
  - **Goal**: new features, bug fixes, and updates are continuously delivered to users in real-time
- E.g., GitHub actions, GitLab workflows, AWS Code, Jenkins

© UMD DATA605



# ##############################################################################
# RESTful API (1)
# ##############################################################################

* RESTful API
- REST API
  - = Web service API conforming to the REST style
  - REST = REpresentational State Transfer
  - Style to develop distributed systems
- **Uniform interface
**
  - Refer to resources (e.g., document, services, URI, persons)
  - Use HTTP methods (GET, POST, PUT, DELETE)
  - Naming convention, link format
  - Response (XML or JSON)
- **Stateless
**
  - Each request from client to server must contain all of the information necessary
  - No shared state
  - Inspired by HTTP (modulo cookies)

© UMD DATA605



# ##############################################################################
# RESTful API (2)
# ##############################################################################

* RESTful API
- **Cacheable
**
  - The data in a response should be labeled as cacheable or non-cacheable
  - If cacheable, the client can reuse the response later
  - Increase scalability and performance
- **Layered system
**
  - Each layer cannot "see" beyond the immediate layer that they interface
  - E.g., in a tier application

© UMD DATA605



![](images/lecture_7_1/lec_7_1_slide_46_image_1.png)

# ##############################################################################
# Stages of deployment
# ##############################################################################

* Stages of deployment
- The deployment of software progresses through several environments
  - Each environment is designed to progressively test, validate, and prepare the software for release to end user
- **Development environment (Dev)
**
  - Individual for each developer or feature team
  - Goal: Where developers write and initially test the code
- **Testing or Quality Assurance (QA) environment
**
  - Mirrors the production environment as closely as possible to perform under conditions similar to production
  - Goal: systematic testing of the software to uncover defects and ensure quality
- **Staging/Pre-Prod environment
**
  - Final testing phase before deployment to production
  - Replica of the production environment used for final checks and for stakeholders to review the new changes
- **Production Environment (Prod)
**
  - It is the live environment where the software is available to their end users
  - Highly optimized for security, performance, and scalability
  - Focus is on uptime, user experience, and data integrity

© UMD DATA605



# ##############################################################################
# Semantic versioning
# ##############################################################################

* Semantic versioning
- Semantic versioning is a versioning scheme for software that aims to convey meaning about the underlying changes
  - Systematic approach
  - Understand the potential impact of updating to a new version
- Major Version (**X**.y.z)
  - Incremented for incompatible API changes or significant updates that may break backward compatibility
- Minor Version (x.**Y**.z)
  - Incremented for backward-compatible enhancements and significant new features that don't break existing functionalities
- Patch Version (x.y.**Z**)
  - Incremented for backward-compatible bug fixes that address incorrect behavior
- Pre-release Version:
  - Label to denote a pre-release version that might not be stable (e.g., `1.0.0-alpha`, `1.0.0-beta`)
  - These releases are for testing and feedback, not for production use
- Build Metadata
  - Optional metadata to denote build information or environment specifics
  - E.g., 1.0.0+20210313120000 or 1.0.0+f8a34b3228c

© UMD DATA605



# ##############################################################################
# Microservices vs Monolithic Architecture
# ##############################################################################

* Microservices vs Monolithic Architecture


![](images/lecture_7_1/lec_7_1_slide_49_image_1.png)

![](images/lecture_7_1/lec_7_1_slide_49_image_2.png)

“Microservice” interest over time (from Google Trends)

- Different styles of building complex systems
- Find the right granularity

# ##############################################################################
# Microservice Architecture
# ##############################################################################

* Microservice Architecture
- **Modularity**: composed of small, independently deployable services, each implementing a specific business functionality
- **Scalability**: services can be scaled independently, allowing for efficient use of resources based on demand for specific features
- **Technology diversity**: each service can be developed using the most appropriate technology stack for its functionality
- **Deployment flexibility**: allows for continuous delivery and deployment practices, enabling faster iterations and updates
- **Resilience**: failure in one service doesn’t necessarily bring down the entire system; easier to isolate and address faults
- **Cons
**
- Complexity to deploy
- Needs tooling



# ##############################################################################
# Monolithic Architecture
# ##############################################################################

* Monolithic Architecture
- **Simplicity**: (initially) simpler to develop, test, deploy, and scale as a single application unit
- **Tightly coupled components**: all components run within the same process, leading to potential scalability and resilience issues as the application grows
- **Technology stack uniformity**: the entire application is developed with a single technology stack, which can limit flexibility
- **Deployment complexity**: Updates to a small part of the application require redeploying the entire application
- **Single point of failure**: Issues in any module can potentially affect the availability of the entire application


