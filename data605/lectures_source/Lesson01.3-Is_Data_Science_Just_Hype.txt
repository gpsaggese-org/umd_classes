// https://docs.google.com/presentation/d/1dWY49EVhAEiOJBbw07oMUDrA3D7QQAGubPAO4SYQvgE

::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605: Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Lesson 1.3: Is Data Science Just Hype?}}$$**
\endgroup

\vspace{1cm}

**Instructor**: Dr. GP Saggese, [gsaggese@umd.edu](gsaggese@umd.edu)

* Is Data Science Just Hype?
::: columns
:::: {.column width=50%}
- **Big data (or data science) is everywhere**
  - _"Any process where interesting information is inferred from data"_
- Data scientist called the "sexiest job" of the 21st century
  - The term has becoming very muddled at this point
- **Is it all hype?**
::::
:::: {.column width=45%}
![](data605/lectures_source/images/lecture_1/lec_1_slide_28_image_1.png)
::::
:::

* Is Data Science Just Hype?
- **No**
  - Extract insights and knowledge from data
  - Big data techniques revolutionize many domains
    - E.g., education, food supply, disease epidemics
- **But**
  - Similar to what statisticians have done for years
- **What is different?**
  - More data is digitally available
  - Easy-to-use programming frameworks (e.g., Hadoop) simplify analysis
  - Cloud computing (e.g., AWS) reduces costs
  - Large-scale data + simple algorithms often outperform small data + complex
    algorithms

* What Was Cool in 2012?

- Big data, Predictive analytics

![](data605/lectures_source/images/lecture_1/lec_1_slide_30_image_1.png)

* What Was Cool in 2017?

- Deep learning, Machine learning

![](data605/lectures_source/images/lecture_1/lec_1_slide_31_image_1.png)

* What Was Cool in 2022?

- Causal AI

![](data605/lectures_source/images/lecture_1/lec_1_slide_32_image_1.png)

* What Was Cool in 2025?

- Causal AI, Decision intelligence

![](data605/lectures_source/images/lecture_1/lec_1_slide_33_image_1.png)

* Key Shifts Before/After Big-Data
- **Datasets: small, curated, clean** $\to$ **large, uncurated, messy**
  - Before:
    - Statistics based on small, carefully collected random samples
    - Costly and careful planning for experiments
    - Hard to do fine-grained analysis
  - Today:
    - Easily collect huge data volumes
    - Feed into algorithms
    - Strong signal overcomes noise

- **Causation $\to$ Correlation**
  - Goal: determine cause and effect
  - Causation hard to determine $\to$ focus on correlation
    - Correlation is sometimes sufficient
    - E.g., diapers and beer bought together

- **"Data-fication"**
  - = converting abstract concepts into data
  - E.g., "sitting posture" data-fied by sensors in your seat
  - Preferences data-fied into likes

* Examples: Election Prediction
::: columns
:::: {.column width=50%}
- Nate Silver and the 2012 Elections
  - Predicted 49/50 states in 2008 US elections
  - Predicted 50/50 states in 2012 US elections
- **Reasons for accuracy**
  - Multiple data sources
  - Historical accuracy incorporation
  - Statistical models
  - Understanding correlations
  - Monte-Carlo simulations for electoral probabilities
  - Focus on probabilities
  - Effective communication
::::
:::: {.column width=45%}
![](data605/lectures_source/images/lecture_1/lec_1_slide_35_image_1.png)
::::
:::

* Examples: Google Flu Trends
- 5% to 20% of the US population gets the flu annually; **40k deaths**
  - Early warnings help in prevention and control
- **Google Flu Trends**
  - Provided early flu outbreak alerts via search query analysis
    - Analyzed 45 search terms
    - Used IP to determine location
  - Predicted regional flu outbreaks 1-2 weeks before CDC
  - Operated from 2008 to 2015
- **Caveat: accuracy issues**
  - Claimed 97% accuracy
  - Lower out-of-sample accuracy (overshot CDC data by 30%)
  - People search about flu without confirmed diagnosis
    - E.g., searching "fever" and "cough"

* Data Scientist
::: columns
:::: {.column width=50%}
- Ambiguous, ill-defined term
- From Drew Conway's Venn Diagram
::::
:::: {.column width=45%}
![](data605/lectures_source/images/lecture_1/lec_1_slide_37_image_1.png)
::::
:::

* Typical Data Scientist Workflow
- From Data Science Workflow

![](data605/lectures_source/images/lecture_1/lec_1_slide_38_image_1.jpeg){ height=70% }

* Where Data Scientist Spends Most Time

::: columns
:::: {.column width=30%}
- 80-90% of the work is data cleaning and wrangling
- "Janitor Work" in Data Science
::::
:::: {.column width=65%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_39_image_1.png){ height=70% }
::::
:::

* What a Data Scientist Should Know
- **Data grappling skills** $\leftarrow DATA605$
  - Move and manipulate data with programming
  - Scripting languages (e.g., Python)
  - Data storage tools: relational databases, key-value stores
  - Programming frameworks: SQL, Hadoop, Spark

- **Data visualization experience**
  - Draw informative data visuals
  - Tools: `D3.js`, plotting libraries
  - Know what to draw

- **Knowledge of statistics**
  - Error-bars, confidence intervals
  - Python libraries, Matlab, R

- **Experience with forecasting and prediction**
  - Basic machine learning techniques

- **Communication skills** $\leftarrow DATA605$
  - Tell the story, communicate findings
