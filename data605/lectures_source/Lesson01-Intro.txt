// https://docs.google.com/presentation/d/1dWY49EVhAEiOJBbw07oMUDrA3D7QQAGubPAO4SYQvgE/edit?slide=id.g185912b214c_0_13#slide=id.g185912b214c_0_13

::: columns
:::: {.column width=15%}
![](msml610/lectures_source/figures/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
UMD DATA605: Big Data Systems
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Introduction}}$$**
\endgroup

\vspace{1cm}

**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`

**References**:

# ##############################################################################
# Class Intro
# ##############################################################################

* Invariants of a Class Lecture
- **Invariants**
  - Focus on intuition over math
  - Emphasize realistic assumptions, numerical methods
    - Analytical solutions are outdated
  - Interactive Jupyter notebook tutorials
    - Tutorials done at home
    - Videos added over time
- **Class flow**
  - Alternate between slides, whiteboard, tutorials
- **Labs**
  - Review complete class project examples
  - Collaborate on class project

* Books of the Class
::: columns
:::: {.column width=65%}
- Slides
  - Are extracted from books, technical articles, Internet
  - Should be self-sufficient

::::
:::: {.column width=40%}

::::
:::

* Learning Outcomes
::: columns
:::: {.column width=65%}
- Model and reason about data
- Process and manipulate data
  - E.g., Python, Pandas
- Introduce a variety of data models
  - E.g., relational, NoSQL, graph DBs
  - Decide appropriate data model for different applications
- Use data management systems
  - E.g., PostgreSQL, MongoDB, HBase
  - Decide appropriate system for scenarios
- Build data processing pipelines
  - E.g., Docker, Airflow
- Build a big-data system end-to-end
  - Class project
  - Contribute to an open-source project
::::
:::: {.column width=30%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_4_image_1.png){ height=1cm }

![](data605/lectures_source/images/lecture_1/lec_1_slide_4_image_2.png){ height=1cm }

![](data605/lectures_source/images/lecture_1/lec_1_slide_4_image_3.png){ height=1cm }

![](data605/lectures_source/images/lecture_1/lec_1_slide_4_image_4.png){ height=1cm }

![](data605/lectures_source/images/lecture_1/lec_1_slide_4_image_5.png){ height=1cm }
::::
:::

* Tools We Will Learn To Use
- **Programming languages**
  - Python

- **Development tools**
  - Bash/Linux OS
  - Git: data model, branching
  - GitHub: Pull Requests (PR), issues
  - Jupyter notebooks
  - Docker

- **Big data tools**
  - Extract-Transform-Load (ETL) pipelines
  - Relational DBs (PostgreSQL)
  - NoSQL DBs (HBase, MongoDB, Couchbase, Redis)
  - Graph DBs (Neo4j, GraphX, Giraph)
  - Computing framework (Hadoop, Spark, Dask)
  - Workflow manager (Airflow)
  - Cloud services (AWS)

- **Tutorials** for tools used in the class projects

* Todos
- Study slides and materials
- DATA605 - ELMS/Canvas site
  - Enable notifications
  - Contact info for me/TAs
- DATA605 - Schedule
- DATA605 - GitHub repo
- Setup computing environment
  - Install Linux/VMware
  - Install Docker on laptop
  - Instructions in class repo
- Bring laptop to class
  - Quizzes at class start
- Lessons recorded
  - Attend class!

* Grading
- **Quizzes**
  - 40% of grade
  - Multi-choice on previous 2 lessons
  - 20 questions in 20 minutes
  - 4-5 quizzes to encourage study during semester
- **Final Project**
  - 60% of grade
  - Comprehensive application of course concepts
  - Big data project in Python from a list of topics
  - Individual or group

* Class Projects
- The project is _"Build $X$ with $Y$"_, where $X$ is a "use case" and $Y$ is a
  "technology"
  - Study and describe technology $Y$
  - Implement use case $X$ using technology $Y$
  - Create Jupyter notebooks to demo your project
  - Commit code to GitHub, contribute to open-source repo
  - Write a blog entry
  - Present your project in a video

- Choose from list of $X$ and $Y$, e.g.,
  - Big data
  - Large language models
  - ...

- Each project:
  - Individual or group ($n < 4$)
  - Varying difficulty levels

* Soft Skills to Succeed in the Workplace
- **Goal**: model class project for workplace preparation
  - Work in a team
  - Design software architecture (OOP, Agile, Design Patterns)
  - Comment your code
  - Write external documentation (tutorials, manuals, how-tos)
  - Write understandable code (including for future-you)
  - Read others' code
  - Follow code conventions (PEP8, Google Code)
  - Communicate clearly (emails, Slack)
  - File a bug report
  - Reproduce a bug
  - Intuition of CS constants
  - Basic understanding of OS (virtual memory, processes)

* Links

- [\textcolor{blue}{\underline{ELMS}}](https://umd.instructure.com/courses/1391619/pages/homepage)

- [\textcolor{blue}{\underline{Syllabus}}](https://docs.google.com/document/d/1YXCrqh6KGg3xm4-Lr4QGdBnjeWEfkqz67FHNHB_rdAk/edit?tab=t.0#heading=h.278ryn4xodsb)
  - Schedule
  - GitHub project
  - Class FAQs

- [\textcolor{blue}{\underline{Project specs}}](TBD)

// TODO: Add link to project specs

* Yours Truly
::: columns
:::: {.column width=65%}

- **GP Saggese**
  - 2001-2006, PhD / Postdoc at the University of Illinois at Urbana-Champaign
  - [\textcolor{blue}{\underline{LinkedIn}}](https://www.linkedin.com/in/gpsaggese/)
  - [\textcolor{blue}{\underline{gsaggese@umd.edu}}](gsaggese@umd.edu)
- **University of Maryland**:
  - 2023-, Lecturer for UMD DATA605: Big Data Systems
  - 2025-, Lecturer for UMD MSML610: Advanced Machine Learning
- **In the real-world**
  - Research scientist at NVIDIA, Synopys, Teza, Engineers' Gate
  - 3x AI and fin-tech startup founder (ZeroSoft, June, Causify AI)
  - 20+ academic papers, 2 US patents
::::
:::: {.column width=30%}
![](msml610/lectures_source/figures/GP_in_coding_state.png)
::::
:::

# ##############################################################################
# Big Data
# ##############################################################################

* Data Science
- **Promises of data science**
  - Give a competitive advantages
  - Make better strategic and tactical business decisions
  - Optimize business processes

- **Data science is not new**, it was called:
  - Operation research (~1970-80s)
  - Decision support, business intelligence (~1990s)
  - Predictive analytics (Early 2010s)
  - ...

- **What has changed**
  - Now learning and applying data science is **easy**
    - No need for hiring a consulting company
  - Tools are open-source
    - E.g., Python + pydata stack (numpy, scipy, Pandas, sklearn)
  - Large data sets available
  - Cheap computing
    - E.g., cloud computing (AWS, Google Cloud), GPUs

* Motivation: Data Overload
- _"Data science is the number one catalyst for economic growth"_ (McKinsey,
  2013)
- **Explosion of data in every domain**
  - Sensing devices/networks monitor processes 24/7
    - E.g., temperature of your room, your vital signs, pollution in the air
  - Sophisticated smart-phones
    - 80% of the world population has a smart-phone
  - Internet and social networks make it easy to publish data
  - Internet of Things (IoT): everything is connected to the internet
    - E.g., power supply, toasters
  - Datafication turns all aspects of life into data
    - E.g., what you like/enjoy turned into a stream of your "likes"
- **Challenges**
  - How to handle the increasing amount data?
  - How to extract actionable insights and scientific knowledge from data?

* Scale of Data Size
::: columns
:::: {.column width=45%}

- **Megabyte** = $2^{20} \approx 10^6$ bytes
  - Typical English book
- **Gigabyte** = $10^9$ bytes = 1,000 MB
  - 1/2 hour of video
  - Wikipedia (compressed, no media) is 22GB
- **Terabyte** = 1 million MB
  - Human genome: ~1 TB
  - 100,000 photos
  - \$50 for 1TB HDD, \$23/mo on AWS S3
- **Petabyte** = 1000 TB
  - 13 years of HD video
  - \$250k/year on AWS S3
::::
:::: {.column width=50%}
- **Exabyte** = 1M TB
  - Global yearly Internet traffic in 2004
- **Zetabyte** = 1B TB = $10^{21}$ bytes
  - Global yearly Internet traffic in 2016
  - Fill 20% of Manhattan, New York with data centers
- **Yottabytes** = $10^{24}$ bytes
  - Yottabyte costs \$100T
  - Fill Delaware and Rhode Island with a million data centers
- **Brontobytes** = $10^{27}$ bytes

::::
:::

* Constants Everybody Should Know
- CPU at 3GHz: 0.3 ns per instruction
- L1 cache reference/register: 1 ns
- L2 cache reference: 4 ns
- Main memory reference: 100 ns
- Read 1MB from memory: 20-100 us
- SSD random read: 16 us
- Send 1KB over network: 1 ms
- Disk seek: 2 ms
- Packet round-trip CA to Netherlands: 150 ms

![](data605/lectures_source/images/lecture_1/lec_1_slide_14_image_1.png)

* Big Data Applications
- **Personalized marketing**
- Target each consumer individually
  - E.g., Amazon personalizes suggestions using:
    - Shopping history
    - Search, click, browse activity
    - Other consumers and trends
    - Reviews (NLP and sentiment analysis)
- Brands understand customer-product relationships
  - Use sentiment analysis from:
    - Social media, online reviews, blogs, surveys
  - Positive, negative, neutral sentiment
- E.g.,
  - In 2022, \$600B spent on digital marketing
// - 50 Stats Showing The Power Of Personalization

* Big Data Applications

::: columns
:::: {.column width=60%}

- **Mobile advertisement**
- Mobile phones are ubiquitous
  - 80% of world population has one
  - 6.5 billion smartphones
- Integrate online and offline databases, e.g.,
  - GPS location
  - Search history
  - Credit card transactions
- E.g.,
  - You've bought a new house
  - You google questions about house renovations
  - You watch shows about renovations
  - Your phone tracks where you are
  - Google sends you coupons for the closest Home Depot
  - _"I feel like Google is following me"_

::::
:::: {.column width=35%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_17_image_1.png){ height=50% }

::::
:::

* Big Data Applications
- **Biomedical data**
- Personalized medicine
  - Patients receive treatment tailored to them for efficacy
  - Genetics
  - Daily activities
  - Environment
  - Habits
- Genome sequencing
- Health tech
  - Personal health trackers (e.g., smart rings, phones)

* Big Data Applications
- **Smart cities**
- Interconnected mesh of sensors
  - E.g., traffic sensors, camera networks, satellites
- Goals:
  - Monitor air pollution
  - Minimize traffic congestion
  - Optimal urban services
  - Maximize energy savings

* Goal of Data Science

::: columns
:::: {.column width=55%}
- **Goal**: from data to wisdom
  - Data (raw bytes)
  - Information (organized, structured)
  - Knowledge (learning)
  - Wisdom (understanding)
- Insights enable decisions and actions
- Combine streams of big data to generate new data
  - New data can be "big data" itself
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_20_image_1.png)

::::
:::

* The Six V'S of Big Data
::: columns
:::: {.column width=55%}
- **Volume**
  - Vast amount of data is generated
- **Variety**
  - Different forms
- **Velocity**
  - Speed of data generation
- **Veracity**
  - Biases, noise, abnormality in data
  - Uncertainty, trustworthiness
- **Valence**
  - Connectedness of data in the form of graphs
- **Value**
  - Data must be valuable
  - Benefit an organization
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_21_image_1.png)

::::
:::

* The Six V's of Big Data
- **Volume**
  - Exponentially increasing data
  - 2.5 exabytes (1m TB) generated daily
    - 90% of data generated in last 2 years
    - Data doubles every 1.2 years
  - Twitter/X: 500M tweets/day (2022)
  - Google: 8.5B queries/day (2022)
  - Meta: 4PB data/day (2022)
  - Walmart: 2.5PB unstructured data/hour (2022)

- **Variety**
  - Different data forms
    - Structured (e.g., spreadsheets, relational data)
    - Semi-structured (e.g., text, sales receipts, class notes)
    - Unstructured (e.g., photos, videos)
  - Different formats (e.g., binary, CSV, XML, JSON)

* The Six V's of Big Data
- **Velocity**
  - Speed of data generation
    - E.g., sensors generate data streams
  - Process data off-line or in real-time
  - Real-time analytics: consume data as fast as generated

- **Veracity**
  - Relates to data quality
  - How to remove noise and bad data?
  - How to fill in missing values?
  - What is an outlier?
  - How do you decide what data to trust?

* Sources of Big Data

::: columns
:::: {.column width=55%}
- Distinguish Big Data by source
  - **Machines**
  - **People**
  - **Organizations**
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_24_image_1.png)

::::
:::

* Sources of Big Data: Machines
- **Machines** generate data
  - Real-time sensors (e.g., sensors on Boeing 787)
  - Cars
  - Website tracking
  - Personal health trackers
  - Scientific experiments
- **Pros**
  - Highly structured
- **Cons**
  - Difficult to move, computed in-place or centralized
  - Streaming, not batch

* Sources of Big Data: People

::: columns
:::: {.column width=55%}

- **People** and their activities generate data
  - Social media (Instagram, Twitter, LinkedIn)
  - Video sharing (YouTube, TikTok)
  - Blogging, website comments
  - Internet searches
  - Text messages (SMS, Whatsapp, Signal, Telegram)
  - Personal documents (Google Docs, emails)
- **Pros**
  - Enable personalization
  - Valuable for business intelligence
- **Cons**
  - Semi-structured or unstructured data
    - Text, images, movies
  - Requires investment to extract value
    - Acquire $\to$ Store $\to$ Clean $\to$ Retrieve $\to$ Process $\to$ Insights
  - Surveillance capitalism

::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_26_image_1.png)

::::
:::

* Sources of Big Data: Orgs
- **Organizations** generate data
  - Commercial transactions
  - Credit cards
  - E-commerce
  - Banking
  - Medical records
  - Website clicks
- **Pros**
  - Highly structured
- **Cons**
  - Store every event to predict future
    - Miss opportunities
  - Stored in "data silos" with different models
    - Each department has own system
    - Additional complexity
    - Data outdated/not visible
    - Cloud computing helps (e.g., data lakes, data warehouses)

* Is Data Science Just Hype?

::: columns
:::: {.column width=55%}

- Big data (or data science)
  - "Any process where interesting information is inferred from data"
- Data scientist called the "sexiest job" of the 21st century
  - The term has becoming very muddled at this point
- **Is it all hype?**

::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_28_image_1.png)

::::
:::

* Is Data Science Just Hype?
- **No**
  - Extract insights and knowledge from data
  - Big data techniques revolutionize many domains
    - E.g., education, food supply, disease epidemics
- **But**
  - Similar to what statisticians have done for years
- **What is different?**
  - More data is digitally available
  - Easy-to-use programming frameworks (e.g., Hadoop) simplify analysis
  - Cloud computing (e.g., AWS) reduces costs
  - Large-scale data + simple algorithms often outperform small data + complex algorithms

* What Was Cool in 2012?

![](data605/lectures_source/images/lecture_1/lec_1_slide_30_image_1.png)

// TODO: Add "From Gartner"

* What Was Cool in 2017?

![](data605/lectures_source/images/lecture_1/lec_1_slide_31_image_1.png)

* What Was Cool in 2022?

// TODO: Remove the title

![](data605/lectures_source/images/lecture_1/lec_1_slide_32_image_1.png)

* What Was Cool in 2023?

// TODO: Remove the title

![](data605/lectures_source/images/lecture_1/lec_1_slide_33_image_1.png)

* Key Shifts Before/After Big-Data
- **Datasets: small, curated, clean $\to$ large, uncurated, messy**
  - Before:
    - Statistics based on small, carefully collected random samples
    - Costly and careful planning for experiments
    - Hard to do fine-grained analysis
  - Today:
    - Easily collect huge data volumes
    - Feed into algorithms
    - Strong signal overcomes noise
- **Causation $\to$ Correlation**
  - Goal: determine cause and effect
  - Causation hard to determine $\to$ focus on correlation
    - Correlation is often sufficient
    - E.g., diapers and beer bought together
- **"Data-fication"**
  - = converting abstract concepts into data
  - E.g., "sitting posture" data-fied by sensors in your seat
  - Preferences data-fied into likes
- From: Rise of Big Data, 2013

* Examples: Election Prediction
::: columns
:::: {.column width=55%}
- Nate Silver and the 2012 Elections
  - Predicted 49/50 states in 2008 US elections
  - Predicted 50/50 states in 2012 US elections
- Reasons for accuracy
  - Multiple data sources
  - Historical accuracy incorporation
  - Statistical models
  - Understanding correlations
  - Monte-Carlo simulations for electoral probabilities
  - Focus on probabilities
  - Effective communication
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_35_image_1.png)
::::
:::

* Examples: Google Flu Trends
- 5% to 20% of US population contracts flu yearly; 40k deaths
- Early warnings enable prevention and control
- Google Flu Trends
  - Early flu outbreak warnings via search query analysis
    - 45 search terms analyzed
    - IP used to determine location
  - Predict regional flu outbreaks 1-2 weeks before CDC
  - Active from 2008 to 2015
- Caveat: accuracy declined
  - Claimed 97% accuracy
  - Out of sample accuracy lower (overshot CDC data by 30%)
  - People search about flu without knowing diagnosis
    - E.g., searching for "fever" and "cough"
  - Google Flu Trends: The Limits of Big Data

* Data Scientist
::: columns
:::: {.column width=55%}
- Ambiguous, ill-defined term
- From Drew Conway's Venn Diagram

::::
:::: {.column width=40%}
![](data605/lectures_source/images/lecture_1/lec_1_slide_37_image_1.png)

::::
:::

* Typical Data Scientist Workflow
- From Data Science Workflow

![](data605/lectures_source/images/lecture_1/lec_1_slide_38_image_1.jpeg)[ height=70% ]

* Where Data Scientist Spends Most Time

- 80-90% of the work is data cleaning and wrangling
- "Janitor Work" in Data Science
- Research Directions in Data Wrangling

![](data605/lectures_source/images/lecture_1/lec_1_slide_39_image_1.png)[ height=70% ]

* What a Data Scientist Should Know
- From: How to hire a data scientist
- **Data grappling skills**
  - Move and manipulate data with programming
  - Scripting languages (e.g., Python)
  - Data storage tools: relational databases, key-value stores
  - Programming frameworks: SQL, Hadoop, Spark

- **Data visualization experience**
  - Draw informative data visuals
  - Tools: `D3.js`, plotting libraries
  - Know what to draw

- **Knowledge of statistics**
  - Error-bars, confidence intervals
  - Python libraries, Matlab, R

- **Experience with forecasting and prediction**
  - Basic machine learning techniques

- **Communication skills**
  - Tell the story, communicate findings

# Data Models

* Data Models
- **Data modeling**
  - Represents and captures structure and properties of real-world entities
  - Abstraction: real-world $\to$ **representation**

- **Data model**
  - Describes how data is _represented_ (e.g., relational, key-value) and _accessed_ (e.g., insert operations, query)
  - Schema in a DB describes a specific data collection using a data model

- **Why need data model?**
  - Know data structure to write general-purpose code
  - Share data across programs, organizations, systems
  - Integrate information from multiple sources
  - Preprocess data for efficient access (e.g., building an index)

* Multiple Layers of Data Modeling
::: columns
:::: {.column width=55%}

- **Physical layer**
  - How is the data physically stored
  - How to represent complex data structures (e.g., B-trees for indexing)
- **Logical layer**
  - Entities
  - Attributes
  - Type of information stored
  - Relationships among the above
- **Views**
  - Restrict information flow
  - Security and/or ease-of-use

::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_43_image_1.png)

::::
:::

* Data Models: Logical Layer

::: columns
:::: {.column width=55%}

- **Modeling constructs**
  - Concepts to represent data structure
  - E.g.,
    - Entity types
    - Entity attributes
    - Relationships between entities
    - Relationships between attributes
- **Integrity constraints**
  - Ensure data integrity
    - Avoid errors and inconsistencies
    - E.g., field can't be empty, must be an integer
- **Manipulation constructs**
  - E.g., insert, update, delete data

::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_44_image_1.png)

::::
:::

* Examples of Data Models
- We will cover:
  - Relational model (SQL)
  - Entity-relationship (ER) model
  - XML
  - Object-oriented (OO)
  - Object-relational
  - RDF
  - Property graph
- Serialization formats as data models
  - CSV
  - Parquet
  - JSON
  - Protocol Buffer
  - Avro/Thrift
  - Python Pickle

* Good Data Models
- Data model should be:
  - Expressive
    - Capture real-world data
  - Easy to use
  - Perform well
- Tension between characteristics
  - Powerful models
    - Represent more datasets
    - Harder to use/query
    - Less efficient (e.g., more memory, time)
- Evolution of data modeling tools captures data structure
  - Structured data $\to$ Relational DBs
  - Semi-structured web data $\to$ XML, JSON
  - Unstructured data $\to$ NoSQL DBs

* Data Independence
- **Logical data independence**
  - Change data representation without altering programs
  - E.g., API abstracting backend


- **Physical data independence**
  - Change data layout on disk without altering programs
    - Index data
    - Partition/distribute/replicate data
    - Compress data
    - Sort data

* Databases: A Brief History
- **1960s: Early beginning**
  - Computers become attractive technology
  - Enterprises adopt computers
  - Applications use own data stores
    - Each application has its own format
    - Data unavailable to other programs
  - **Database**: term for "shared data banks" by multiple applications
    - Define data format
    - Store as "data dictionary" (schema)
    - Implement "database management" software to access data
  - Issues:
    - How to write data dictionaries?
    - How to access data?
    - Who controls the data?
    - E.g., integrity, security, privacy concerns

* Databases: A Brief History
- **1960s, Hierarchical and Network Model**
  - Connect records of different types
  - Example: connect accounts with customers
  - Network model aimed for generality and flexibility
- IBM's IMS Hierarchical Database (1966)
  - Designed for Apollo space program
  - Predates hard disks
  - Used by over 95% of top Fortune 1000 companies
  - Processes 50 billion transactions daily, manages 15 million gigabytes of data
- Cons:
  - Exposed too much internal data (structures/pointers)
  - Leaky abstraction

* Relational, Hierarchical, Network Model

::: columns
:::: {.column width=55%}
- **Relational model**
  - Data as tuples in relations
  - SQL
- **Hierarchical model**
  - Tree-like structure
    - One parent, many children
    - Connected through links
  - XML DBs resurgence in 1990s
- **Network model**
  - Graph organization
    - Multiple parents and children
  - Graph DBs resurgence in 2010s
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_50_image_1.png)

![](data605/lectures_source/images/lecture_1/lec_1_slide_50_image_2.png)

![](data605/lectures_source/images/lecture_1/lec_1_slide_50_image_3.png)
::::
:::

* Databases: A Brief History
- **1970s: Relational model**
  - Set theory, first-order predicate logic
    - Ted Codd developed the Relational Model
  - Elegant, formal model
    - Provided data independence
    - Users didn't worry about data storage, processing
  - High-level query language
    - SQL based on relational algebra
  - Notion of normal forms
    - Reason about data and relations
    - Remove redundancies
- Influential projects:
  - INGRES (UC Berkeley), System R (IBM)
  - Ignored IMS compatibility
- Debates: Relational Model vs Network Model proponents

* Entity-Relationship Model

::: columns
:::: {.column width=55%}

- 1976: Peter Chen proposed "Entity-Relationship Model"
- Describes knowledge as entities and relationships
- **Entities**: Physical or logical objects, "Nouns"
- **Relationships**: Connections between entities, "Verbs"
- Map ER model to relational DB: Entities, relationships $\to$ tables
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_52_image_1.png)
![](data605/lectures_source/images/lecture_1/lec_1_slide_52_image_2.png)
![](data605/lectures_source/images/lecture_1/lec_1_slide_52_image_3.png)

::::
:::

* Databases: A Brief History
- **1980s: Relational model acceptance**
  - SQL standard due to IBM's backing
  - Enhanced relational model
    - Set-valued attributes, aggregation
- Late 80's
  - Object-oriented DBs
    - Store objects, not tables
    - Overcome _impedance mismatch_ between languages and databases
  - Object-relational DBs
    - User-defined types
    - Combine object-oriented benefits with relational model
  - No expressive difference from pure relational model

* Object-Oriented
- OOP is a data model
  - Object behavior described through data (fields) and code (methods)

- **Composition**
  - Aka `has-a` relationships
  - E.g., `Employee` class has an `Address` class
- **Inheritance**
  - Aka `is-a` relationships
  - E.g., `Employee` class derives from `Person` class

- **Polymorphism**
  - Code executed depends on the class of the object
  - One interface, many implementations
  - E.g., `draw()` method on a `Circle` vs `Square` object, both descending from
    `Shape` class

- **Encapsulation**
  - E.g., private vs public fields/members
  - Prevents external code from accessing inner workings of an object

* Databases: A Brief History
- **Late 90's-today**
- Web/Internet emerges
- XML: eXtensible Markup Language
  - For _semi-structured_ data
  - Tree-like structure
  - Flexible schema
```
<?xml version="1.0" encoding="UTF-8"?>
<!-- Edited by XMLSpy -->
  <CATALOG>
    <CD>
      <TITLE>Empire Burlesque</TITLE>
      <ARTIST>Bob Dylan</ARTIST>
      <COUNTRY>USA</COUNTRY>
      <COMPANY>Columbia</COMPANY>
      <PRICE>10.90</PRICE>
      <YEAR>1985</YEAR>
    </CD>
    <CD>
      <TITLE>Hide your heart</TITLE>
      <ARTIST>Bonnie Tyler</ARTIST>
      <COUNTRY>UK</COUNTRY>
      <COMPANY>CBS Records</COMPANY>
      <PRICE>9.90</PRICE>
      <YEAR>1988</YEAR>
    </CD>
    ...
```

* Resource Description Framework
::: columns
:::: {.column width=55%}
- Aka RDF
- Key construct: "subject-predicate-object" triple
  - Subject=sky
  - Predicate=has-the-color
  - Object=blue
- Maps to a labeled, directed multi-graph
  - More general than a tree
- Stored in:
  - Relational DBs
  - Dedicated "triple-stores" DBs
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_56_image_1.png)
```
01    <http://example.org/bob#me> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Person> .
02    <http://example.org/bob#me> <http://xmlns.com/foaf/0.1/knows> <http://example.org/alice#me> .
03    <http://example.org/bob#me> <http://schema.org/birthDate> "1990-07-04"^^<http://www.w3.org/2001/XMLSchema#date> .
04    <http://example.org/bob#me> <http://xmlns.com/foaf/0.1/topic_interest> <http://www.wikidata.org/entity/Q12418> .
05    <http://www.wikidata.org/entity/Q12418> <http://purl.org/dc/terms/title> "Mona Lisa" .
06    <http://www.wikidata.org/entity/Q12418> <http://purl.org/dc/terms/creator> <http://dbpedia.org/resource/Leonardo_da_Vinci> .
07    <http://data.europeana.eu/item/04802/243FA8618938F4117025F17A8B813C5F9AA4D619> <http://purl.org/dc/terms/subject> <http://www.wikidata.org/entity/Q12418> .
```
::::
:::

* Property Graph Model

::: columns
:::: {.column width=55%}
- Graph:
  - Vertices and edges
  - Properties for each edge and vertex
- Stored in:
  - Relational DBs
  - Graph DBs
::::
:::: {.column width=40%}

![](data605/lectures_source/images/lecture_1/lec_1_slide_57_image_1.jpeg)

::::
:::
